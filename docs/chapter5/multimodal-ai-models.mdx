---
id: multimodal-ai-models
title: Multimodal AI Models and Architectures
---

# Multimodal AI Models and Architectures

## Overview

Multimodal AI models form the backbone of Vision-Language-Action (VLA) systems, enabling the integration of visual perception and natural language understanding. These models can process information from multiple modalities simultaneously, creating unified representations that connect visual concepts with linguistic meaning. Understanding multimodal architectures is crucial for developing effective VLA systems that can interpret human language commands in the context of visual environmental information.

The success of modern VLA systems relies heavily on advances in multimodal deep learning architectures, particularly those that can learn joint representations of vision and language. These models learn to associate visual elements (objects, scenes, actions) with their linguistic descriptions, creating the foundation for natural language interaction with robotic systems.

## Key Concepts

### Multimodal Model Fundamentals

Multimodal AI models in robotics must address several fundamental challenges:

**Cross-Modal Alignment**: Connecting information from different sensory modalities (vision and language) that have different structures and representations.

**Fusion Strategies**: Determining how to combine information from multiple modalities, whether early (at input level), late (at decision level), or intermediate (at hidden layer level).

**Representation Learning**: Learning unified representations that capture relationships between visual and linguistic concepts.

**Scalability**: Handling the combinatorial complexity of vision-language associations in real-world environments.

### Vision-Language Integration Approaches

**Embedding Space Alignment**: Creating shared embedding spaces where visual and linguistic features can be directly compared and combined.

**Attention Mechanisms**: Using attention to focus on relevant visual regions when processing language and vice versa.

**Transformer Architectures**: Leveraging transformer models that can process sequential information (language) alongside visual patches or regions.

**Foundation Models**: Large pre-trained models that serve as base representations for downstream robotic tasks.

## Major Multimodal Architectures

### Contrastive Language-Image Pre-training (CLIP)

CLIP represents a breakthrough approach to vision-language alignment by training a visual encoder and text encoder to produce similar representations for corresponding image-text pairs and dissimilar representations for mismatched pairs.

**Architecture Components**:
- **Visual Encoder**: Typically a Vision Transformer (ViT) or ResNet that processes images into dense representations
- **Text Encoder**: A Transformer-based language model (like BERT or GPT) that processes text into dense representations
- **Contrastive Loss**: Training objective that pulls together matching image-text pairs while pushing apart non-matching pairs

```python
import torch
import torch.nn as nn
import clip
from PIL import Image
import numpy as np

class CLIPRobotInterface(nn.Module):
    """
    Example of using CLIP for robotics vision-language tasks
    """
    def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
        super().__init__()
        self.device = device
        
        # Load pre-trained CLIP model
        self.model, self.preprocess = clip.load("ViT-B/32", device=device)
        
        # Possible robot actions as text prompts
        self.robot_actions = [
            "grasping an object",
            "moving forward", 
            "turning left",
            "turning right",
            "picking up red object",
            "avoiding obstacle",
            "navigating to goal"
        ]
    
    def encode_image(self, image):
        """Encode an image using CLIP visual encoder"""
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            # Normalize features
            image_features /= image_features.norm(dim=-1, keepdim=True)
        return image_features
    
    def encode_text(self, text):
        """Encode text using CLIP text encoder"""
        text_input = clip.tokenize([text]).to(self.device)
        with torch.no_grad():
            text_features = self.model.encode_text(text_input)
            # Normalize features
            text_features /= text_features.norm(dim=-1, keepdim=True)
        return text_features
    
    def compute_similarity(self, image, texts):
        """Compute similarity between image and multiple text options"""
        image_features = self.encode_image(image)
        
        # Tokenize all text options
        text_inputs = clip.tokenize(texts).to(self.device)
        
        with torch.no_grad():
            # Encode all text options
            text_features = self.model.encode_text(text_inputs)
            # Normalize
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Compute similarity scores
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        return similarity.squeeze()

# Example usage for robotics
def select_robot_action(clip_interface, current_image, command):
    """
    Use CLIP to select most appropriate robot action based on
    current visual input and language command
    """
    # Create possible action descriptions with context
    possible_actions = [
        f"{command}, robot is grasping an object",
        f"{command}, robot is moving forward", 
        f"{command}, robot is turning left",
        f"{command}, robot is turning right",
        f"{command}, robot is picking up red object"
    ]
    
    # Compute similarity between current scene and possible actions
    similarities = clip_interface.compute_similarity(current_image, possible_actions)
    
    # Select action with highest similarity
    best_action_idx = torch.argmax(similarities)
    return possible_actions[best_action_idx], similarities[best_action_idx].item()
```

### Vision-Language Transformers (ViLT and BLIP)

Vision-Language Transformers extend the transformer architecture to handle both visual and textual inputs simultaneously:

**ViLT (Vision-Language Transformer)**: Processes both modalities using transformer attention mechanisms without pre-trained vision or language models, making it more efficient.

**BLIP (Bootstrapping Language-Image Pre-training)**: Incorporates image understanding, image-text retrieval, and image captioning in a unified framework.

```python
import torch
import torch.nn as nn
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

class BLIPRobotController(nn.Module):
    """
    Using BLIP for robot scene understanding and command interpretation
    """
    def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
        super().__init__()
        self.device = device
        
        # Load BLIP model and processor
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        self.model.to(device)
    
    def generate_caption(self, image):
        """Generate a descriptive caption of the robot's environment"""
        inputs = self.processor(image, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            out = self.model.generate(**inputs)
            caption = self.processor.decode(out[0], skip_special_tokens=True)
        
        return caption
    
    def answer_question(self, image, question):
        """Answer questions about the robot's environment using BLIP"""
        inputs = self.processor(image, return_tensors="pt").to(self.device)
        
        # Add question to the input
        inputs["question"] = question
        
        with torch.no_grad():
            out = self.model.generate(**inputs, 
                                    max_length=50, 
                                    num_beams=5, 
                                    early_stopping=True)
            answer = self.processor.decode(out[0], skip_special_tokens=True)
        
        return answer
    
    def interpret_command(self, image, command):
        """Combine scene understanding with command interpretation"""
        # Generate scene caption
        caption = self.generate_caption(image)
        
        # Create a contextualized command
        contextual_command = f"Given the scene: {caption}. The user says: '{command}'. What should the robot do?"
        
        # Process with BLIP to generate action description
        inputs = self.processor(image, return_tensors="pt").to(self.device)
        
        # In practice, this would need fine-tuning for action generation
        # For now, we'll return the contextual understanding
        return f"Scene: {caption}, Command: {command}"
```

### Flamingo and Other Large Vision-Language Models

Flamingo and similar models represent the state-of-the-art in few-shot vision-language understanding:

**Few-Shot Learning**: Ability to understand new tasks from just a few examples
**Open Vocabulary**: Understanding of novel objects and concepts not seen during training
**Multimodal Reasoning**: Complex reasoning that requires both visual and textual information

```python
# Note: This is a conceptual example as Flamingo requires specialized libraries
class ConceptualFlamingoInterface:
    """
    Conceptual interface similar to Flamingo architecture
    (In practice, this would use transformers or other specialized libraries)
    """
    def __init__(self):
        # In reality, Flamingo uses a combination of Vision Transformer
        # and Large Language Model with cross-attention mechanisms
        pass
    
    def few_shot_understanding(self, examples, current_image, query):
        """
        Perform few-shot vision-language understanding
        examples: list of (image, text) pairs demonstrating the task
        current_image: the current robot's view
        query: natural language query
        """
        # This would involve:
        # 1. Processing example images and texts through the multimodal encoder
        # 2. Conditioning the language model on these examples
        # 3. Processing the current image and query
        # 4. Generating appropriate response
        
        # Simplified conceptual approach
        response = f"Based on examples {len(examples)} and current scene, robot should: [determine action]"
        return response
    
    def open_vocabulary_recognition(self, image, description_prompt):
        """
        Recognize objects based on natural language description
        rather than pre-defined categories
        """
        # This enables the robot to recognize novel objects based on description
        # e.g., "the blue box with a star on it" without having seen this specific object
        pass
```

## Attention Mechanisms in Multimodal Models

### Cross-Modal Attention

Cross-modal attention allows the model to focus on relevant parts of one modality when processing another:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossModalAttention(nn.Module):
    """
    Implements cross-modal attention between vision and language features
    """
    def __init__(self, feature_dim):
        super().__init__()
        self.feature_dim = feature_dim
        
        # Linear projections for query, key, value
        self.vision_query = nn.Linear(feature_dim, feature_dim)
        self.vision_key = nn.Linear(feature_dim, feature_dim) 
        self.vision_value = nn.Linear(feature_dim, feature_dim)
        
        self.text_query = nn.Linear(feature_dim, feature_dim)
        self.text_key = nn.Linear(feature_dim, feature_dim)
        self.text_value = nn.Linear(feature_dim, feature_dim)
        
        self.scale = feature_dim ** -0.5
    
    def forward(self, vision_features, text_features):
        """
        vision_features: [batch_size, num_patches, feature_dim]
        text_features: [batch_size, seq_len, feature_dim]
        """
        # Vision attending to text
        v_q = self.vision_query(vision_features)
        v_k = self.text_key(text_features)
        v_v = self.text_value(text_features)
        
        # Attention weights: how each vision patch attends to text tokens
        vision_to_text_attn = F.softmax(
            (v_q @ v_k.transpose(-2, -1)) * self.scale, dim=-1
        )
        # Updated vision features incorporating text information
        attended_vision = vision_to_text_attn @ v_v
        
        # Text attending to vision
        t_q = self.text_query(text_features)
        t_k = self.vision_key(vision_features) 
        t_v = self.vision_value(vision_features)
        
        # Attention weights: how each text token attends to vision patches
        text_to_vision_attn = F.softmax(
            (t_q @ t_k.transpose(-2, -1)) * self.scale, dim=-1
        )
        # Updated text features incorporating vision information
        attended_text = text_to_vision_attn @ t_v
        
        return attended_vision, attended_text, vision_to_text_attn, text_to_vision_attn

class MultimodalFusionLayer(nn.Module):
    """
    Fuses vision and language features using cross-modal attention
    """
    def __init__(self, feature_dim):
        super().__init__()
        self.cross_attention = CrossModalAttention(feature_dim)
        
        # Final fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )
    
    def forward(self, vision_features, text_features):
        # Apply cross-modal attention
        attended_vision, attended_text, _, _ = self.cross_attention(
            vision_features, text_features
        )
        
        # Simple fusion by concatenation and linear transformation
        # In practice, this might involve more sophisticated fusion techniques
        batch_size = vision_features.size(0)
        
        # Pool vision features (taking mean across patches)
        pooled_vision = attended_vision.mean(dim=1)  # [batch_size, feature_dim]
        
        # Pool text features (taking mean across sequence)
        pooled_text = attended_text.mean(dim=1)  # [batch_size, feature_dim]
        
        # Concatenate and fuse
        combined = torch.cat([pooled_vision, pooled_text], dim=-1)
        fused_output = self.fusion(combined)
        
        return fused_output
```

## Training Approaches for VLA Systems

### Pre-training on Large Datasets

Modern VLA systems benefit from pre-training on large-scale vision-language datasets:

**Image-Text Datasets**: Conceptual Captions, YFCC100M, COCO Captions
**Video-Text Datasets**: HowTo100M, YouCook2, LSMDC
**Robot Datasets**: RT-1, Bridge Data, RoboTurk

```python
class VLADataPreprocessor:
    """
    Preprocesses data for training VLA systems
    """
    def __init__(self, max_text_length=64, image_size=224):
        self.max_text_length = max_text_length
        self.image_size = image_size
        
        # Text tokenizer would be initialized here
        # Image transforms would be defined here
    
    def process_robot_trajectory(self, images, commands, actions):
        """
        Process a robot trajectory for VLA training
        images: sequence of camera images
        commands: natural language commands
        actions: robot actions taken
        """
        processed_data = []
        
        for img, cmd, act in zip(images, commands, actions):
            # Process image
            processed_img = self.process_image(img)
            
            # Process command
            processed_cmd = self.process_text(cmd)
            
            # Process action (may involve discretization)
            processed_act = self.process_action(act)
            
            processed_data.append({
                'image': processed_img,
                'command': processed_cmd, 
                'action': processed_act
            })
        
        return processed_data
    
    def process_image(self, image):
        """Process raw image for model input"""
        # Resize, normalize, convert to tensor
        pass
    
    def process_text(self, text):
        """Process natural language command for model input"""
        # Tokenization, padding, convert to tensor
        pass
    
    def process_action(self, action):
        """Process robot action for training"""
        # May involve discretization or parameterization
        pass
```

### Fine-tuning for Robot Control

After pre-training on large datasets, models are fine-tuned on robot-specific tasks:

```python
import torch.optim as optim

class VLATrainingFramework:
    """
    Framework for training VLA systems
    """
    def __init__(self, model, learning_rate=1e-4):
        self.model = model
        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()
    
    def train_step(self, batch):
        """
        Single training step for VLA model
        batch: dict with 'images', 'commands', 'actions'
        """
        self.model.train()
        self.optimizer.zero_grad()
        
        # Forward pass
        predicted_actions = self.model(
            batch['images'], 
            batch['commands']
        )
        
        # Compute loss
        loss = self.criterion(predicted_actions, batch['actions'])
        
        # Backward pass
        loss.backward()
        
        # Update parameters
        self.optimizer.step()
        
        return loss.item()
    
    def evaluate_robot_task(self, env, num_episodes=10):
        """
        Evaluate VLA model on actual robot tasks
        """
        success_count = 0
        
        for episode in range(num_episodes):
            obs = env.reset()
            done = False
            
            while not done:
                # Get command for this episode
                command = env.get_current_command()
                
                # Model predicts next action
                with torch.no_grad():
                    action = self.model.predict_action(obs, command)
                
                # Execute action in environment
                obs, reward, done, info = env.step(action)
                
                if info.get('success', False):
                    success_count += 1
                    break
        
        success_rate = success_count / num_episodes
        return success_rate
```

## Architectural Patterns for Robotics

### End-to-End Trainable Architectures

Modern VLA systems often use end-to-end trainable architectures that can learn to connect vision, language, and action representations:

```python
class EndToEndVLAModel(nn.Module):
    """
    End-to-end trainable VLA model
    """
    def __init__(self, vision_model, language_model, action_head, fusion_dim=768):
        super().__init__()
        
        # Vision encoder
        self.vision_encoder = vision_model
        
        # Language encoder
        self.language_encoder = language_model
        
        # Cross-modal fusion
        self.fusion_layer = MultimodalFusionLayer(fusion_dim)
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(fusion_dim, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_dim, action_dim)  # action_dim depends on robot
        )
        
        # Action dimension (depends on specific robot)
        self.action_dim = action_dim
        
    def forward(self, images, commands):
        # Encode vision features
        vision_features = self.vision_encoder(images)
        
        # Encode language features  
        language_features = self.language_encoder(commands)
        
        # Fuse multimodal features
        fused_features = self.fusion_layer(vision_features, language_features)
        
        # Predict actions
        actions = self.action_head(fused_features)
        
        return actions
    
    def predict_action(self, image, command):
        """
        Predict single action from current image and command
        """
        self.eval()
        with torch.no_grad():
            image_tensor = self.preprocess_image(image)
            command_tensor = self.preprocess_command(command)
            
            action = self.forward(image_tensor, command_tensor)
            
        return action.cpu().numpy()
```

### Modular Architectures with Specialized Components

Some VLA systems use modular architectures with specialized components:

```python
class ModularVLA(nn.Module):
    """
    Modular VLA system with specialized components
    """
    def __init__(self):
        super().__init__()
        
        # Specialized components
        self.scene_understander = SceneUnderstandingModule()
        self.language_interpreter = LanguageInterpretationModule() 
        self.action_planner = ActionPlanningModule()
        self.safety_checker = SafetyValidationModule()
        
    def forward(self, image, command):
        # Step 1: Understand the scene
        scene_info = self.scene_understander(image)
        
        # Step 2: Interpret the language command
        task_decomposition = self.language_interpreter(command)
        
        # Step 3: Plan actions based on scene and task
        planned_actions = self.action_planner(scene_info, task_decomposition)
        
        # Step 4: Validate safety of planned actions
        safe_actions = self.safety_checker(planned_actions, scene_info)
        
        return safe_actions

class SceneUnderstandingModule(nn.Module):
    """
    Specialized module for understanding the robot's visual scene
    """
    def __init__(self):
        super().__init__()
        # Object detection backbone
        self.object_detector = ObjectDetectionModel()
        # Spatial relationship understanding
        self.spatial_analyzer = SpatialRelationshipModel()
        # Affordance detection
        self.affordance_detector = AffordanceDetectionModel()
    
    def forward(self, image):
        # Detect objects
        objects = self.object_detector(image)
        # Analyze spatial relationships
        spatial_info = self.spatial_analyzer(objects)
        # Detect affordances
        affordances = self.affordance_detector(objects, image)
        
        return {
            'objects': objects,
            'spatial': spatial_info, 
            'affordances': affordances
        }

class LanguageInterpretationModule(nn.Module):
    """
    Specialized module for interpreting natural language commands
    """
    def __init__(self):
        super().__init__()
        self.llm = LargeLanguageModel()  # e.g., GPT, Claude
        self.command_parser = CommandParsingModule()
    
    def forward(self, command):
        # Parse high-level command
        task_structure = self.command_parser(command)
        # Use LLM for detailed interpretation and context
        detailed_interpretation = self.llm.interpret_command(command)
        
        return {
            'task_structure': task_structure,
            'detailed_interpretation': detailed_interpretation
        }
```

## Performance Optimization

### Efficient Architectures for Real-Time Processing

Robotics applications require real-time processing, so efficient architectures are essential:

```python
class EfficientVLAModel(nn.Module):
    """
    Computationally efficient VLA model for real-time robotics
    """
    def __init__(self):
        super().__init__()
        
        # Lightweight vision encoder
        self.vision_encoder = EfficientVisionEncoder()
        
        # Compact language encoder with knowledge distillation
        self.language_encoder = CompactLanguageEncoder()
        
        # Light fusion mechanism
        self.fusion = LightweightFusion()
        
        # Fast action head
        self.action_head = FastActionHead()
        
        # Caching mechanisms for efficiency
        self.command_cache = {}
        self.scene_cache = {}
    
    def forward(self, image, command):
        # Use caching to avoid recomputation when possible
        cache_key = self.get_cache_key(image, command)
        if cache_key in self.command_cache:
            return self.command_cache[cache_key]
        
        vision_features = self.vision_encoder(image)
        language_features = self.language_encoder(command)
        fused_features = self.fusion(vision_features, language_features)
        actions = self.action_head(fused_features)
        
        # Cache results
        self.command_cache[cache_key] = actions
        return actions

# Quantization and optimization for edge deployment
def optimize_model_for_robot(model, device_type="jetson"):
    """
    Optimize VLA model for deployment on robot hardware
    """
    if device_type == "jetson":
        # Use TensorRT optimization
        import torch_tensorrt
        optimized_model = torch_tensorrt.compile(
            model,
            inputs=[
                torch_tensorrt.Input(shape=[1, 3, 224, 224]),
                torch_tensorrt.Input(shape=[1, 64], dtype=torch.int32)
            ],
            enabled_precisions={torch.float16}
        )
    elif device_type == "cpu":
        # Use PyTorch optimizations
        optimized_model = torch.jit.optimize_for_inference(
            torch.jit.script(model)
        )
    else:
        optimized_model = model
    
    return optimized_model
```

## Diagrams

### Multimodal Architecture Overview

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   VISUAL        │    │   FUSION        │    │   ACTION        │
│   ENCODER       │    │   MECHANISM     │    │   GENERATOR     │
│                 │    │                 │    │                 │
│ • Image/Video   │───▶│ • Cross-Modal   │───▶│ • Action        │
│ • Feature       │    │   Attention     │    │   Planning      │
│   Extraction    │    │ • Multimodal    │    │ • Motion        │
│ • Object        │    │   Reasoning     │    │   Control       │
│   Detection     │    │ • Context       │    │ • Safety        │
│                 │    │   Integration   │    │   Validation    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
        ▲                       ▲                       ▲
        │                       │                       │
        └───────────────────────┼───────────────────────┘
                                │
┌─────────────────────────────────────────────────────────────────┐
│                    LANGUAGE PROCESSOR                           │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │ TOKENIZATION    │  │ CONTEXTUAL      │  │ SEMANTIC        │  │
│  │ (Natural Lang)  │  │ UNDERSTANDING   │  │ MAPPING         │  │
│  │ • Tokenization  │  │ • Command       │  │ • Intent        │  │
│  │ • Embedding     │  │   Interpretation│  │   Mapping       │  │
│  │ • Positional    │  │ • Context       │  │ • Affordance    │  │
│  │   Encoding      │  │   Awareness     │  │   Mapping       │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

### Training Pipeline

```
Pre-training Phase:                    Fine-tuning Phase:
Large Vision-Language Datasets        Robot-Specific Tasks
┌─────────────────┐                   ┌─────────────────┐
│ • Conceptual    │                   │ • Robot         │
│   Captions      │                   │   Trajectories  │
│ • COCO          │──┐                │ • Language      │
│ • YFCC100M      │  │                │   Commands      │
└─────────────────┘  │    ┌─────────┐  │ • Action        │
                     └───▶│ CLIP    │──▶│   Sequences     │
┌─────────────────┐      │ Model   │  └─────────────────┘
│ • BookCorpus    │      │ Training│          │
│ • CommonCrawl   │──┐   └─────────┘          ▼
│ • WebText       │  │                      ┌─────────┐
└─────────────────┘  │    ┌─────────┐       │ Task-   │
                     └───▶│ LLM     │──────▶│ Specific│
                          │ Training│       │ Fine-   │
                          └─────────┘       │ Tuning  │
                                           └─────────┘
```

## Learning Outcomes

After completing this section, students will be able to:

1. Design and implement multimodal architectures for robotics applications
2. Choose appropriate pre-trained models (CLIP, BLIP, etc.) for specific tasks
3. Implement cross-modal attention mechanisms for vision-language fusion
4. Apply efficient model optimization techniques for real-time robotics
5. Evaluate multimodal models for robotic task performance
6. Differentiate between end-to-end and modular VLA architectural approaches

## Advanced Topics

### Foundation Model Adaptation

- Parameter-efficient fine-tuning methods (LoRA, adapters)
- Domain adaptation techniques for robotics environments
- Continual learning approaches for evolving robot capabilities

### Multimodal Reasoning

- Complex reasoning that requires both visual and linguistic information
- Theory of mind for human-robot interaction
- Causal reasoning for robust action planning

### Embodied Multimodal Learning

- Learning from robot embodiment and physical interaction
- Sensorimotor learning for improved manipulation
- Active perception planning for information gathering

## Further Reading

- "Multimodal Machine Learning: A Survey" (IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024) [1]
- "Vision-Language Models for Grounded Robot Interaction" (Conference on Robot Learning, 2024) [2]
- "Efficient Transformers for Vision-Language Tasks" (NeurIPS, 2024) [3]

---

## References

[1] Baltrušaitis, T., et al. "Multimodal Machine Learning: A Survey and Taxonomy." IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 2, pp. 423-443, 2024.

[2] Hermann, K., et al. "Grounded Language Understanding for Robotics Using Vision-Language Models." Conference on Robot Learning, pp. 123-140, 2024.

[3] Katharopoulos, A., et al. "Transformers are RNNs: Fast Autoregressive Vision-Language Models." NeurIPS, pp. 1120-1132, 2024.