---
sidebar_position: 6
title: "Sensor Systems for Physical AI"
description: "Comprehensive overview of sensor systems in robotics, including vision, range, proprioceptive, and tactile sensors, with practical examples of sensor fusion and ROS2 integration"
chapter_number: 1
module_number: 6
---

# 1.4 Sensor Systems for Physical AI

## سیکھنے کے مقاصد

By the end of this section, you will be able to:

- Identify and categorize different types of sensors used in physical AI systems
- Understand the principles and trade-offs of vision, range, proprioceptive, and tactile sensors
- Implement sensor data processing and fusion algorithms
- Integrate sensors with ROS2 for robotic applications
- Apply calibration and noise handling techniques to improve sensor reliability
- Design sensor processing pipelines for real-world robotics applications

## Introduction to Sensor Systems in Robotics

Sensor systems form the perception foundation of physical AI, bridging the gap between the physical world and computational intelligence. While digital AI systems process pre-digitized data, physical AI must first transduce physical phenomena—light, sound, force, motion—into electrical signals that can be interpreted by algorithms. The quality, diversity, and integration of these sensors fundamentally determine a robot's ability to perceive, understand, and interact with its environment.

Modern humanoid robots employ dozens of sensors across multiple modalities, creating a rich sensory tapestry analogous to human perception. These sensors operate at different frequencies, resolutions, and latencies, requiring sophisticated fusion algorithms to create coherent world models. Understanding sensor characteristics, limitations, and integration strategies is essential for building robust physical AI systems.

## Sensor Categories and Characteristics

### Vision Sensors

Vision sensors provide dense, high-dimensional information about the environment, making them the primary perception modality for most modern robots.

**RGB Cameras** capture color images at high resolution and frame rates. Modern cameras provide resolutions from VGA (640×480) to 4K (3840×2160) at frame rates ranging from 30 to 120 Hz. However, monocular cameras lack direct depth information and struggle in poor lighting conditions.

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import numpy as np

class VisionProcessor(Node):
    def __init__(self):
        super().__init__('vision_processor')
        self.bridge = CvBridge()

        # Subscribe to camera feed
        self.subscription = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        # Publisher for processed images
        self.publisher = self.create_publisher(
            Image,
            '/camera/image_processed',
            10
        )

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV format
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Perform image processing
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        edges = cv2.Canny(blurred, 50, 150)

        # Detect features for visual odometry
        corners = cv2.goodFeaturesToTrack(
            gray,
            maxCorners=100,
            qualityLevel=0.01,
            minDistance=10
        )

        # Visualize detected features
        if corners is not None:
            for corner in corners:
                x, y = corner.ravel()
                cv2.circle(cv_image, (int(x), int(y)), 3, (0, 255, 0), -1)

        # Convert back to ROS message and publish
        processed_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')
        processed_msg.header = msg.header
        self.publisher.publish(processed_msg)

def main(args=None):
    rclpy.init(args=args)
    node = VisionProcessor()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Depth Cameras** (RGB-D sensors) combine color and depth information, providing 3D perception crucial for navigation and manipulation. Technologies include:

- **Structured Light**: Projects infrared patterns and triangulates depth (Intel RealSense D400 series, range: 0.1-10m, accuracy: ±2%)
- **Time-of-Flight (ToF)**: Measures light travel time (Microsoft Azure Kinect, range: 0.5-5m, accuracy: ±1%)
- **Active Stereo**: Uses infrared projector with stereo cameras (Intel RealSense D435, combines both approaches)

**Stereo Cameras** use binocular vision to compute depth through triangulation, mimicking human depth perception. They require calibration and perform poorly in textureless regions but work in any lighting condition where features are visible.

### Range Sensors

Range sensors measure distance to objects using various physical principles, providing complementary information to vision systems.

**LIDAR (Light Detection and Ranging)** emits laser pulses and measures return time, creating precise 3D point clouds. Modern LIDARs offer:

- **Scanning LIDAR**: Rotating lasers (Velodyne, Ouster) provide 360° coverage at ranges up to 200m with centimeter accuracy
- **Solid-State LIDAR**: No moving parts (Luminar, Livox) with selective scanning and lower cost
- **Flash LIDAR**: Illuminates entire scene simultaneously with faster update rates but shorter range

**Ultrasonic Sensors** emit high-frequency sound waves (40 kHz typical) and measure echo return time. They excel at close-range obstacle detection (0.02-4m), work in any lighting, but suffer from specular reflection on smooth surfaces and have wide beam patterns that reduce angular resolution.

**Radar Sensors** use radio waves for long-range detection (up to 300m) and velocity measurement via Doppler shift. Automotive radar (77 GHz) provides all-weather operation but lower resolution than LIDAR.

### Proprioceptive Sensors

Proprioceptive sensors measure the robot's internal state, providing self-awareness analogous to human proprioception.

**Encoders** measure joint angles and wheel rotations. Incremental encoders provide relative position through pulse counting, while absolute encoders give direct position readings without initialization. High-resolution encoders (4096 counts per revolution) enable precise position control.

**Inertial Measurement Units (IMU)** combine accelerometers and gyroscopes (and often magnetometers) to measure acceleration, angular velocity, and orientation. MEMS IMUs in modern robots provide:

- 3-axis accelerometer (measuring specific force up to ±16g)
- 3-axis gyroscope (measuring angular velocity up to ±2000°/s)
- Update rates: 100-1000 Hz
- Drift: 0.1-10°/hour depending on grade

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from geometry_msgs.msg import Quaternion
import numpy as np

class IMUProcessor(Node):
    def __init__(self):
        super().__init__('imu_processor')

        self.subscription = self.create_subscription(
            Imu,
            '/imu/data_raw',
            self.imu_callback,
            10
        )

        # Complementary filter parameters
        self.alpha = 0.98  # Trust gyro more for short term
        self.dt = 0.01     # 100 Hz sensor rate

        # State estimation
        self.roll = 0.0
        self.pitch = 0.0
        self.yaw = 0.0

    def imu_callback(self, msg):
        # Extract sensor data
        ax = msg.linear_acceleration.x
        ay = msg.linear_acceleration.y
        az = msg.linear_acceleration.z

        gx = msg.angular_velocity.x
        gy = msg.angular_velocity.y
        gz = msg.angular_velocity.z

        # Compute accelerometer-based angles (noisy but drift-free)
        accel_roll = np.arctan2(ay, az)
        accel_pitch = np.arctan2(-ax, np.sqrt(ay**2 + az**2))

        # Integrate gyroscope (smooth but drifts)
        gyro_roll = self.roll + gx * self.dt
        gyro_pitch = self.pitch + gy * self.dt
        gyro_yaw = self.yaw + gz * self.dt

        # Complementary filter: blend short-term gyro with long-term accel
        self.roll = self.alpha * gyro_roll + (1 - self.alpha) * accel_roll
        self.pitch = self.alpha * gyro_pitch + (1 - self.alpha) * accel_pitch
        self.yaw = gyro_yaw  # No accelerometer reference for yaw

        self.get_logger().info(
            f'Orientation - Roll: {np.degrees(self.roll):.2f}°, '
            f'Pitch: {np.degrees(self.pitch):.2f}°, '
            f'Yaw: {np.degrees(self.yaw):.2f}°'
        )

def main(args=None):
    rclpy.init(args=args)
    node = IMUProcessor()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Force/Torque Sensors** measure interaction forces and torques, essential for manipulation and compliant control. Six-axis force/torque sensors at the wrist provide full wrench information (3 forces, 3 torques) with resolution down to 0.01N and 0.001Nm.

### Tactile Sensors

Tactile sensors provide contact information crucial for dexterous manipulation and safe human-robot interaction.

**Force-Sensitive Resistors (FSR)** change resistance under applied force, providing simple binary or scalar contact detection at low cost but with limited spatial resolution and hysteresis.

**Capacitive Sensors** detect contact through capacitance changes, offering better linearity and dynamic range than FSRs.

**Optical Tactile Sensors** (e.g., GelSight, DIGIT) use cameras to observe deformation of a soft surface under contact, providing high-resolution tactile images that capture geometry, forces, and slip with unprecedented detail.

## Sensor Comparison and Selection

The following table summarizes key characteristics of major sensor types to guide system design:

| Sensor Type | Range | Accuracy | Update Rate | Cost | Environment Sensitivity | Primary Use Cases |
|-------------|-------|----------|-------------|------|------------------------|-------------------|
| RGB Camera | Unlimited | N/A (2D) | 30-120 Hz | $ | Lighting, weather | Object recognition, visual servoing |
| Depth Camera | 0.1-10m | 1-2% | 30-90 Hz | $$ | Lighting, surfaces | Navigation, manipulation, SLAM |
| Stereo Camera | 0.5-20m | 2-5% | 30-60 Hz | $$ | Textures, lighting | Outdoor navigation, depth estimation |
| 2D LIDAR | 0.1-30m | ±2cm | 10-40 Hz | $$ | Dust, rain | Indoor navigation, mapping |
| 3D LIDAR | 0.5-200m | ±2cm | 10-20 Hz | $$$$ | Weather, reflectivity | Autonomous vehicles, outdoor mapping |
| Ultrasonic | 0.02-4m | ±1cm | 50 Hz | $ | Surface angle, texture | Obstacle avoidance, parking assistance |
| Radar | 1-300m | ±10cm | 10-20 Hz | $$$ | All-weather | Long-range detection, velocity measurement |
| Encoder | N/A | 0.1-0.01° | 1000+ Hz | $ | None | Joint control, odometry |
| IMU | N/A | 0.1-10°/hr drift | 100-1000 Hz | $-$$$ | Magnetic interference | Orientation, motion tracking |
| F/T Sensor | N/A | 0.01-0.1N | 100-1000 Hz | $$$ | None | Manipulation, contact control |
| Tactile | Contact only | 0.1-1N | 100-1000 Hz | $$-$$$$ | Surface properties | Grasping, texture sensing |

**Cost scale**: `$ (<$100), $$ ($100–$1000), $$$ ($1000–$10,000), $$$$ (>$10,000)`

## Sensor Fusion Principles

No single sensor provides complete, reliable perception. Sensor fusion combines multiple sensor modalities to create robust state estimates that exceed the capabilities of individual sensors. Effective fusion exploits complementary sensor characteristics:

- **Complementary strengths**: Vision provides rich features; IMU provides high-rate motion data
- **Redundancy**: Multiple sensors measure the same quantity with different error characteristics
- **Cross-validation**: Inconsistent sensor readings reveal failures or outliers

**Visual-Inertial Odometry (VIO)** exemplifies sensor fusion by combining camera and IMU data. Cameras provide drift-free but computationally expensive localization, while IMUs provide high-rate motion estimates that drift over time. Their fusion creates accurate, high-frequency pose estimates.

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, Imu
from geometry_msgs.msg import PoseStamped
from cv_bridge import CvBridge
import numpy as np
import cv2
from collections import deque

class VisualInertialOdometry(Node):
    def __init__(self):
        super().__init__('visual_inertial_odometry')
        self.bridge = CvBridge()

        # Subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10
        )
        self.imu_sub = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10
        )

        # Publisher for estimated pose
        self.pose_pub = self.create_publisher(PoseStamped, '/vio/pose', 10)

        # State variables
        self.position = np.zeros(3)
        self.velocity = np.zeros(3)
        self.orientation = np.eye(3)

        # Feature tracking
        self.prev_gray = None
        self.prev_points = None

        # IMU integration
        self.imu_buffer = deque(maxlen=100)
        self.last_imu_time = None

        # Camera calibration (example values - replace with actual calibration)
        self.camera_matrix = np.array([
            [500, 0, 320],
            [0, 500, 240],
            [0, 0, 1]
        ])
        self.focal_length = 500

    def imu_callback(self, msg):
        current_time = self.get_clock().now()

        if self.last_imu_time is not None:
            dt = (current_time - self.last_imu_time).nanoseconds / 1e9

            # Extract angular velocity and linear acceleration
            omega = np.array([
                msg.angular_velocity.x,
                msg.angular_velocity.y,
                msg.angular_velocity.z
            ])

            accel = np.array([
                msg.linear_acceleration.x,
                msg.linear_acceleration.y,
                msg.linear_acceleration.z
            ])

            # Remove gravity (assumes accelerometer measures specific force)
            gravity = np.array([0, 0, -9.81])
            accel_world = self.orientation @ accel + gravity

            # Integrate velocity and position (simple Euler integration)
            self.velocity += accel_world * dt
            self.position += self.velocity * dt

            # Update orientation (simplified - real systems use quaternions)
            omega_skew = np.array([
                [0, -omega[2], omega[1]],
                [omega[2], 0, -omega[0]],
                [-omega[1], omega[0], 0]
            ])
            self.orientation += self.orientation @ omega_skew * dt

            # Store in buffer for visual correction
            self.imu_buffer.append({
                'time': current_time,
                'position': self.position.copy(),
                'velocity': self.velocity.copy()
            })

        self.last_imu_time = current_time

    def image_callback(self, msg):
        # Convert image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)

        if self.prev_gray is None:
            # Initialize feature tracking
            self.prev_points = cv2.goodFeaturesToTrack(
                gray, maxCorners=100, qualityLevel=0.01, minDistance=10
            )
            self.prev_gray = gray
            return

        if self.prev_points is None or len(self.prev_points) == 0:
            self.prev_points = cv2.goodFeaturesToTrack(
                gray, maxCorners=100, qualityLevel=0.01, minDistance=10
            )
            self.prev_gray = gray
            return

        # Track features using optical flow
        curr_points, status, _ = cv2.calcOpticalFlowPyrLK(
            self.prev_gray, gray, self.prev_points, None
        )

        # Select good matches
        good_prev = self.prev_points[status == 1]
        good_curr = curr_points[status == 1]

        if len(good_prev) >= 5:
            # Estimate essential matrix (camera motion)
            E, mask = cv2.findEssentialMat(
                good_curr, good_prev, self.camera_matrix,
                method=cv2.RANSAC, prob=0.999, threshold=1.0
            )

            # Recover rotation and translation
            _, R, t, mask = cv2.recoverPose(
                E, good_curr, good_prev, self.camera_matrix, mask=mask
            )

            # Update position estimate using visual information
            # Scale ambiguity resolved using IMU velocity
            scale = np.linalg.norm(self.velocity) * 0.033  # Assume 30 Hz camera
            visual_translation = (self.orientation @ t.flatten()) * scale

            # Fusion: blend IMU-predicted position with visual correction
            fusion_weight = 0.7  # Trust visual more when available
            self.position = (1 - fusion_weight) * self.position + \
                           fusion_weight * (self.position + visual_translation)

            # Update orientation
            self.orientation = R @ self.orientation

        # Publish estimated pose
        pose_msg = PoseStamped()
        pose_msg.header.stamp = self.get_clock().now().to_msg()
        pose_msg.header.frame_id = 'odom'

        pose_msg.pose.position.x = float(self.position[0])
        pose_msg.pose.position.y = float(self.position[1])
        pose_msg.pose.position.z = float(self.position[2])

        # Convert rotation matrix to quaternion (simplified)
        # Real implementation should use proper conversion
        pose_msg.pose.orientation.w = 1.0
        pose_msg.pose.orientation.x = 0.0
        pose_msg.pose.orientation.y = 0.0
        pose_msg.pose.orientation.z = 0.0

        self.pose_pub.publish(pose_msg)

        # Update for next iteration
        self.prev_gray = gray
        self.prev_points = good_curr.reshape(-1, 1, 2)

def main(args=None):
    rclpy.init(args=args)
    node = VisualInertialOdometry()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

This VIO implementation demonstrates key fusion concepts:

1. **IMU Integration**: High-rate (100+ Hz) propagation of position and orientation using inertial measurements
2. **Visual Correction**: Lower-rate (30 Hz) camera-based updates that correct accumulated drift
3. **Scale Resolution**: IMU velocity resolves the scale ambiguity inherent in monocular visual odometry
4. **Complementary Filtering**: Weighted combination of predictions and measurements based on confidence

Advanced fusion techniques include Extended Kalman Filters (EKF), Unscented Kalman Filters (UKF), and graph-based optimization (factor graphs) which model sensor uncertainties and optimize historical state estimates.

## ROS2 Sensor Integration

ROS2 (Robot Operating System 2) provides standardized interfaces for sensor integration through predefined message types and drivers:

**Key ROS2 Sensor Message Types:**
- `sensor_msgs/Image`: Camera images with encoding and metadata
- `sensor_msgs/PointCloud2`: 3D point clouds from LIDAR and depth cameras
- `sensor_msgs/Imu`: IMU data with orientation, angular velocity, and linear acceleration
- `sensor_msgs/LaserScan`: 2D LIDAR scans
- `sensor_msgs/JointState`: Joint encoder positions and velocities
- `geometry_msgs/WrenchStamped`: Force/torque sensor data

**ROS2 Sensor Integration Pipeline:**

1. **Driver Layer**: Hardware-specific drivers publish raw sensor data (e.g., `realsense2_camera`, `velodyne_driver`)
2. **Processing Layer**: Nodes process and filter sensor data (e.g., `image_proc` for rectification, `pointcloud_to_laserscan` for dimensionality reduction)
3. **Fusion Layer**: Algorithms combine multiple sensors (e.g., `robot_localization` for EKF-based sensor fusion)
4. **Application Layer**: High-level nodes consume processed sensor data for planning and control

**Example: Multi-Sensor ROS2 Launch File**

```python
from launch import LaunchDescription
from launch_ros.actions import Node
from launch.actions import DeclareLaunchArgument
from launch.substitutions import LaunchConfiguration

def generate_launch_description():
    return LaunchDescription([
        # Camera driver
        Node(
            package='realsense2_camera',
            executable='realsense2_camera_node',
            name='camera',
            parameters=[{
                'enable_depth': True,
                'enable_color': True,
                'depth_module.profile': '640x480x30',
                'rgb_camera.profile': '640x480x30',
            }],
            remappings=[
                ('/camera/color/image_raw', '/camera/image_raw'),
                ('/camera/depth/image_rect_raw', '/camera/depth/image')
            ]
        ),

        # IMU driver
        Node(
            package='bno055_driver',
            executable='bno055_driver_node',
            name='imu',
            parameters=[{
                'port': '/dev/ttyUSB0',
                'frame_id': 'imu_link'
            }]
        ),

        # LIDAR driver
        Node(
            package='rplidar_ros',
            executable='rplidar_composition',
            name='lidar',
            parameters=[{
                'serial_port': '/dev/ttyUSB1',
                'frame_id': 'laser_frame',
                'angle_compensate': True
            }]
        ),

        # Depth image to point cloud converter
        Node(
            package='depth_image_proc',
            executable='point_cloud_xyz_node',
            name='depth_to_pointcloud',
            remappings=[
                ('image_rect', '/camera/depth/image'),
                ('camera_info', '/camera/depth/camera_info'),
                ('points', '/camera/depth/points')
            ]
        ),

        # Robot localization - EKF fusion
        Node(
            package='robot_localization',
            executable='ekf_node',
            name='ekf_localization',
            parameters=[{
                'frequency': 30.0,
                'sensor_timeout': 0.1,
                'two_d_mode': False,
                'odom0': '/wheel/odometry',
                'odom0_config': [True, True, True,   # x, y, z
                                False, False, False,  # roll, pitch, yaw
                                True, True, False,    # vx, vy, vz
                                False, False, True,   # vroll, vpitch, vyaw
                                False, False, False], # ax, ay, az
                'imu0': '/imu/data',
                'imu0_config': [False, False, False,
                               True, True, True,     # orientation
                               False, False, False,
                               True, True, True,     # angular velocities
                               True, True, True],    # linear accelerations
            }]
        )
    ])
```

## Calibration and Noise Handling

Accurate sensor data requires calibration to correct systematic errors and noise handling to manage random variations.

### Camera Calibration

Camera calibration estimates intrinsic parameters (focal length, principal point, distortion coefficients) and extrinsic parameters (position and orientation relative to robot base).

**Intrinsic Calibration** using checkerboard patterns:

```python
import cv2
import numpy as np
import glob

def calibrate_camera(image_path_pattern, checkerboard_size=(9, 6)):
    """
    Calibrate camera using checkerboard images.

    Args:
        image_path_pattern: Glob pattern for calibration images
        checkerboard_size: (width, height) in internal corners

    Returns:
        camera_matrix: 3x3 intrinsic matrix
        dist_coeffs: Distortion coefficients
        rvecs, tvecs: Rotation and translation vectors for each image
    """
    # Prepare object points (0,0,0), (1,0,0), (2,0,0), ...
    objp = np.zeros((checkerboard_size[0] * checkerboard_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:checkerboard_size[0],
                            0:checkerboard_size[1]].T.reshape(-1, 2)

    # Arrays to store object points and image points
    objpoints = []  # 3D points in real world space
    imgpoints = []  # 2D points in image plane

    images = glob.glob(image_path_pattern)
    image_size = None

    for fname in images:
        img = cv2.imread(fname)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        if image_size is None:
            image_size = gray.shape[::-1]

        # Find checkerboard corners
        ret, corners = cv2.findChessboardCorners(gray, checkerboard_size, None)

        if ret:
            objpoints.append(objp)

            # Refine corner locations
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
            corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
            imgpoints.append(corners_refined.reshape(-1, 2))

    # Perform calibration
    ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(
        objpoints, imgpoints, image_size, None, None
    )

    # Calculate reprojection error
    total_error = 0
    for i in range(len(objpoints)):
        imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i],
                                          camera_matrix, dist_coeffs)
        error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2) / len(imgpoints2)
        total_error += error

    mean_error = total_error / len(objpoints)
    print(f"Camera calibration complete. Mean reprojection error: {mean_error:.3f} pixels")

    return camera_matrix, dist_coeffs, rvecs, tvecs

# Example usage
if __name__ == '__main__':
    camera_matrix, dist_coeffs, _, _ = calibrate_camera('calibration_images/*.jpg')

    print("\nCamera Matrix:")
    print(camera_matrix)
    print("\nDistortion Coefficients:")
    print(dist_coeffs)

    # Save calibration
    np.savez('camera_calibration.npz',
             camera_matrix=camera_matrix,
             dist_coeffs=dist_coeffs)
```

### Sensor Noise Filtering

Sensor noise can be managed through filtering techniques:

1. **Low-pass filters**: Remove high-frequency noise (e.g., moving average, Butterworth filter)
2. **Median filters**: Robust to outliers and impulse noise
3. **Kalman filters**: Optimal linear filters for systems with Gaussian noise
4. **Outlier rejection**: Remove measurements that deviate significantly from predictions

Most sensor fusion algorithms incorporate noise models explicitly, weighting measurements by their inverse covariance to emphasize reliable sensors.

## Example Sensor Processing Pipeline

A complete sensor processing pipeline for a humanoid robot typically follows this architecture:

**Hardware Layer** → **Driver Layer** → **Preprocessing** → **Fusion** → **World Model** → **Decision Making**

For a humanoid performing navigation and manipulation:

1. **RGB-D cameras** (head-mounted) provide visual features and depth at 30 Hz
2. **IMU** (torso-mounted) provides orientation and acceleration at 200 Hz
3. **Joint encoders** report all joint angles at 1000 Hz
4. **F/T sensors** (wrists and ankles) measure contact forces at 1000 Hz
5. **2D LIDAR** (chest-mounted) provides planar obstacle detection at 10 Hz

**Processing Pipeline:**
- Visual odometry runs at 30 Hz using RGB-D data
- IMU integration runs at 200 Hz for high-rate pose prediction
- EKF fusion combines visual odometry, IMU, and encoder data to estimate full body state at 100 Hz
- Contact forces are low-pass filtered and used for balance control at 1000 Hz
- LIDAR scans are converted to 2D occupancy grid for collision checking

This multi-rate, multi-sensor pipeline enables robust perception for dynamic locomotion and manipulation tasks.

## Key Points Summary

- **Sensor diversity is essential**: Different sensor modalities provide complementary information (vision for features, IMU for dynamics, proprioception for self-state)

- **Sensor fusion overcomes individual limitations**: Combining sensors with different characteristics (camera drift-free but low-rate; IMU high-rate but drifts) produces superior estimates

- **ROS2 provides standardized interfaces**: Predefined message types and driver architectures simplify sensor integration and enable algorithm reuse

- **Calibration is critical**: Systematic sensor errors must be corrected through calibration to achieve accurate measurements

- **Noise handling requires principled approaches**: Filtering, outlier rejection, and probabilistic fusion manage random sensor variations

- **Trade-offs guide sensor selection**: Range, accuracy, update rate, cost, and environmental robustness determine appropriate sensors for each application

- **Multi-rate processing is necessary**: Different sensors operate at different frequencies, requiring hierarchical processing pipelines

- **Sensor placement affects performance**: Strategic positioning on the robot body (cameras in head, IMU in torso, contact sensors in extremities) optimizes perception quality

Understanding these principles and implementing robust sensor processing pipelines forms the foundation for reliable physical AI systems that can perceive and interact with complex, dynamic environments.

---

**Next Steps**: In Chapter 2, we will explore how sensor data feeds into world modeling and state estimation algorithms that enable robots to build coherent representations of their environment for planning and control.