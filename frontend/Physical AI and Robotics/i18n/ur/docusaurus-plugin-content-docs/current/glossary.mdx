---
id: glossary
title: Glossary of Terms
---

# Glossary of Terms

## A

**Affordance**: In robotics, the property of an object that indicates its possible uses. For example, a handle affords grasping, and a flat surface affords placing objects.

**Artificial Intelligence (AI)**: The simulation of human intelligence processes by machines, especially computer systems. In robotics, AI enables robots to perform tasks that typically require human intelligence.

## B

**Behavior Trees**: A mathematical model of task planning hierarchy that can be used to manage complex robot behaviors in a modular way.

## C

**CLIP (Contrastive Language-Image Pre-training)**: A neural network architecture that connects visual and linguistic representations by contrasting matching and non-matching image-text pairs.

**Computer Vision**: A field of artificial intelligence that trains computers to interpret and understand the visual world. Used extensively in robotics for object recognition and scene understanding.

**Cross-Modal Attention**: A mechanism in multimodal AI that allows information from one modality (e.g., text) to guide processing in another modality (e.g., vision).

## D

**Dense Object Detector**: A computer vision system that detects and classifies all objects in an image, providing bounding boxes and class labels.

**Digital Twin**: A virtual representation of a physical robot or robotic system that exists in real-time, allowing engineers to simulate, analyze, and optimize robot behavior before deploying on the physical robot.

**Domain Randomization**: A technique used in simulation to train AI models by randomizing visual properties such as textures, lighting, and colors to improve model generalization to real-world conditions.

**DQN (Deep Q-Network)**: A deep reinforcement learning algorithm that combines Q-learning with deep neural networks to enable agents to learn policies for complex decision-making tasks.

## E

**Embodied AI**: Artificial intelligence systems that are implemented in physical or simulated robots, allowing them to interact with the physical world.

**Embodied Intelligence**: The idea that intelligence emerges from the interaction between an agent and its environment, where the physical form influences cognitive processes.

## F

**Feature Extraction**: The process of identifying and extracting relevant information from raw data for use in machine learning models.

**Foundation Model**: Large-scale machine learning models trained on diverse data that can be adapted to various downstream tasks.

## G

**Gazebo**: A robotics simulation environment that provides accurate physics simulation, realistic sensor simulation, and rendering capabilities.

**GPU (Graphics Processing Unit)**: A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. Used extensively in robotics for deep learning acceleration.

**Grounding**: The process of connecting abstract linguistic or conceptual representations to physical entities in the environment.

## I

**Isaac ROS**: A collection of hardware-accelerated perception, navigation, and manipulation packages that run on NVIDIA Jetson and RTX GPU platforms.

**Isaac Sim**: A high-fidelity simulation environment built on NVIDIA's Omniverse platform for training AI models for robotics applications.

**IMU (Inertial Measurement Unit)**: A device that measures and reports a body's specific force, angular rate, and sometimes the magnetic field surrounding the body.

**Inertial Navigation**: Navigation based on the integration of acceleration measurements from accelerometers and angular rate measurements from gyroscopes.

## K

**Kinematics**: The branch of mechanics concerned with the motion of objects without reference to the forces that cause the motion.

## L

**Large Language Model (LLM)**: A type of language model that is trained on vast amounts of text data and typically contains billions of parameters.

**LIDAR (Light Detection and Ranging)**: A remote sensing method that uses light in the form of a pulsed laser to measure distances to objects.

**Loop Closure**: In SLAM (Simultaneous Localization and Mapping), recognizing when a robot returns to a previously visited location to correct accumulated errors.

## M

**Multimodal AI**: Artificial intelligence systems that can process and integrate information from multiple modalities such as vision, language, audio, and tactile sensing.

**Machine Learning**: A subset of AI that enables systems to automatically learn and improve from experience without being explicitly programmed.

## N

**Natural Language Processing (NLP)**: A field of AI that focuses on the interaction between computers and humans through natural language.

**Neural Radiance Fields (NeRFs)**: A machine learning approach for synthesizing novel views of complex 3D scenes based on a sparse set of 2D images.

## O

**Object Detection**: A computer vision technique for identifying and locating objects within an image or video.

**Ontology**: A formal naming and definition of the types, properties, and interrelationships of entities in a particular domain.

## P

**Perception Pipeline**: A sequence of computer vision and machine learning algorithms that process sensor data to extract meaningful information about the environment.

**Point Cloud**: A collection of data points in a three-dimensional coordinate system, typically produced by 3D scanners or LIDAR sensors.

**Probabilistic Robotics**: A framework for robot perception and action that uses probability theory to represent and reason about uncertainty in the robot's environment.

## Q

**Quality of Service (QoS)**: In ROS 2, configurable policies that define communication behavior including reliability, durability, and history.

## R

**ROS (Robot Operating System)**: A flexible framework for writing robot software that provides a collection of tools, libraries, and conventions for creating robot applications.

**ROS 2**: The second generation of the Robot Operating System, designed to be production-ready with improved security, determinism, and real-time capabilities.

**Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward.

**Robot Operating System (ROS)**: A flexible framework for writing robot software that provides a collection of tools, libraries, and conventions for creating robot applications.

**ROS 2 Navigation (Nav2)**: The navigation stack for ROS 2, providing capabilities for path planning, obstacle avoidance, and autonomous navigation.

## S

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**Sensor Fusion**: The process of combining data from multiple sensors to achieve more accurate and reliable information than possible with any single sensor.

**Sim-to-Real Transfer**: The process of transferring behaviors, policies, or models learned in simulation to real-world robotic systems.

**Spatial Understanding**: The ability to comprehend and navigate the 3D spatial relationships between objects and locations in an environment.

**SDF (Simulation Description Format)**: The native file format for describing simulation environments in Gazebo.

**URDF (Unified Robot Description Format)**: An XML format for representing a robot model including links, joints, and inertial properties.

## T

**Transformer Architecture**: A deep learning model architecture that uses self-attention mechanisms to process sequence data, widely used in NLP and computer vision.

**Task Planning**: The process of determining a sequence of actions to achieve a specific goal.

**Teleoperation**: The remote control of a robot by a human operator, often with sensory feedback to enhance the operator's awareness.

## V

**VLA (Vision-Language-Action)**: Systems that integrate visual perception, natural language understanding, and robot action execution.

**Visual Servoing**: A control strategy that uses visual feedback to control robot motion.

**Virtual Reality (VR)**: A simulated experience that can be similar to or completely different from the real world, used in robotics for training and teleoperation.

**Velocity Controllers**: Controllers that regulate the velocity of robot joints or actuators.

## W

**World Model**: An internal representation of the environment that a robot uses for planning and decision-making.

## X

**Xacro (XML Macros)**: An XML macro language that extends URDF, allowing for more readable and maintainable robot descriptions through parameterization and modular inclusion.

## Y

**Yaw**: The rotation of an object around its vertical axis.

## Z

**Zero-Shot Learning**: The ability of a model to perform tasks it has never been explicitly trained on, using knowledge transferred from related tasks.