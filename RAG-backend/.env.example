# Primary LLM Provider
# Options: anthropic (recommended), openai, openrouter, gemini, xai
# Example configurations:

# ============================================
# RECOMMENDED: Anthropic Claude (Primary)
# ============================================
# Claude Sonnet 4.5 - Best for production RAG systems
LLM_PROVIDER=anthropic
LLM_API_KEY=your_anthropic_api_key_here
LLM_BASE_URL=https://api.anthropic.com/v1
LLM_MODEL=claude-sonnet-4-5-20250929

# Note: For Anthropic, API keys start with 'sk-ant-'
# Get your API key from: https://console.anthropic.com/

# ============================================
# Alternative Option 1: OpenRouter (Free tier available)
# ============================================
# Note: These are the legacy variables; new implementation uses specific OpenRouter variables
# LLM_PROVIDER=openrouter
# LLM_API_KEY=your_openrouter_api_key_here
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=mistralai/devstral-2512:free

# OpenRouter-specific variables (if using OpenRouter)
# OPENROUTER_API_KEY=your_openrouter_api_key_here
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# OPENROUTER_MODEL=mistralai/devstral-2512:free
# OPENROUTER_SITE_URL=your_site_url_here  # Optional, for rankings
# OPENROUTER_SITE_NAME=AI Native Book RAG  # Optional, for rankings

# ============================================
# Alternative Option 2: X.AI Grok
# ============================================
# LLM_PROVIDER=xai
# LLM_API_KEY=your_xai_api_key_here
# LLM_BASE_URL=https://api.x.ai/v1
# LLM_MODEL=grok-beta

# ============================================
# Fallback Providers (OPTIONAL - for high availability)
# ============================================
# Uncomment to enable automatic failover if primary provider fails
# Leave commented out for single-provider setup

# Fallback Provider 1 (OpenAI)
# LLM_PROVIDER_FALLBACK_1=openai
# LLM_API_KEY_FALLBACK_1=your_openai_api_key_here
# LLM_BASE_URL_FALLBACK_1=https://api.openai.com/v1
# LLM_MODEL_FALLBACK_1=gpt-4o-mini

# Fallback Provider 2 (Gemini)
# LLM_PROVIDER_FALLBACK_2=gemini
# LLM_API_KEY_FALLBACK_2=your_gemini_api_key_here
# LLM_BASE_URL_FALLBACK_2=https://generativelanguage.googleapis.com/v1beta/openai/
# LLM_MODEL_FALLBACK_2=gemini-2.0-flash-exp

# ============================================
# Embedding Provider
# ============================================
# Choose ONE of the following for text embeddings:

# Option 1: OpenAI (recommended for Claude users)
OPENAI_API_KEY=your_openai_api_key_here

# Option 2: Cohere (alternative, commented out by default)
# COHERE_API_KEY=your_cohere_api_key_here

# Note: If both are set, Cohere will be used. If neither is set, embeddings will fail.

# ============================================
# Qdrant Vector Database
# ============================================
# Choose ONE of the following configurations:

# Option 1: Local Qdrant (Development - Recommended for Windows)
# Requires Docker: docker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=dev_key_not_required_for_local

# Option 2: Qdrant Cloud (Production)
# Get free cluster at: https://cloud.qdrant.io
# QDRANT_URL=https://your-cluster-id.qdrant.io
# QDRANT_API_KEY=your_qdrant_api_key_here

# WINDOWS TROUBLESHOOTING:
# If you get "WinError 10061: No connection could be made":
# 1. Start Qdrant: docker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant
# 2. Verify: curl http://localhost:6333/health
# 3. Check Docker Desktop is running
# 4. Use "localhost" (not "127.0.0.1") in QDRANT_URL
# 5. Check Windows Firewall isn't blocking port 6333

# Neon Serverless Postgres
NEON_DATABASE_URL=postgresql://user:password@host/database?sslmode=require

# Application Configuration
APP_ENV=development
LOG_LEVEL=INFO

# CORS Configuration (comma-separated origins)
# For development, use "*" to allow all origins
# For production, specify exact origins
CORS_ORIGINS=*

# ===============================
# Agent Architecture Selection (Spec 4 Compliance)
# ===============================

# Use OpenAI Agents SDK (requires OpenAI GPT-4o access)
# - Pros: Native SDK support, official OpenAI patterns
# - Cons: Requires OPENAI_API_KEY, works only with OpenAI models
# - Use Case: Testing OpenAI's native agent framework
use_openai_agents_sdk=False

# Use custom multi-agent system with provider failover (RECOMMENDED)
# - Pros: Works with OpenRouter/Gemini/OpenAI, automatic failover, token-optimized
# - Cons: Custom implementation (but well-tested)
# - Use Case: Production deployment with flexible LLM providers
# - Agents: QueryRouter, Retrieval, Synthesis, ErrorRecovery, AnswerValidation
use_multi_agent_system=True

# NOTE: Only ONE of the above should be True. If both False, falls back to legacy monolithic agent.

# ===============================
# Spec 4 Compliance Verification
# ===============================
# ✅ Frontend ↔ Backend Integration: FastAPI /query endpoint
# ✅ Agent Orchestration: Multi-agent system with handoffs
# ✅ Sub-Agent Utilization: 5 specialized agents with clear roles
# ✅ Reusable Intelligence: Shared tools (embed, retrieve, synthesize)
# ✅ Token Efficiency: 8000 token limit, deduplication, minimal payloads
# ✅ Local Development: localhost:8000 (backend), localhost:3000 (frontend)

# ===============================
# Optional Services
# ===============================

# GitHub Token (for GitHub-based ingestion features)
GITHUB_TOKEN=your_github_token_here

# ===============================
# Rate Limiting Configuration
# ===============================

# Maximum number of concurrent LLM API calls (default: 2)
# Reduce this if you're experiencing 429 errors
LLM_MAX_CONCURRENT=2

# Initial delay for retry attempts in seconds (default: 1.0)
RETRY_INITIAL_DELAY=2.0

# Maximum delay for retry attempts in seconds (default: 60.0)
RETRY_MAX_DELAY=120.0

# Maximum number of retry attempts (default: 3)
MAX_RETRIES=5
