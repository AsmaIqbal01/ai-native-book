[
  {
    "chunk_id": "44eafc75-bdd9-4a2d-a75b-8fd74b97575b",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "# 13-Week Learning Strategy: Physical AI & Humanoid Robotics",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "a11ffd14-6f6e-4a21-8cb8-e590894e1460",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Course Overview\n\nThis 13-week course provides a comprehensive introduction to Physical AI and humanoid robotics, covering the fundamental concepts, technologies, and practical implementations needed to understand and work with modern robotic systems. The course follows a progressive learning approach, building from foundational concepts to advanced multimodal AI systems.",
    "chapter": 0,
    "section": "Course Overview",
    "page": 2,
    "token_count": 59
  },
  {
    "chunk_id": "a8de80fc-cf90-4e91-8fbe-cf2365ef4677",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Prerequisites\n\n- Basic programming knowledge (Python preferred)\n- Understanding of linear algebra and calculus basics\n- Familiarity with Linux command line\n- Interest in robotics and AI technologies",
    "chapter": 0,
    "section": "Prerequisites",
    "page": 3,
    "token_count": 37
  },
  {
    "chunk_id": "1b7fe7dc-47bd-4c9e-a05e-e415ae2ced3c",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Weekly Breakdown",
    "chapter": 0,
    "section": "Weekly Breakdown",
    "page": 4,
    "token_count": 4
  },
  {
    "chunk_id": "f5c49445-1462-4e8a-870e-af23a5ac5be1",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 1: Introduction to Physical AI\n**Duration**: 7 days (Day 1-7)\n\n**Learning Objectives**:\n- Understand the concept of Physical AI vs. traditional digital AI\n- Explore the landscape of humanoid robotics platforms (Tesla Optimus, Figure AI, Boston Dynamics, etc.)\n- Learn about the importance of embodiment in AI systems\n- Study physical laws and their application in robotics\n\n**Content Distribution**:\n- Day 1-2: What is Physical AI? (Theory + Examples)\n- Day 3-4: Embodied Intelligence concepts\n- Day 5-6: Physical laws in robotics\n- Day 7: Review and practical exercises\n\n**Key Concepts**:\n- Embodiment and its importance in AI systems\n- Differences between digital AI and embodied intelligence\n- Physical constraints in robotic systems\n- Sensor systems in humanoid robots\n\n**Resources**:\n- Chapter 1 readings\n- Video lectures on Physical AI concepts\n- Research papers on embodied intelligence\n\n**Self-Assessment**:\n- Quiz on Physical AI concepts\n- Discussion forum participation\n- Reflection essay on Physical vs. Digital AI",
    "chapter": 0,
    "section": "Week 1: Introduction to Physical AI",
    "page": 5,
    "token_count": 231
  },
  {
    "chunk_id": "be239b9a-7d09-41f1-8640-dec33bfbe4fe",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 2: The Robotic Nervous System (ROS 2)\n**Duration**: 7 days (Day 8-14)\n\n**Learning Objectives**:\n- Master ROS 2 architecture and communication patterns\n- Create and run basic ROS 2 nodes\n- Implement publish/subscribe communication\n- Build simple robot controllers using ROS 2\n\n**Content Distribution**:\n- Day 8-9: ROS 2 basics and setup\n- Day 10-11: Creating and running ROS 2 nodes\n- Day 12-13: Topics, services, and actions\n- Day 14: Review and hands-on practice\n\n**Key Concepts**:\n- Computation graph and nodes\n- Topics, services, and actions\n- Quality of Service (QoS) settings\n- ROS 2 launch files and parameters\n\n**Resources**:\n- Chapter 2 readings\n- ROS 2 tutorials\n- Simulation exercises with ROS 2\n\n**Self-Assessment**:\n- ROS 2 node implementation exercise\n- Topic communication quiz\n- Practical ROS 2 troubleshooting tasks",
    "chapter": 0,
    "section": "Week 2: The Robotic Nervous System (ROS 2)",
    "page": 6,
    "token_count": 221
  },
  {
    "chunk_id": "5d2931e2-dd85-4966-9b8e-9e6d4c44c1be",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 3: The Digital Twin (Simulation Environments)\n**Duration**: 7 days (Day 15-21)\n\n**Learning Objectives**:\n- Understand digital twin concepts in robotics\n- Create 3D worlds using Gazebo\n- Integrate Unity for high-fidelity visualization\n- Implement sensor simulation in virtual environments\n\n**Content Distribution**:\n- Day 15-16: Digital twin concepts and importance\n- Day 17-18: Gazebo world creation and simulation\n- Day 19-20: Unity integration for high-fidelity graphics\n- Day 21: Review and simulation exercises\n\n**Key Concepts**:\n- Physics simulation and sensor modeling\n- Environment creation and customization\n- Simulation-to-reality transfer challenges\n- Quality considerations in digital twins\n\n**Resources**:\n- Chapter 3 readings\n- Gazebo and Unity tutorials\n- Simulation environment design exercises\n\n**Self-Assessment**:\n- Custom Gazebo world creation project\n- Simulation parameters optimization\n- Reality gap analysis assignment",
    "chapter": 0,
    "section": "Week 3: The Digital Twin (Simulation Environments)",
    "page": 7,
    "token_count": 209
  },
  {
    "chunk_id": "ef850141-cfd8-4b39-a3a6-b4fff7ecec10",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 4: The AI-Robot Brain (NVIDIA Isaac™)\n**Duration**: 7 days (Day 22-28)\n\n**Learning Objectives**:\n- Understand NVIDIA Isaac technology stack\n- Implement Isaac perception pipelines\n- Integrate Isaac Sim for simulation and training\n- Deploy Isaac applications on real hardware\n\n**Content Distribution**:\n- Day 22-23: Isaac technology overview\n- Day 24-25: Isaac Sim for AI training\n- Day 26-27: Isaac ROS perception pipeline\n- Day 28: Integration and deployment\n\n**Key Concepts**:\n- GPU-accelerated robotics AI\n- Isaac Sim for simulation and training\n- Perception pipelines and sensor fusion\n- Real-time AI inference on robotics platforms\n\n**Resources**:\n- Chapter 4 readings\n- Isaac Sim tutorials\n- GPU-accelerated AI implementation exercises\n\n**Self-Assessment**:\n- Isaac perception pipeline implementation\n- GPU performance optimization\n- Simulation-to-reality transfer project",
    "chapter": 0,
    "section": "Week 4: The AI-Robot Brain (NVIDIA Isaac™)",
    "page": 8,
    "token_count": 203
  },
  {
    "chunk_id": "44fc1c70-ec46-45c5-abe3-c8c97df946ac",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 5: Vision-Language-Action (VLA) Systems\n**Duration**: 7 days (Day 29-35)\n\n**Learning Objectives**:\n- Master multimodal AI concepts in robotics\n- Understand Vision-Language-Action architectures\n- Implement sensor fusion and multimodal pipelines\n- Create ROS 2 + Isaac implementations\n\n**Content Distribution**:\n- Day 29-30: Multimodal AI in Physical AI\n- Day 31-32: Vision-Language-Action architecture\n- Day 33-34: Sensor fusion and multimodal pipelines\n- Day 35: ROS 2 + Isaac implementation\n\n**Key Concepts**:\n- Vision-Language models for robotics\n- Action execution from language commands\n- Multimodal fusion techniques\n- ROS 2 and Isaac integration\n\n**Resources**:\n- Chapter 5 readings\n- VLA system implementation tutorials\n- Multimodal AI exercises\n\n**Self-Assessment**:\n- VLA system implementation project\n- Multimodal pipeline design\n- Performance evaluation of VLA systems",
    "chapter": 0,
    "section": "Week 5: Vision-Language-Action (VLA) Systems",
    "page": 9,
    "token_count": 213
  },
  {
    "chunk_id": "5b393352-c596-4c43-b401-f8162cd02982",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 6: Advanced Perception Systems\n**Duration**: 7 days (Day 36-42)\n\n**Learning Objectives**:\n- Implement advanced computer vision techniques for robotics\n- Develop sensor fusion algorithms\n- Create perception pipelines for humanoid robots\n- Understand depth estimation and 3D perception\n\n**Content Distribution**:\n- Day 36-37: Advanced computer vision for robots\n- Day 38-39: Depth estimation and 3D reconstruction\n- Day 40-41: Sensor fusion algorithms\n- Day 42: Perception pipeline optimization\n\n**Key Concepts**:\n- 3D object detection and tracking\n- Multi-camera systems and stereo vision\n- LiDAR-camera fusion\n- Real-time perception optimization\n\n**Resources**:\n- Advanced perception tutorials\n- Research papers on 3D perception\n- Practical implementation exercises\n\n**Self-Assessment**:\n- 3D object detection implementation\n- Multi-sensor fusion project\n- Performance analysis of perception systems",
    "chapter": 0,
    "section": "Week 6: Advanced Perception Systems",
    "page": 10,
    "token_count": 199
  },
  {
    "chunk_id": "6c58d668-864b-4421-831d-d729d344be96",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 7: Human-Robot Interaction\n**Duration**: 7 days (Day 43-49)\n\n**Learning Objectives**:\n- Design intuitive human-robot interfaces\n- Implement natural language processing for robotics\n- Develop gesture recognition and interpretation systems\n- Create multimodal interaction paradigms\n\n**Content Distribution**:\n- Day 43-44: Natural language processing for robots\n- Day 45-46: Gesture recognition and interpretation\n- Day 47-48: Multimodal interaction paradigms\n- Day 49: Safety and ethics in HRI\n\n**Key Concepts**:\n- Natural language understanding in robotics\n- Social robotics and interaction design\n- Safety considerations in human-robot interaction\n- Ethical considerations in AI-robot systems\n\n**Resources**:\n- HRI research papers\n- Interaction design principles\n- Safety and ethics guidelines\n\n**Self-Assessment**:\n- Natural language interface implementation\n- Gesture recognition system\n- Safety protocol design",
    "chapter": 0,
    "section": "Week 7: Human-Robot Interaction",
    "page": 11,
    "token_count": 199
  },
  {
    "chunk_id": "14c9e329-8b6c-49fa-beec-82be192a2da3",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 8: Navigation and Path Planning\n**Duration**: 7 days (Day 50-56)\n\n**Learning Objectives**:\n- Master robot navigation algorithms\n- Implement SLAM (Simultaneous Localization and Mapping)\n- Create path planning and obstacle avoidance systems\n- Understand navigation in dynamic environments\n\n**Content Distribution**:\n- Day 50-51: Fundamentals of robot navigation\n- Day 52-53: SLAM algorithms and implementation\n- Day 54-55: Path planning and optimization\n- Day 56: Dynamic environment navigation\n\n**Key Concepts**:\n- Global and local path planning\n- A* and Dijkstra algorithms\n- Dynamic obstacle avoidance\n- Multi-robot navigation\n\n**Resources**:\n- Navigation stack tutorials\n- SLAM algorithm implementations\n- Path planning exercises\n\n**Self-Assessment**:\n- Navigation system implementation\n- SLAM project with real sensors\n- Path planning optimization",
    "chapter": 0,
    "section": "Week 8: Navigation and Path Planning",
    "page": 12,
    "token_count": 189
  },
  {
    "chunk_id": "284f3179-2491-410b-a86f-0bef77c8bf5c",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 9: Manipulation and Control\n**Duration**: 7 days (Day 57-63)\n\n**Learning Objectives**:\n- Understand robotic manipulation principles\n- Implement grasping and manipulation algorithms\n- Create control systems for dexterous manipulation\n- Study force control and tactile feedback\n\n**Content Distribution**:\n- Day 57-58: Fundamentals of robotic manipulation\n- Day 59-60: Grasping and grasp planning\n- Day 61-62: Force control and tactile feedback\n- Day 63: Dexterous manipulation systems\n\n**Key Concepts**:\n- Kinematics and inverse kinematics\n- Grasp synthesis and quality metrics\n- Force/torque sensing and control\n- Multi-fingered hand control\n\n**Resources**:\n- Manipulation algorithm tutorials\n- Control system design exercises\n- Research on dexterous manipulation\n\n**Self-Assessment**:\n- Grasping algorithm implementation\n- Force control system design\n- Manipulation task execution",
    "chapter": 0,
    "section": "Week 9: Manipulation and Control",
    "page": 13,
    "token_count": 201
  },
  {
    "chunk_id": "c9d1b7d9-c8f1-4b39-9b82-decc798d28c3",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 10: Learning from Demonstration\n**Duration**: 7 days (Day 64-70)\n\n**Learning Objectives**:\n- Implement imitation learning techniques\n- Develop skill transfer methods\n- Create behavior cloning algorithms\n- Understand few-shot learning for robotics\n\n**Content Distribution**:\n- Day 64-65: Imitation learning fundamentals\n- Day 66-67: Behavior cloning and policy learning\n- Day 68-69: Few-shot learning for robots\n- Day 70: Skill transfer and generalization\n\n**Key Concepts**:\n- Learning from human demonstrations\n- Policy gradient methods for robotics\n- Transfer learning between robots\n- Meta-learning for robotics\n\n**Resources**:\n- Imitation learning tutorials\n- Research papers on robot learning\n- Practical implementation exercises\n\n**Self-Assessment**:\n- Imitation learning implementation\n- Policy learning project\n- Skill transfer experiment",
    "chapter": 0,
    "section": "Week 10: Learning from Demonstration",
    "page": 14,
    "token_count": 185
  },
  {
    "chunk_id": "e7dc1d45-1dd1-449b-907f-2cf1443fc36e",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 11: Reinforcement Learning in Robotics\n**Duration**: 7 days (Day 71-77)\n\n**Learning Objectives**:\n- Understand reinforcement learning for robotics\n- Implement deep RL algorithms for robotic tasks\n- Create reward functions for physical tasks\n- Study sim-to-real transfer in RL\n\n**Content Distribution**:\n- Day 71-72: RL fundamentals for robotics\n- Day 73-74: Deep RL algorithms (DQN, PPO, SAC)\n- Day 75-76: Reward design for physical tasks\n- Day 77: Sim-to-real transfer\n\n**Key Concepts**:\n- Markov Decision Processes in robotics\n- Deep Q-Networks and policy gradients\n- Sparse reward problems\n- Sample efficiency in physical systems\n\n**Resources**:\n- RL for robotics tutorials\n- Deep RL algorithm implementations\n- Research on sim-to-real transfer\n\n**Self-Assessment**:\n- DQN implementation for robotic task\n- Policy optimization project\n- Sim-to-real transfer experiment",
    "chapter": 0,
    "section": "Week 11: Reinforcement Learning in Robotics",
    "page": 15,
    "token_count": 206
  },
  {
    "chunk_id": "fbe83a01-2faf-4f05-bde6-f6fb4c5fa7da",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 12: Safety and Ethics in Physical AI\n**Duration**: 7 days (Day 78-84)\n\n**Learning Objectives**:\n- Understand safety considerations in physical AI\n- Implement safety mechanisms in robotic systems\n- Study ethical implications of humanoid robots\n- Create safety validation protocols\n\n**Content Distribution**:\n- Day 78-79: Safety principles in robotics\n- Day 80-81: Safety mechanisms and validation\n- Day 82-83: Ethics in AI and robotics\n- Day 84: Legal and regulatory considerations\n\n**Key Concepts**:\n- Fail-safe mechanisms\n- Collision avoidance and safe motion planning\n- Ethical AI principles\n- Regulatory compliance for robotics\n\n**Resources**:\n- Safety standard documents\n- Ethics in AI research papers\n- Regulatory guidelines\n\n**Self-Assessment**:\n- Safety system design project\n- Ethical analysis of AI-robot systems\n- Compliance framework development",
    "chapter": 0,
    "section": "Week 12: Safety and Ethics in Physical AI",
    "page": 16,
    "token_count": 189
  },
  {
    "chunk_id": "328e4c50-d831-49d1-a2ea-696e634c7848",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Week 13: Capstone Project and Integration\n**Duration**: 7 days (Day 85-91)\n\n**Learning Objectives**:\n- Integrate all learned concepts into a comprehensive project\n- Implement a complete Physical AI system\n- Demonstrate multimodal capabilities\n- Present and evaluate the final system\n\n**Content Distribution**:\n- Day 85-87: Capstone project planning and design\n- Day 88-90: Implementation and testing\n- Day 91: Presentation and evaluation\n\n**Key Concepts**:\n- System integration and optimization\n- Multimodal AI system deployment\n- Performance evaluation and benchmarking\n- Future research directions\n\n**Resources**:\n- Integration guidelines\n- Project presentation templates\n- Evaluation rubrics\n\n**Self-Assessment**:\n- Complete capstone project implementation\n- System integration and testing\n- Professional presentation and documentation",
    "chapter": 0,
    "section": "Week 13: Capstone Project and Integration",
    "page": 17,
    "token_count": 176
  },
  {
    "chunk_id": "8250ab86-0e94-465d-bdbf-d89e6bd55da6",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Assessment Methods",
    "chapter": 0,
    "section": "Assessment Methods",
    "page": 18,
    "token_count": 3
  },
  {
    "chunk_id": "94fd1868-2e99-4091-be7f-a859495e7512",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Continuous Assessment (60%)\n- Weekly quizzes: 15%\n- Practical assignments: 25%\n- Participation and engagement: 10%\n- Midterm project: 10%",
    "chapter": 0,
    "section": "Continuous Assessment (60%)",
    "page": 19,
    "token_count": 37
  },
  {
    "chunk_id": "a61bb7d0-9ea1-4341-be84-a567ce063e13",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Final Assessment (40%)\n- Capstone project: 30%\n- Final exam: 10%",
    "chapter": 0,
    "section": "Final Assessment (40%)",
    "page": 20,
    "token_count": 22
  },
  {
    "chunk_id": "623d2902-5795-4859-bec4-0307d722a990",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Resources and Tools",
    "chapter": 0,
    "section": "Resources and Tools",
    "page": 21,
    "token_count": 4
  },
  {
    "chunk_id": "f92cdef9-e412-4921-be74-5b7564cc8a2b",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Required Software\n- Ubuntu 20.04 or 22.04 LTS\n- ROS 2 Humble Hawksbill\n- NVIDIA Isaac Sim or Gazebo Garden\n- Unity (Personal Edition)\n- Python 3.8 or higher\n- Git and GitHub account",
    "chapter": 0,
    "section": "Required Software",
    "page": 22,
    "token_count": 56
  },
  {
    "chunk_id": "80105c18-3d14-41be-bed4-fdccb72ada79",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Recommended Hardware\n- NVIDIA GPU (RTX 3070 or equivalent)\n- 16GB RAM minimum (32GB recommended)\n- Multi-core CPU (Intel i7 or AMD Ryzen 7)",
    "chapter": 0,
    "section": "Recommended Hardware",
    "page": 23,
    "token_count": 41
  },
  {
    "chunk_id": "e41378bd-9525-4ebe-a2c3-b97bb40acaa2",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### Additional Resources\n- NVIDIA Isaac ROS documentation\n- ROS 2 official tutorials\n- Research papers and conference proceedings\n- Online communities and forums",
    "chapter": 0,
    "section": "Additional Resources",
    "page": 24,
    "token_count": 29
  },
  {
    "chunk_id": "ed10113d-04e6-4e6f-a4eb-fa15af5ad70e",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Learning Outcomes\n\nUpon successful completion of this 13-week course, students will be able to:\n\n1. Design and implement Physical AI systems with multimodal capabilities\n2. Integrate ROS 2 with NVIDIA Isaac for advanced robotics applications\n3. Create digital twins for robot development and testing\n4. Implement Vision-Language-Action systems for natural human-robot interaction\n5. Apply reinforcement learning techniques to robotic problems\n6. Ensure safety and ethical considerations in Physical AI systems\n7. Evaluate and optimize robot systems for real-world deployment",
    "chapter": 0,
    "section": "Learning Outcomes",
    "page": 25,
    "token_count": 108
  },
  {
    "chunk_id": "cc185a3e-5a5e-4e07-80f2-474304ecd4c5",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Prerequisites Check\n\nBefore starting Week 1, students should be able to:\n- Write basic Python scripts\n- Navigate Linux command line\n- Understand basic concepts of linear algebra\n- Have access to appropriate hardware or cloud computing resources",
    "chapter": 0,
    "section": "Prerequisites Check",
    "page": 26,
    "token_count": 47
  },
  {
    "chunk_id": "f01cc07d-7bea-473d-b0ab-0efc69d993db",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "## Supplementary Learning Pathways",
    "chapter": 0,
    "section": "Supplementary Learning Pathways",
    "page": 27,
    "token_count": 5
  },
  {
    "chunk_id": "564db951-86c4-41f8-863e-3b40bd1b719c",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### For Advanced Learners:\n- Additional research paper readings\n- Extra implementation challenges\n- Individual research projects",
    "chapter": 0,
    "section": "For Advanced Learners:",
    "page": 28,
    "token_count": 21
  },
  {
    "chunk_id": "b77b139b-8742-44ee-aa52-7b4f59090abf",
    "doc_id": "4bce1411-5561-46ce-beed-d9b6b263dca2",
    "chunk_text": "### For Beginners:\n- Additional Python programming tutorials\n- Basic Linux command line training\n- Mathematics refreshers\n\nThis 13-week strategy provides a comprehensive roadmap for mastering Physical AI and humanoid robotics, with each week building upon previous knowledge while introducing new concepts and practical implementations. The progression moves from foundational concepts to advanced system integration, preparing students for careers in robotics and AI development.",
    "chapter": 0,
    "section": "For Beginners:",
    "page": 29,
    "token_count": 75
  },
  {
    "chunk_id": "af92b526-4453-4330-9146-b4dfa54f6a8d",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "# AI-Native Robotics: Physical AI & Humanoid Robotics Textbook",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "3e0c8416-8ad6-42b6-aa06-fa5139e8a5b3",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Welcome to AI-Native Robotics\n\nWelcome to the AI-Native Robotics textbook, a comprehensive educational resource that bridges the gap between traditional robotics and the new era of AI-integrated physical systems. This book provides a complete, practical, and theoretically-grounded approach to understanding and implementing Physical AI - the discipline of creating artificial intelligence systems that are fundamentally tied to physical embodiment and interaction with the real world.\n\nIn an era where artificial intelligence has predominantly operated in digital spaces, we are entering a new age where AI systems must understand, interact with, and manipulate the physical world. This textbook focuses on developing the next generation of robots - AI-native robots that think, reason, and act as integrated wholes with natural language interfaces, sophisticated perception, and embodied intelligence.",
    "chapter": 0,
    "section": "Welcome to AI-Native Robotics",
    "page": 2,
    "token_count": 150
  },
  {
    "chunk_id": "ba332fe8-8ef8-4ef5-991d-d1e8223dbe99",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## The Physical AI Revolution",
    "chapter": 0,
    "section": "The Physical AI Revolution",
    "page": 3,
    "token_count": 5
  },
  {
    "chunk_id": "6ccf6627-0304-45dc-a4f2-527862c6798f",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### From Digital AI to Embodied Intelligence\n\nTraditional artificial intelligence has operated primarily in virtual spaces - processing text, images, audio, or numerical data without the constraints and complexities of physical reality. Physical AI represents a fundamental shift, recognizing that true intelligence emerges from the interaction between an embodied agent and its environment.\n\nPhysical AI systems must grapple with:\n- **Physical Laws**: Gravity, friction, momentum, and collision mechanics\n- **Real-World Constraints**: Limited sensors, noisy information, and energy constraints\n- **Safety Requirements**: Ensuring safe interaction with humans and environments\n- **Embodied Cognition**: How the physical form influences cognitive processes\n- **Continuous Interaction**: Maintaining ongoing, real-time engagement with the world",
    "chapter": 0,
    "section": "From Digital AI to Embodied Intelligence",
    "page": 4,
    "token_count": 145
  },
  {
    "chunk_id": "e6a38d43-a626-45f3-8d46-855fb76d2191",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Why Physical AI Matters\n\nThe importance of Physical AI extends beyond technical achievement to address fundamental human needs:\n\n**Collaboration**: Robots that can work alongside humans in shared spaces with natural interaction\n**Accessibility**: Assistive technologies that understand and respond to human needs\n**Safety**: Systems that can operate safely in unpredictable human environments\n**Natural Interaction**: Communication through natural language, gestures, and social cues\n**Adaptability**: Systems that learn and adapt to new situations and environments",
    "chapter": 0,
    "section": "Why Physical AI Matters",
    "page": 5,
    "token_count": 96
  },
  {
    "chunk_id": "a25143fb-2d4c-4ef5-a0ed-4f510f65ed8e",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Book Structure and Philosophy",
    "chapter": 0,
    "section": "Book Structure and Philosophy",
    "page": 6,
    "token_count": 5
  },
  {
    "chunk_id": "f7f374b8-f373-4a42-bec7-290adf7f1c00",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Progressive Learning Approach\n\nThis textbook follows a carefully structured progression that mirrors the development of successful robotics systems:\n\n1. **Foundation**: Understanding the principles of Physical AI and embodied intelligence\n2. **Infrastructure**: Learning communication and control systems (ROS 2)\n3. **Development**: Mastering simulation and digital twin technologies\n4. **Intelligence**: Integrating AI perception and decision-making\n5. **Interaction**: Creating natural human-robot interfaces",
    "chapter": 0,
    "section": "Progressive Learning Approach",
    "page": 7,
    "token_count": 89
  },
  {
    "chunk_id": "75ea0f39-c0f0-4f2f-a7e0-08a6331ef433",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Hands-On Learning\n\nEach chapter combines theoretical understanding with practical implementation, providing:\n- **Real Code Examples**: Working implementations in Python, ROS 2, and modern frameworks\n- **Simulation Environments**: Safe, repeatable testing grounds for learning\n- **Progressive Complexity**: Building from simple concepts to complex integrated systems\n- **Safety-First Approach**: Emphasizing safe development and deployment practices",
    "chapter": 0,
    "section": "Hands-On Learning",
    "page": 8,
    "token_count": 80
  },
  {
    "chunk_id": "cda82430-12d5-48d0-9b88-2b24c3e37094",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Integration Focus\n\nRather than treating robotics technologies in isolation, this book emphasizes integration:\n- **Technology Bridges**: How ROS 2 connects with Isaac, Gazebo, Unity, and other tools\n- **Multimodal Systems**: Combining vision, language, touch, and other sensory modalities\n- **Real-World Applications**: Focusing on practical implementations that work in real environments",
    "chapter": 0,
    "section": "Integration Focus",
    "page": 9,
    "token_count": 78
  },
  {
    "chunk_id": "3eda9d5b-e7ce-4337-8ec5-a6bdfe2b521b",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Target Audience\n\nThis textbook serves multiple audiences:",
    "chapter": 0,
    "section": "Target Audience",
    "page": 10,
    "token_count": 10
  },
  {
    "chunk_id": "51d0f352-697e-484c-90d6-d756ed6fac36",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Students and Researchers\n- Undergraduate and graduate students in robotics, AI, or related fields\n- Researchers entering the field of embodied AI\n- Academics developing robotics curricula",
    "chapter": 0,
    "section": "Students and Researchers",
    "page": 11,
    "token_count": 36
  },
  {
    "chunk_id": "169a7754-67af-4f9c-9006-3d7f176fc482",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Engineers and Practitioners\n- Robotics engineers seeking to integrate AI capabilities\n- AI practitioners moving into physical systems\n- System integrators developing robot applications",
    "chapter": 0,
    "section": "Engineers and Practitioners",
    "page": 12,
    "token_count": 31
  },
  {
    "chunk_id": "b7235142-b225-4c2f-bc5b-afe6039731d6",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Innovators and Entrepreneurs\n- Technology leaders evaluating robotics solutions\n- Startup founders building AI-robotic products\n- Visionaries shaping the future of human-robot interaction",
    "chapter": 0,
    "section": "Innovators and Entrepreneurs",
    "page": 13,
    "token_count": 36
  },
  {
    "chunk_id": "64b0bdf7-a3a1-4576-b0d7-f9d5bf779228",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Prerequisites and Requirements",
    "chapter": 0,
    "section": "Prerequisites and Requirements",
    "page": 14,
    "token_count": 5
  },
  {
    "chunk_id": "98b65bf8-0f5e-4f14-9625-a580f7b6256f",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Technical Background\n- **Programming**: Proficiency in Python and familiarity with object-oriented concepts\n- **Mathematics**: Basic understanding of linear algebra, calculus, and probability\n- **Systems**: General knowledge of computer systems and networking concepts",
    "chapter": 0,
    "section": "Technical Background",
    "page": 15,
    "token_count": 47
  },
  {
    "chunk_id": "34035a58-d219-4efd-8341-d86ababa1fc0",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Hardware and Software Requirements\n- **Platform**: Linux (Ubuntu 20.04/22.04 recommended) or containerized environment\n- **Hardware**: Access to GPU-capable computers for AI training (NVIDIA GPU recommended)\n- **Software**: ROS 2 environment, simulation platforms, and development tools",
    "chapter": 0,
    "section": "Hardware and Software Requirements",
    "page": 16,
    "token_count": 62
  },
  {
    "chunk_id": "93e39db3-2214-4434-92f3-933e6caede43",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Learning Approach\n- **Curiosity**: Willingness to explore and connect concepts across disciplines\n- **Persistence**: Understanding that robotics systems require patience and iterative improvement\n- **Safety Consciousness**: Commitment to developing systems that operate safely",
    "chapter": 0,
    "section": "Learning Approach",
    "page": 17,
    "token_count": 48
  },
  {
    "chunk_id": "0fe7f6ee-d5c4-42bd-8b10-1e2d7ae592bd",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Key Topics Covered",
    "chapter": 0,
    "section": "Key Topics Covered",
    "page": 18,
    "token_count": 4
  },
  {
    "chunk_id": "a504fc11-90b4-4648-9dbc-8ec6daa6fdda",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Core Concepts\n- **Embodied Intelligence**: How physical embodiment shapes cognition and behavior\n- **Physical Laws in Robotics**: Understanding the constraints and opportunities of physical systems\n- **Human-Robot Interaction**: Natural interfaces and social robotics considerations\n- **Safety and Ethics**: Responsible development of physical AI systems",
    "chapter": 0,
    "section": "Core Concepts",
    "page": 19,
    "token_count": 60
  },
  {
    "chunk_id": "87e2fe08-17ae-48ec-b5f0-d5b6dbcb840b",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Technical Systems\n- **ROS 2 Architecture**: Communication, nodes, topics, services, and actions\n- **Simulation Technologies**: Gazebo, Unity, and digital twin methodologies\n- **AI Integration**: NVIDIA Isaac, perception pipelines, and learning systems\n- **Multimodal AI**: Vision-Language-Action systems and sensor fusion",
    "chapter": 0,
    "section": "Technical Systems",
    "page": 20,
    "token_count": 68
  },
  {
    "chunk_id": "b84fbbd6-b0df-457e-861b-337263fba40f",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Practical Implementation\n- **Development Workflows**: From simulation to real robot deployment\n- **Testing and Validation**: Ensuring system reliability and safety\n- **Performance Optimization**: Efficient implementation for real-time systems\n- **Troubleshooting**: Diagnosing and resolving common robotics challenges",
    "chapter": 0,
    "section": "Practical Implementation",
    "page": 21,
    "token_count": 56
  },
  {
    "chunk_id": "2e82e236-e466-4105-a7af-95427a57850b",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Learning Outcomes\n\nUpon completing this textbook, readers will be able to:",
    "chapter": 0,
    "section": "Learning Outcomes",
    "page": 22,
    "token_count": 16
  },
  {
    "chunk_id": "9fed1277-df3b-4e9f-9424-6bad96189f66",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Technical Skills\n- Design and implement ROS 2-based robotic systems with AI integration\n- Create and customize simulation environments for robot development and testing\n- Integrate multiple sensor modalities for robust perception and interaction\n- Implement natural language interfaces for human-robot interaction\n- Develop vision-language-action systems for complex task execution",
    "chapter": 0,
    "section": "Technical Skills",
    "page": 23,
    "token_count": 64
  },
  {
    "chunk_id": "20a9df74-bf11-443c-b93d-fed193ae1f6d",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Conceptual Understanding\n- Explain the principles of embodied intelligence and their importance\n- Analyze the relationship between physical embodiment and cognitive capabilities\n- Evaluate the trade-offs between simulation and real-world deployment\n- Assess the safety and ethical implications of physical AI systems\n- Understand the current state and future directions of robotics technology",
    "chapter": 0,
    "section": "Conceptual Understanding",
    "page": 24,
    "token_count": 63
  },
  {
    "chunk_id": "9506742c-4cf4-4e58-8523-207ab5b1ae76",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Practical Application\n- Plan and execute robotics project development lifecycles\n- Integrate multiple technology stacks and platforms effectively\n- Validate and test robotic systems for safety and reliability\n- Adapt robotic systems to new environments and applications\n- Collaborate effectively in multidisciplinary robotics teams",
    "chapter": 0,
    "section": "Practical Application",
    "page": 25,
    "token_count": 56
  },
  {
    "chunk_id": "bdcc173d-8a83-496a-8627-3b9fcfae8453",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Book Organization",
    "chapter": 0,
    "section": "Book Organization",
    "page": 26,
    "token_count": 3
  },
  {
    "chunk_id": "34be6e1f-92d4-4e04-9b95-5cf4b26e9064",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Chapter 1: Introduction to Physical AI\nEstablishes foundational concepts of Physical AI, embodied intelligence, and the humanoid robotics landscape. Readers will understand the fundamental differences between traditional AI and Physical AI systems, learn about current humanoid platforms (Tesla Optimus, Figure AI, Unitree H1, Agility Digit), and explore the sensor systems essential for physical AI applications.",
    "chapter": 0,
    "section": "Chapter 1: Introduction to Physical AI",
    "page": 27,
    "token_count": 73
  },
  {
    "chunk_id": "26b836f2-4d8a-4b5e-844f-37a7978c5264",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Chapter 2: The Robotic Nervous System (ROS 2)\nCovers the middleware that connects AI systems to physical robots. Readers will learn ROS 2 architecture, create and run nodes, implement communication patterns (topics, services, actions), and bridge Python agents to ROS controllers with practical examples using rclpy and Gazebo Classic simulation.",
    "chapter": 0,
    "section": "Chapter 2: The Robotic Nervous System (ROS 2)",
    "page": 28,
    "token_count": 73
  },
  {
    "chunk_id": "739ced70-1a93-4f15-8348-095f9b1b8a7f",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Chapter 3: The Digital Twin (Gazebo & Unity)\nFocuses on simulation environments that enable safe, cost-effective robot development. Readers will create Gazebo worlds, simulate various sensors (cameras, LIDAR, IMU), understand simulation-to-reality transfer, and explore Unity for high-fidelity visualization with photorealistic rendering for AI training.",
    "chapter": 0,
    "section": "Chapter 3: The Digital Twin (Gazebo & Unity)",
    "page": 29,
    "token_count": 75
  },
  {
    "chunk_id": "78f75708-f0d5-48f7-b12a-a6b0b31119a2",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)\nExplores advanced AI and perception systems with NVIDIA's hardware-accelerated platforms. Readers will understand Isaac Sim for high-fidelity AI training, implement Isaac ROS perception pipelines, and integrate navigation and path planning systems with GPU acceleration.",
    "chapter": 0,
    "section": "Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)",
    "page": 30,
    "token_count": 63
  },
  {
    "chunk_id": "5f4778ae-93e0-40c7-9818-168b02fab35c",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Chapter 5: Vision-Language-Action Systems\nCovers the integration of vision, language, and action for natural human-robot interaction. Readers will understand multimodal AI architectures, implement vision-language grounding, and create systems that respond to natural language commands with appropriate physical actions.",
    "chapter": 0,
    "section": "Chapter 5: Vision-Language-Action Systems",
    "page": 31,
    "token_count": 56
  },
  {
    "chunk_id": "42966bd0-bd2c-49d8-bc63-53cda82b2d39",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Technology Stack\n\nThis textbook utilizes state-of-the-art technologies that represent the current standard in robotics research and development:",
    "chapter": 0,
    "section": "Technology Stack",
    "page": 32,
    "token_count": 23
  },
  {
    "chunk_id": "afce84ac-a770-4601-8b2c-97de39b28661",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### ROS 2 (Robot Operating System 2)\nThe middleware standard for robotics, providing communication, tooling, and integration capabilities for complex robotic systems.",
    "chapter": 0,
    "section": "ROS 2 (Robot Operating System 2)",
    "page": 33,
    "token_count": 31
  },
  {
    "chunk_id": "0e04701c-b79b-42a2-ad3d-11f634f67eb3",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### NVIDIA Isaac\nGPU-accelerated AI toolchains for robotics, including Isaac Sim for simulation, Isaac ROS for perception, and Isaac Apps for reference implementations.",
    "chapter": 0,
    "section": "NVIDIA Isaac",
    "page": 34,
    "token_count": 33
  },
  {
    "chunk_id": "0c0c8e9f-601f-4153-a73c-e1b322e7a28c",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Gazebo Classic/Sim\nIndustry-standard simulation environments that provide accurate physics and sensor simulation for robot development and testing.",
    "chapter": 0,
    "section": "Gazebo Classic/Sim",
    "page": 35,
    "token_count": 25
  },
  {
    "chunk_id": "42dbb859-3483-4ecd-9b95-4dc69952b8da",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Unity\nHigh-fidelity 3D development platform used for photorealistic simulation and advanced visualization for AI training.",
    "chapter": 0,
    "section": "Unity",
    "page": 36,
    "token_count": 24
  },
  {
    "chunk_id": "4e747634-e4e6-4521-903d-a93bbbc1f12a",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Modern AI Frameworks\nIntegration with PyTorch, TensorFlow, and other deep learning frameworks for perception and decision-making systems.",
    "chapter": 0,
    "section": "Modern AI Frameworks",
    "page": 37,
    "token_count": 26
  },
  {
    "chunk_id": "c9045aef-d4e1-45fc-992d-af9ab9359079",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Safety and Ethics\n\nThis textbook places paramount importance on safety and ethical considerations in robotics development:",
    "chapter": 0,
    "section": "Safety and Ethics",
    "page": 38,
    "token_count": 19
  },
  {
    "chunk_id": "932f761a-1926-4b0c-8028-edf24cb088e9",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Safety-First Development\n- Simulation-first approach to minimize risks\n- Safety validation and testing protocols\n- Collision avoidance and emergency stop systems\n- Safe human-robot interaction protocols",
    "chapter": 0,
    "section": "Safety-First Development",
    "page": 39,
    "token_count": 36
  },
  {
    "chunk_id": "3482f994-6a7d-4258-9d56-9f87419b4cce",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Ethical Considerations\n- Privacy and data protection in robotic systems\n- Bias prevention in AI-integrated robots\n- Transparency and explainability requirements\n- Social impact and accessibility considerations",
    "chapter": 0,
    "section": "Ethical Considerations",
    "page": 40,
    "token_count": 37
  },
  {
    "chunk_id": "5062ce6f-7bbc-48a5-a25b-0c379893578f",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Getting Started",
    "chapter": 0,
    "section": "Getting Started",
    "page": 41,
    "token_count": 3
  },
  {
    "chunk_id": "3d78a8dc-bc8c-4b16-8a69-419705a161e3",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Prerequisites Setup\nBefore beginning this textbook, ensure you have:\n1. **Linux Environment**: Ubuntu 20.04 or 22.04 (or appropriate container)\n2. **Hardware**: Computer with NVIDIA GPU (recommended) or sufficient CPU resources\n3. **Development Tools**: Python 3.8+, Git, and build tools\n4. **Internet Access**: For package installation, documentation, and resources",
    "chapter": 0,
    "section": "Prerequisites Setup",
    "page": 42,
    "token_count": 84
  },
  {
    "chunk_id": "2f54d8dc-b410-4ce6-bd9d-142081a4949b",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Learning Path Recommendations\n- **Beginner**: Start with Chapter 1 and progress sequentially through all chapters\n- **Intermediate**: Review Chapter 1, then focus on technology gaps in your knowledge\n- **Advanced**: Use the book as a reference, focusing on integration and advanced topics",
    "chapter": 0,
    "section": "Learning Path Recommendations",
    "page": 43,
    "token_count": 57
  },
  {
    "chunk_id": "3ce81fb7-97dd-4d92-9485-5af805e12368",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "### Practice and Experimentation\n- Run all provided code examples in simulation before real robot deployment\n- Modify examples to understand implementation details\n- Create your own variations and extensions to build practical skills\n- Join robotics communities for additional support and learning",
    "chapter": 0,
    "section": "Practice and Experimentation",
    "page": 44,
    "token_count": 48
  },
  {
    "chunk_id": "955335f0-584c-46c8-b49d-f29a17ecb68c",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## The Future of AI-Native Robotics\n\nAs you progress through this textbook, you'll be learning about technologies and concepts that are shaping the future of robotics. The integration of AI with physical systems is creating new possibilities for robots that can truly collaborate with humans, adapt to changing environments, and perform complex tasks that were previously impossible.\n\nWe're not just learning to program robots; we're learning to create partners that can think, perceive, and act in harmony with physical reality and human needs. This is the promise and challenge of AI-native robotics.",
    "chapter": 0,
    "section": "The Future of AI-Native Robotics",
    "page": 45,
    "token_count": 109
  },
  {
    "chunk_id": "a38dd14b-b21d-4e12-9b65-785aa56b6fa9",
    "doc_id": "f40fabcc-3c89-4034-8ae1-d34100466c3c",
    "chunk_text": "## Conclusion\n\nThe journey through this textbook will provide you with the knowledge, skills, and perspective necessary to contribute to the exciting field of AI-native robotics. From understanding the fundamental principles of Physical AI to implementing complex Vision-Language-Action systems, you'll develop the capabilities to build the next generation of intelligent, embodied AI systems.\n\nThe future belongs to robots that can understand and interact with the physical world naturally, safely, and effectively. Through careful study and practical application of the concepts in this book, you'll be well-prepared to help create that future.\n\nWelcome to the world of AI-Native Robotics. The journey begins now.",
    "chapter": 0,
    "section": "Conclusion",
    "page": 46,
    "token_count": 126
  },
  {
    "chunk_id": "d435cbfd-6f61-42ab-8b81-56deda4f6908",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "# AI-Native Robotics Textbook: Summary & Roadmap",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 12
  },
  {
    "chunk_id": "4337ddaf-57d4-49bb-99a5-1736c7792b11",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Book Overview\n\nThis comprehensive textbook provides a complete journey from foundational concepts in Physical AI to advanced Vision-Language-Action systems. The book is structured to progressively build understanding of how artificial intelligence and robotics converge to create intelligent, embodied systems that can interact naturally with the physical world and with humans. Each chapter builds upon previous foundations while introducing new capabilities and integration concepts.",
    "chapter": 0,
    "section": "Book Overview",
    "page": 2,
    "token_count": 72
  },
  {
    "chunk_id": "d57a8c55-d4f5-4980-b525-fc9d4d8e3ada",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Chapter Summaries",
    "chapter": 0,
    "section": "Chapter Summaries",
    "page": 3,
    "token_count": 4
  },
  {
    "chunk_id": "c11e017f-32a9-4db2-b309-f7bac992114c",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Chapter 1: Introduction to Physical AI\n- **Focus**: Foundational concepts of Physical AI and embodied intelligence\n- **Key Topics**:\n  - Definition and characteristics of Physical AI\n  - Distinction between traditional software AI and embodied intelligence\n  - Physical laws and their importance in robotics\n  - Humanoid robotics landscape and platforms\n  - Sensor systems in humanoid robots",
    "chapter": 0,
    "section": "Chapter 1: Introduction to Physical AI",
    "page": 4,
    "token_count": 77
  },
  {
    "chunk_id": "17b144d9-a7ff-4ec1-9482-cacb0d7caf6a",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Chapter 2: The Robotic Nervous System (ROS 2)\n- **Focus**: Core middleware and communication systems for robotics\n- **Key Topics**:\n  - ROS 2 architecture and computation graph\n  - Creating and running ROS 2 nodes\n  - Communication patterns: Topics, Services, Actions\n  - Bridging Python agents to ROS controllers\n  - Understanding URDF for humanoid robots",
    "chapter": 0,
    "section": "Chapter 2: The Robotic Nervous System (ROS 2)",
    "page": 5,
    "token_count": 84
  },
  {
    "chunk_id": "63bbbaa3-0ae8-4c95-9fd3-16b752e563f3",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Chapter 3: The Digital Twin (Gazebo & Unity)\n- **Focus**: Simulation and digital twin technology for robotics\n- **Key Topics**:\n  - Digital twin concepts and importance\n  - Gazebo world creation and simulation\n  - Sensor simulation and integration\n  - Unity for high-fidelity visualization\n  - Simulation-to-reality transfer",
    "chapter": 0,
    "section": "Chapter 3: The Digital Twin (Gazebo & Unity)",
    "page": 6,
    "token_count": 74
  },
  {
    "chunk_id": "c70b3f8e-42ad-43d1-9b77-f2fdc1e9e037",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)\n- **Focus**: AI and perception systems integration\n- **Key Topics**:\n  - NVIDIA Isaac technology overview\n  - Isaac Sim for AI training\n  - Isaac ROS perception pipeline\n  - Navigation and path planning with Isaac\n  - Hardware acceleration for robotics AI",
    "chapter": 0,
    "section": "Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)",
    "page": 7,
    "token_count": 70
  },
  {
    "chunk_id": "6ab6c15d-6a50-4524-a52c-2fef883610ea",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Chapter 5: Vision-Language-Action (VLA)\n- **Focus**: Natural human-robot interaction through multimodal AI\n- **Key Topics**:\n  - Multimodal AI in Physical AI systems\n  - Vision-Language-Action architecture\n  - Sensor fusion and multimodal pipelines\n  - ROS 2 + Isaac implementation\n  - Integration for natural interaction",
    "chapter": 0,
    "section": "Chapter 5: Vision-Language-Action (VLA)",
    "page": 8,
    "token_count": 76
  },
  {
    "chunk_id": "8bbd699b-6418-4b43-a356-978ff61092bf",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Learning Progression\n\nThe book is designed with a carefully planned learning progression that moves from fundamental concepts to advanced applications:",
    "chapter": 0,
    "section": "Learning Progression",
    "page": 9,
    "token_count": 24
  },
  {
    "chunk_id": "6cb0b829-bfea-4937-b2d6-033e9553b238",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Phase 1: Foundation (Chapter 1)\n- Understanding the nature of Physical AI vs. traditional AI\n- Grasping the importance of grounding AI in physical reality\n- Learning about humanoid robotics landscape\n- Understanding the physical laws governing robotic systems",
    "chapter": 0,
    "section": "Phase 1: Foundation (Chapter 1)",
    "page": 10,
    "token_count": 51
  },
  {
    "chunk_id": "0d92f896-e376-471e-9ac2-8961cd35abe6",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Phase 2: Communication Infrastructure (Chapter 2)\n- Mastering ROS 2 as the communication backbone\n- Learning to create distributed robotic systems\n- Understanding various communication patterns\n- Building bridges between AI and robotic control",
    "chapter": 0,
    "section": "Phase 2: Communication Infrastructure (Chapter 2)",
    "page": 11,
    "token_count": 45
  },
  {
    "chunk_id": "b20b415a-469d-42e1-8c60-3c2ad7a08bfb",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Phase 3: Digital Development Environment (Chapter 3)\n- Creating and validating concepts in simulation\n- Learning to build and customize simulation environments\n- Understanding sensor integration in simulated environments\n- Preparing for safe AI training in digital twins",
    "chapter": 0,
    "section": "Phase 3: Digital Development Environment (Chapter 3)",
    "page": 12,
    "token_count": 48
  },
  {
    "chunk_id": "30b8cc47-3735-47ad-9f0d-851bb132ec6c",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Phase 4: Intelligence Integration (Chapter 4)\n- Incorporating AI for perception and decision making\n- Leveraging hardware acceleration for real-time AI\n- Understanding navigation and planning systems\n- Building the \"brain\" of the robotic system",
    "chapter": 0,
    "section": "Phase 4: Intelligence Integration (Chapter 4)",
    "page": 13,
    "token_count": 49
  },
  {
    "chunk_id": "d2bfb30a-1c37-4eb2-83d9-840f8d8c3740",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Phase 5: Natural Interaction (Chapter 5)\n- Creating intuitive human-robot interfaces\n- Integrating vision, language, and action\n- Building multimodal AI systems\n- Implementing real-world applications",
    "chapter": 0,
    "section": "Phase 5: Natural Interaction (Chapter 5)",
    "page": 14,
    "token_count": 43
  },
  {
    "chunk_id": "f4f97148-0e96-4697-a464-c5e88950fe1f",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Technical Integration",
    "chapter": 0,
    "section": "Technical Integration",
    "page": 15,
    "token_count": 3
  },
  {
    "chunk_id": "27f56880-6454-4dec-8d8c-7d3ed262d8e3",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Key Technologies Covered\n- **ROS 2**: Middleware and communication framework\n- **Gazebo**: Physics simulation and testing\n- **Unity**: High-fidelity visualization and training\n- **NVIDIA Isaac**: AI-accelerated robotics frameworks\n- **Sensor Systems**: Multiple modalities integration",
    "chapter": 0,
    "section": "Key Technologies Covered",
    "page": 16,
    "token_count": 61
  },
  {
    "chunk_id": "5d3f2b4b-6b84-4992-87c5-7b4a25d565b4",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Integration Points\nThe book emphasizes how these technologies work together:\n- ROS 2 provides the communication backbone\n- Gazebo and Unity enable safe development and testing\n- Isaac provides AI acceleration and capabilities\n- Multiple sensor systems enable multimodal interaction",
    "chapter": 0,
    "section": "Integration Points",
    "page": 17,
    "token_count": 50
  },
  {
    "chunk_id": "37a1f827-dcad-4a20-a639-8eeebf4a5bab",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Skill Development Pathway",
    "chapter": 0,
    "section": "Skill Development Pathway",
    "page": 18,
    "token_count": 5
  },
  {
    "chunk_id": "afd979c9-338b-4db4-871f-da6f0fc6ebee",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Beginner Skills\n- Understanding Physical AI concepts\n- Basic ROS 2 node development\n- Simple simulation environments\n- Basic sensor integration",
    "chapter": 0,
    "section": "Beginner Skills",
    "page": 19,
    "token_count": 27
  },
  {
    "chunk_id": "088dc4bc-edea-4812-8eb4-03e8a86ad510",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Intermediate Skills\n- Complex ROS 2 systems and architectures\n- Advanced simulation techniques\n- AI model integration with robotics\n- Multi-sensor fusion",
    "chapter": 0,
    "section": "Intermediate Skills",
    "page": 20,
    "token_count": 30
  },
  {
    "chunk_id": "6a9554e1-b683-4e5e-a124-c84509ac44ca",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Advanced Skills\n- Vision-Language-Action system design\n- Hardware-accelerated AI implementations\n- Natural human-robot interaction\n- Complex multimodal system integration",
    "chapter": 0,
    "section": "Advanced Skills",
    "page": 21,
    "token_count": 34
  },
  {
    "chunk_id": "759a8d4f-c2fe-472d-9622-21e836e7d215",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Applications and Use Cases\n\nThroughout the book, practical applications are emphasized:",
    "chapter": 0,
    "section": "Applications and Use Cases",
    "page": 22,
    "token_count": 15
  },
  {
    "chunk_id": "8f3e4f9a-05f1-4b71-b671-38ca3dea7d67",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Industrial Robotics\n- Automated manufacturing\n- Quality inspection\n- Material handling\n- Collaborative robotics",
    "chapter": 0,
    "section": "Industrial Robotics",
    "page": 23,
    "token_count": 20
  },
  {
    "chunk_id": "59905473-cdf9-412b-9ea5-8e9f309e70ff",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Service Robotics\n- Home assistance\n- Healthcare support\n- Retail and hospitality\n- Educational robots",
    "chapter": 0,
    "section": "Service Robotics",
    "page": 24,
    "token_count": 20
  },
  {
    "chunk_id": "23fefda0-c77f-4526-b46a-d23a21a8052c",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Research and Development\n- Academic research platforms\n- Prototype development\n- Algorithm validation\n- Human-robot interaction studies",
    "chapter": 0,
    "section": "Research and Development",
    "page": 25,
    "token_count": 24
  },
  {
    "chunk_id": "0439dc0f-dbeb-44b4-91c7-ce6f6d24b0e8",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Future Directions",
    "chapter": 0,
    "section": "Future Directions",
    "page": 26,
    "token_count": 3
  },
  {
    "chunk_id": "9c7d22c3-534b-4c7f-b54b-7b7a168572b5",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Emerging Technologies\n- Foundation models for robotics\n- Continual learning systems\n- Improved simulation-to-reality transfer\n- Advanced human-robot collaboration",
    "chapter": 0,
    "section": "Emerging Technologies",
    "page": 27,
    "token_count": 30
  },
  {
    "chunk_id": "95693c03-1ba7-4a18-b457-306672fbbb3e",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Industry Trends\n- Increased adoption of AI in robotics\n- Growing importance of natural interfaces\n- Expansion into new application domains\n- Focus on safety and reliability",
    "chapter": 0,
    "section": "Industry Trends",
    "page": 28,
    "token_count": 32
  },
  {
    "chunk_id": "45d357ab-ba66-4995-b1b8-f348f26679e6",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Implementation Guidelines",
    "chapter": 0,
    "section": "Implementation Guidelines",
    "page": 29,
    "token_count": 3
  },
  {
    "chunk_id": "9c3406fe-9ef4-46f7-bbcf-9012f4ccdea1",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Best Practices\n- Safety-first approach in all implementations\n- Modular, reusable code components\n- Comprehensive testing and validation\n- Proper documentation and comments",
    "chapter": 0,
    "section": "Best Practices",
    "page": 30,
    "token_count": 30
  },
  {
    "chunk_id": "8369f21d-af6f-4afd-b154-29077127991c",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Development Workflow\n1. Design in simulation with digital twins\n2. Validate with multiple testing methods\n3. Implement with safety checks and validation\n4. Deploy with monitoring and feedback systems",
    "chapter": 0,
    "section": "Development Workflow",
    "page": 31,
    "token_count": 38
  },
  {
    "chunk_id": "496e16a4-cbe8-44af-93d4-45916b8ec4d9",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Performance Considerations\n- Real-time performance requirements\n- Resource optimization for embedded systems\n- Balancing accuracy and speed\n- Scalability for multiple robots",
    "chapter": 0,
    "section": "Performance Considerations",
    "page": 32,
    "token_count": 31
  },
  {
    "chunk_id": "690e4379-3a5d-4f33-907f-45974ef4d493",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Resource Integration",
    "chapter": 0,
    "section": "Resource Integration",
    "page": 33,
    "token_count": 3
  },
  {
    "chunk_id": "2f2842d6-ece3-4120-aa0c-636ba2d21b80",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### External Resources\n- Official documentation for ROS 2, Isaac, Unity, etc.\n- Research papers and academic references\n- Industry case studies and examples\n- Community forums and support channels",
    "chapter": 0,
    "section": "External Resources",
    "page": 34,
    "token_count": 38
  },
  {
    "chunk_id": "8c1874c1-1d15-4cc0-bc5e-bf74459e977b",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "### Tool Integration\n- Development environment setup\n- Testing and debugging tools\n- Simulation and visualization tools\n- Hardware integration tools",
    "chapter": 0,
    "section": "Tool Integration",
    "page": 35,
    "token_count": 25
  },
  {
    "chunk_id": "8faac4db-cdaf-4c78-af64-5a827e2db807",
    "doc_id": "efa4ef25-5369-4dad-b454-099397d9cd6b",
    "chunk_text": "## Conclusion\n\nThis textbook provides a comprehensive pathway for understanding and implementing AI-native robotics systems. By progressing from fundamental concepts through communication infrastructure, simulation environments, AI integration, and natural interaction, readers gain a complete understanding of how to build sophisticated robotic systems that can safely and effectively operate in human environments with natural language interfaces.\n\nThe integration of multiple technologies and the emphasis on practical implementation ensure that readers can apply what they learn to real-world robotics projects. The focus on safety, reliability, and natural human interaction prepares readers for the future of robotics where AI-enabled robots will be common in everyday life.\n\nBy mastering the concepts and techniques covered in this book, readers will be well-equipped to contribute to the rapidly advancing field of AI-native robotics and to develop the next generation of intelligent, embodied systems.",
    "chapter": 0,
    "section": "Conclusion",
    "page": 36,
    "token_count": 156
  },
  {
    "chunk_id": "347ceb2c-5ccb-4a1f-85c2-8c2ccc3823d2",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "# From Digital to Physical Intelligence",
    "chapter": 1,
    "section": "Introduction",
    "page": 1,
    "token_count": 6
  },
  {
    "chunk_id": "0e72066a-f5e0-4c74-bca3-56197a94226e",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## 🎯 Learning Objectives\n\n- Understand the technical differences between digital and physical AI\n- Learn about the embodiment problem in robotics\n- Explore the perception-action loop\n- Recognize the computational challenges of real-time physical systems",
    "chapter": 1,
    "section": "🎯 Learning Objectives",
    "page": 2,
    "token_count": 46
  },
  {
    "chunk_id": "f8cdb982-9a8f-4355-a984-4091dd29c974",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## The Embodiment Challenge\n\nMoving AI from digital to physical domains introduces fundamental new requirements:\n\n| Digital AI | Physical AI |\n|------------|-------------|\n| Operates on data | Operates in 3D space |\n| Instant computation | Real-time constraints |\n| Perfect information | Noisy sensors |\n| Virtual consequences | Physical safety critical |\n| Easy to parallelize | Limited by hardware |\n| Costless experimentation | Expensive data collection |",
    "chapter": 1,
    "section": "The Embodiment Challenge",
    "page": 3,
    "token_count": 89
  },
  {
    "chunk_id": "2508b029-49d5-43c9-babb-2ea1e9c0aa24",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## The Perception-Action Loop\n\nPhysical AI systems operate in a continuous cycle:\n\n```python\nwhile robot.is_running():\n    # 1. PERCEIVE\n    sensor_data = robot.get_sensor_readings()\n\n    # 2. UNDERSTAND\n    world_state = perception_model.process(sensor_data)\n\n    # 3. DECIDE\n    action = policy_model.plan(world_state, goal)\n\n    # 4. ACT\n    robot.execute(action)\n\n    # 5. LEARN (optional)\n    if training_mode:\n        feedback = evaluate_outcome()\n        model.update(feedback)\n```\n\nThis cycle must complete in **milliseconds** for responsive behavior.",
    "chapter": 1,
    "section": "The Perception-Action Loop",
    "page": 4,
    "token_count": 135
  },
  {
    "chunk_id": "bdf7fbe4-3425-4c47-8c57-f9632fd5c9f8",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## Technical Architecture",
    "chapter": 1,
    "section": "Technical Architecture",
    "page": 5,
    "token_count": 3
  },
  {
    "chunk_id": "91659eda-25dd-48ab-9bb5-3e2fd43fb16c",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### Digital AI System\n```\n┌──────────────┐\n│    Input     │  (Text, Image, Audio)\n└──────┬───────┘\n       │\n┌──────▼───────┐\n│ Neural Model │  (Process, unlimited time)\n└──────┬───────┘\n       │\n┌──────▼───────┐\n│   Output     │  (Prediction, Classification)\n└──────────────┘\n```",
    "chapter": 1,
    "section": "Digital AI System",
    "page": 6,
    "token_count": 113
  },
  {
    "chunk_id": "7862d338-f271-4746-b547-4f6ef25d6372",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### Physical AI System\n```\n       ┌─────────────┐\n       │   Sensors   │  (Camera, LIDAR, IMU)\n       └──────┬──────┘\n              │\n       ┌──────▼──────┐\n       │  Perception │  (Object detection, SLAM)\n       └──────┬──────┘\n              │\n       ┌──────▼──────┐\n       │  Planning   │  (Path planning, task planning)\n       └──────┬──────┘\n              │\n       ┌──────▼──────┐\n       │   Control   │  (Motor commands)\n       └──────┬──────┘\n              │\n       ┌──────▼──────┐\n       │  Actuators  │  (Motors, Grippers)\n       └──────┬──────┘\n              │\n         Physical World\n              │\n       (Feedback Loop)\n```",
    "chapter": 1,
    "section": "Physical AI System",
    "page": 7,
    "token_count": 223
  },
  {
    "chunk_id": "70d8b882-a6a4-435a-9385-ad2ec1fd322f",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## Key Technical Transitions",
    "chapter": 1,
    "section": "Key Technical Transitions",
    "page": 8,
    "token_count": 5
  },
  {
    "chunk_id": "0f3a051a-faa2-44bc-82cc-f643a0605944",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 1. From Images to 3D Space\n\n**Digital Vision**:\n```python\n# 2D image classification\nimage = load_image(\"cat.jpg\")  # Shape: (224, 224, 3)\nprediction = model.predict(image)\n# Output: \"cat\" (0.95 confidence)\n```\n\n**Physical Vision**:\n```python\n# 3D scene understanding for robotics\npoint_cloud = lidar.scan()  # Shape: (N, 3) - 3D points\nobjects = detector.detect_3d(point_cloud)\n# Output: {\"chair\": {\"position\": [1.2, 0.5, 0.0],\n#                    \"orientation\": [0, 0, 90],\n#                    \"graspable\": True}}\n```",
    "chapter": 1,
    "section": "1. From Images to 3D Space",
    "page": 9,
    "token_count": 166
  },
  {
    "chunk_id": "52c061be-31ce-4e5e-9986-ac8055a5864f",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 2. From Predictions to Actions\n\n**Digital Output**:\n```python\n# Generate text\nresponse = llm.generate(\"What is 2+2?\")\n# Output: \"4\" (no physical consequence)\n```\n\n**Physical Output**:\n```python\n# Generate robot action\naction = policy.predict(observation)\n# action = {\"joint_positions\": [0.5, 1.2, -0.3, ...],\n#           \"gripper\": \"close\"}\nrobot.execute(action)\n# Real motors move! Safety critical!\n```",
    "chapter": 1,
    "section": "2. From Predictions to Actions",
    "page": 10,
    "token_count": 114
  },
  {
    "chunk_id": "a29caa77-9afb-4f4e-9f4d-e3b88b30c492",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 3. From Batch Processing to Real-Time\n\n**Digital Processing**:\n```python\n# Process entire dataset\nfor batch in dataloader:\n    predictions = model(batch)\n    # No time constraint\n```\n\n**Physical Processing**:\n```python\n# Real-time control loop (30-100 Hz)\nrate = rospy.Rate(30)  # 30 Hz = 33ms per cycle\nwhile True:\n    obs = get_observation()  # Must complete in <33ms\n    action = compute_action(obs)\n    execute(action)\n    rate.sleep()\n```",
    "chapter": 1,
    "section": "3. From Batch Processing to Real-Time",
    "page": 11,
    "token_count": 119
  },
  {
    "chunk_id": "6d786f0a-6f92-4884-af20-7200fe991cc9",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## The Reality Gap\n\nChallenges unique to physical deployment:",
    "chapter": 1,
    "section": "The Reality Gap",
    "page": 12,
    "token_count": 12
  },
  {
    "chunk_id": "005d641f-c566-438d-b7d2-c73321301970",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 1. **Sensor Noise**\n```python\n# Ideal (simulation)\ndistance = lidar.measure_distance()\n# Returns: 1.523 meters (exact)\n\n# Reality\ndistance = lidar.measure_distance()\n# Returns: 1.523 ± 0.05 meters (noisy)\n# Plus: outliers, missing data, interference\n```",
    "chapter": 1,
    "section": "1. **Sensor Noise**",
    "page": 13,
    "token_count": 72
  },
  {
    "chunk_id": "c61f49fb-5c32-4715-a0b1-c54d7ff6b976",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 2. **Model Uncertainty**\n```python\n# Digital: Wrong prediction = bad metric\nprediction = model.predict(image)\nif prediction != ground_truth:\n    accuracy -= 1\n\n# Physical: Wrong action = broken robot/injury\naction = model.predict(state)\nrobot.execute(action)  # Could crash into wall!\n# Need: Safety constraints, uncertainty quantification\n```",
    "chapter": 1,
    "section": "2. **Model Uncertainty**",
    "page": 14,
    "token_count": 78
  },
  {
    "chunk_id": "2df29b75-597c-4770-a06a-ec79e4b370ef",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 3. **Latency**\n```python\n# Digital: Async is fine\nasync def process_request(data):\n    result = await model.infer(data)\n    return result\n\n# Physical: Delay = disaster\ndef control_loop():\n    obs = sense()  # 10ms\n    action = plan(obs)  # 15ms\n    execute(action)  # 5ms\n    # Total: 30ms (must be < control frequency)\n```",
    "chapter": 1,
    "section": "3. **Latency**",
    "page": 15,
    "token_count": 96
  },
  {
    "chunk_id": "707ccfb1-fdd3-4e2e-986d-a97fa3145079",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## Solutions and Best Practices",
    "chapter": 1,
    "section": "Solutions and Best Practices",
    "page": 16,
    "token_count": 5
  },
  {
    "chunk_id": "0c3703fc-d109-42f1-be53-e8a3ef891b1a",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 1. **Simulation-First Development**\n\nTest in digital twins before real deployment:\n\n```python\n# Step 1: Train in simulation\nenv = gym.make('RobotEnv-v0')\nmodel.train(env, episodes=10000)\n\n# Step 2: Domain randomization\nenv = RandomizedEnv(\n    lighting_range=(0.3, 1.0),\n    texture_variations=True,\n    physics_noise=0.1\n)\nmodel.finetune(env)\n\n# Step 3: Real-world deployment\nrobot = PhysicalRobot()\nrobot.load_policy(model)\n```",
    "chapter": 1,
    "section": "1. **Simulation-First Development**",
    "page": 17,
    "token_count": 118
  },
  {
    "chunk_id": "f755f5fc-5a2d-41d3-9af0-b0f1139de19e",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 2. **Hierarchical Control**\n\nSeparate fast and slow processing:\n\n```python\n# High-level (slow, 1-10 Hz)\ndef task_planner():\n    goal = \"pick up cup\"\n    plan = [\n        \"move_to_object\",\n        \"reach_and_grasp\",\n        \"lift_object\"\n    ]\n    return plan\n\n# Low-level (fast, 100-1000 Hz)\ndef motor_controller():\n    current_pos = encoder.read()\n    target_pos = plan.next_waypoint()\n    control_signal = pid.compute(current_pos, target_pos)\n    motor.send(control_signal)\n```",
    "chapter": 1,
    "section": "2. **Hierarchical Control**",
    "page": 18,
    "token_count": 124
  },
  {
    "chunk_id": "6d706f36-4483-4c6b-bc70-29464d896ed3",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### 3. **Safety Layers**\n\nAlways include safety constraints:\n\n```python\ndef execute_action(action):\n    # 1. Collision checking\n    if collision_detector.would_collide(action):\n        action = stop_action()\n\n    # 2. Joint limits\n    action = np.clip(action, joint_min, joint_max)\n\n    # 3. Speed limits\n    if np.linalg.norm(action.velocity) > max_speed:\n        action.velocity = normalize(action.velocity) * max_speed\n\n    # 4. Emergency stop\n    if emergency_button.pressed():\n        robot.halt()\n        return\n\n    # Finally: Execute\n    robot.apply(action)\n```",
    "chapter": 1,
    "section": "3. **Safety Layers**",
    "page": 19,
    "token_count": 133
  },
  {
    "chunk_id": "8f91894d-54af-4cd7-9c53-ae6a7163e03e",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## Case Study: Robotic Grasping\n\nLet's see how digital AI becomes physical AI:",
    "chapter": 1,
    "section": "Case Study: Robotic Grasping",
    "page": 20,
    "token_count": 20
  },
  {
    "chunk_id": "972ed159-f807-4c44-a1dd-50e8f055d90a",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### Digital Approach (Won't Work)\n```python\n# This ignores physics!\nimage = camera.capture()\nobject_class = classifier(image)  # \"cup\"\nprint(f\"I see a {object_class}\")\n```",
    "chapter": 1,
    "section": "Digital Approach (Won't Work)",
    "page": 21,
    "token_count": 42
  },
  {
    "chunk_id": "94dfa1e0-1be5-4477-807f-bf3f21cf784b",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "### Physical Approach (Production Ready)\n```python\n# 1. Perceive in 3D\nrgbd = camera.capture_rgbd()\npoint_cloud = rgbd_to_points(rgbd)\n\n# 2. Detect and localize\nobjects = detector.detect(point_cloud)\ncup = objects.get_by_class(\"cup\")\n\n# 3. Plan grasp\ngrasps = grasp_planner.plan(cup.geometry)\nbest_grasp = grasps[0]  # Ranked by success probability\n\n# 4. Execute motion\ntrajectory = motion_planner.plan_to_grasp(\n    current_pose=robot.get_pose(),\n    target_grasp=best_grasp,\n    obstacles=point_cloud\n)\n\n# 5. Control loop\nfor waypoint in trajectory:\n    while not at_waypoint(waypoint):\n        current = robot.get_pose()\n        control = controller.compute(current, waypoint)\n        robot.apply(control)\n        time.sleep(0.01)  # 100 Hz\n\n# 6. Grasp\nrobot.close_gripper()\n\n# 7. Verify\nif force_sensor.detect_object():\n    print(\"Grasp successful!\")\nelse:\n    print(\"Grasp failed, retrying...\")\n```",
    "chapter": 1,
    "section": "Physical Approach (Production Ready)",
    "page": 22,
    "token_count": 241
  },
  {
    "chunk_id": "886d20a3-5e41-4675-8fc5-c784d4257250",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## The Integration Challenge\n\nPhysical AI requires integrating multiple technologies:\n\n```\n┌────────────────────────────────────────┐\n│  Application Layer (Task Logic)        │\n├────────────────────────────────────────┤\n│  AI/ML Models (Vision, Planning, VLA)  │\n├────────────────────────────────────────┤\n│  ROS2 Middleware (Communication)       │\n├────────────────────────────────────────┤\n│  Robot Drivers (Hardware Interface)    │\n├────────────────────────────────────────┤\n│  Operating System (Linux RT)           │\n├────────────────────────────────────────┤\n│  Hardware (Compute + Sensors + Motors) │\n└────────────────────────────────────────┘\n```\n\nEach layer has different requirements and constraints.",
    "chapter": 1,
    "section": "The Integration Challenge",
    "page": 23,
    "token_count": 167
  },
  {
    "chunk_id": "f0ab48e6-8b91-42dd-be6a-c94f640704fb",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## Summary\n\nTransitioning from digital to physical AI requires:\n\n✅ **Real-time processing** - Millisecond-level responsiveness\n✅ **3D understanding** - Spatial reasoning, not just 2D pixels\n✅ **Safety-critical design** - Physical actions have consequences\n✅ **Robustness** - Handle noise, uncertainty, and failures\n✅ **Integration** - Combine perception, planning, and control\n✅ **Sim-to-real transfer** - Bridge simulation and reality\n\n:::tip Key Takeaway\nPhysical AI isn't just digital AI with motors attached. It requires rethinking algorithms, architectures, and development processes to handle the challenges of the real world.\n:::",
    "chapter": 1,
    "section": "Summary",
    "page": 24,
    "token_count": 141
  },
  {
    "chunk_id": "f3fe83bd-6542-4ffb-b322-4f2f1fb4c7ab",
    "doc_id": "a11f3a2e-9d08-40b3-a2bb-1d8b33a95725",
    "chunk_text": "## Next Steps\n\nNow that we understand the transition challenge, we'll explore:\n- The current landscape of humanoid robots\n- Sensor systems that enable perception\n- Practical implementation with ROS2\n\n---",
    "chapter": 1,
    "section": "Next Steps",
    "page": 25,
    "token_count": 39
  },
  {
    "chunk_id": "ae5aa6d9-bdd5-4fa7-a279-c86aa5f4b29f",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "# Embodied Intelligence",
    "chapter": 1,
    "section": "Introduction",
    "page": 1,
    "token_count": 4
  },
  {
    "chunk_id": "ffdcd5ee-a7e6-4b9f-a185-251b8d881112",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Overview\n\nEmbodied Intelligence represents a paradigm shift in robotics and artificial intelligence where intelligence is not just a computational process but emerges from the interaction between an agent and its physical environment. This approach recognizes that intelligence is fundamentally shaped by the physical form and the ways an agent can interact with the world around it. Unlike traditional approaches that treat perception, cognition, and action as separate modules, embodied intelligence integrates these functions through the physical embodiment of AI systems.\n\nThe concept builds on the idea that the body plays an active role in cognition, rather than simply being a tool for executing pre-planned actions. This perspective has profound implications for humanoid robotics, where the human-like form factor is not just aesthetic but functional, enabling robots to navigate and interact with human-designed environments more effectively.",
    "chapter": 1,
    "section": "Overview",
    "page": 2,
    "token_count": 153
  },
  {
    "chunk_id": "c1e51591-8ff9-4c33-9d27-9ee1164b3273",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Key Concepts",
    "chapter": 1,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "17e76604-391e-4cd5-8911-4c049d5d3eeb",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Definition of Embodied Intelligence\n\nEmbodied Intelligence refers to the integration of artificial intelligence into physical systems that can actively interact with their environment. This concept encompasses:\n\n- **Morphological Computation**: The body's physical properties contribute to intelligent behavior\n- **Situated Cognition**: Intelligence emerges from the interaction between the agent and its environment\n- **Sensorimotor Coupling**: Perception and action are tightly integrated rather than separate processes\n- **Environmental Affordances**: The physical environment provides opportunities for action that shape intelligent behavior",
    "chapter": 1,
    "section": "Definition of Embodied Intelligence",
    "page": 4,
    "token_count": 106
  },
  {
    "chunk_id": "8da58888-b655-4d67-aa24-085530c0ec6c",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### The Importance of Physical Embodiment in AI\n\nPhysical embodiment is crucial for several reasons:\n\n1. **Real-World Learning**: Robots must learn to handle the complexity and unpredictability of the real world, which cannot be fully captured in simulation\n2. **Physics Understanding**: Embodied agents develop intuition about physical laws through interaction with objects and environments\n3. **Embodied Cognition**: The physical form influences cognitive processes, leading to more effective problem-solving strategies\n4. **Sensorimotor Coordination**: Physical embodiment requires the integration of multiple sensory modalities and motor responses",
    "chapter": 1,
    "section": "The Importance of Physical Embodiment in AI",
    "page": 5,
    "token_count": 116
  },
  {
    "chunk_id": "fc8d2222-dae7-4109-bc07-cdb2ace138f0",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### How Robots Learn Through Interaction with the Physical World\n\nEmbodied robots learn through several mechanisms:\n\n- **Trial and Error**: Physical interaction allows robots to learn from successes and failures\n- **Active Perception**: Robots learn by actively exploring their environment rather than passively receiving information\n- **Imitation Learning**: Physical embodiment allows robots to learn by observing and mimicking human actions\n- **Reinforcement Learning**: Physical rewards and consequences guide learning processes in real environments",
    "chapter": 1,
    "section": "How Robots Learn Through Interaction with the Physical World",
    "page": 6,
    "token_count": 92
  },
  {
    "chunk_id": "2fce39fb-1b6b-4141-8fc2-4b078b509168",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Examples",
    "chapter": 1,
    "section": "Examples",
    "page": 7,
    "token_count": 2
  },
  {
    "chunk_id": "e5da0068-679b-4fc0-be8f-6e46f0bcb417",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Humanoid Learning Through Embodiment\n\n**Boston Dynamics Atlas Robot**: Demonstrates embodied intelligence through its ability to navigate complex terrains, maintain balance, and perform acrobatic movements. The robot's physical form and interaction with the environment enable it to develop sophisticated locomotion strategies.\n\n**Honda ASIMO**: Showcased how embodied intelligence allows robots to walk, climb stairs, and interact with humans in human-designed spaces. The robot's human-like form factor enabled it to navigate doorways, use stairs, and interact with objects designed for humans.\n\n**Toyota HSR (Human Support Robot)**: Designed with embodied intelligence principles, this robot can perform tasks in human environments by understanding the affordances of objects and spaces shaped for human use.",
    "chapter": 1,
    "section": "Humanoid Learning Through Embodiment",
    "page": 8,
    "token_count": 147
  },
  {
    "chunk_id": "64e78813-b7a9-4f55-99fb-0ec9f1af0706",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Industrial Applications\n\n**Amazon Warehouse Robots**: These systems demonstrate embodied intelligence through their ability to navigate dynamic environments, recognize objects, and adapt their behavior based on real-time sensory feedback.\n\n**Collaborative Robots (Cobots)**: Designed to work safely alongside humans, these robots embody intelligence that allows them to adapt their behavior based on physical interaction with humans and objects.",
    "chapter": 1,
    "section": "Industrial Applications",
    "page": 9,
    "token_count": 74
  },
  {
    "chunk_id": "336564ff-0068-4332-bceb-fb4e1a5de0bf",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Research Demonstrations\n\n**iCub Humanoid Robot**: Developed by the Italian Institute of Technology, the iCub demonstrates embodied intelligence through its ability to learn manipulation skills through physical interaction with objects in its environment.\n\n**NAO Robot**: Used extensively in research and education, NAO demonstrates embodied intelligence through its ability to learn from social interaction and physical tasks.",
    "chapter": 1,
    "section": "Research Demonstrations",
    "page": 10,
    "token_count": 73
  },
  {
    "chunk_id": "71aa8d15-0c7a-4d15-950c-7e505df7ce87",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Theoretical Foundations",
    "chapter": 1,
    "section": "Theoretical Foundations",
    "page": 11,
    "token_count": 4
  },
  {
    "chunk_id": "7b1a1bad-2320-487e-81ae-b88b22dc5fe0",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### The Three Stages of Embodied Intelligence\n\nAccording to research in the field, the application of embodied intelligence in robots involves three critical stages:\n\n1. **Perception**: Gathering sensory information from the environment through various sensors\n2. **Reasoning**: Processing sensory information to make decisions about appropriate actions\n3. **Execution**: Implementing actions in the physical world and observing the consequences",
    "chapter": 1,
    "section": "The Three Stages of Embodied Intelligence",
    "page": 12,
    "token_count": 77
  },
  {
    "chunk_id": "b927212c-58ff-49d3-9362-de7e0268213e",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Affordance-Based Interaction\n\nThis concept, derived from ecological psychology, suggests that the environment offers opportunities for action that are perceived by embodied agents. For example, a handle affords grasping, a chair affords sitting, and a ball affords rolling. Embodied robots learn to recognize these affordances through physical interaction.",
    "chapter": 1,
    "section": "Affordance-Based Interaction",
    "page": 13,
    "token_count": 66
  },
  {
    "chunk_id": "98c06a02-cee5-4d5e-a533-8b0058c27305",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Diagrams",
    "chapter": 1,
    "section": "Diagrams",
    "page": 14,
    "token_count": 3
  },
  {
    "chunk_id": "aa828d47-cf86-4483-8078-30651c7fba2e",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Embodied Intelligence Architecture",
    "chapter": 1,
    "section": "Embodied Intelligence Architecture",
    "page": 15,
    "token_count": 5
  },
  {
    "chunk_id": "70a25bfe-ebc6-476b-9108-737430fa0ac9",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Embodied Intelligence Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                 Embodied Intelligence System                │\n├─────────────────────────────────────────────────────────────┤\n│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐   │\n│  │ Interaction │────▶│ Intelligence│────▶│ Action      │   │\n│  │ with        │     │ Processing  │     │ Execution   │   │\n│  │ Environment │     │ (Learning & │     │ (Physical  │   │\n│  │             │     │ Reasoning)  │     │ Response)   │   │\n│  └─────────────┘     └─────────────┘     └─────────────┘   │\n└─────────────────────────────────────────────────────────────┘\n              ▲                                   │\n              │                                   ▼\n    ┌─────────────────┐                ┌─────────────────┐\n    │ Physical World  │                │ Robot Body      │\n    │ • Objects       │                │ • Sensors       │\n    │ • Gravity       │                │ • Actuators     │\n    │ • Friction      │                │ • Controllers   │\n    │ • Obstacles     │                │ • Mechanics     │\n    └─────────────────┘                └─────────────────┘\n```",
    "chapter": 1,
    "section": "Embodied Intelligence Architecture",
    "page": 16,
    "token_count": 312
  },
  {
    "chunk_id": "e74af1fe-ff34-4673-b7a6-b78a322cdd6b",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "### Embodied vs Non-Embodied AI Comparison\n\n```\nNon-Embodied AI:              Embodied AI:\n┌─────────────────┐         ┌─────────────────┐\n│  Computational  │         │  Physical Body  │\n│  System         │         │  + Sensors +    │\n│                 │         │  Actuators      │\n│  ┌───────────┐  │         │                 │\n│  │ AI Engine │  │         │  ┌───────────┐  │\n│  └───────────┘  │         │  │ AI Engine │  │\n│                 │    +    │  └───────────┘  │\n│  Inputs:       │         │                 │\n│  • Text        │         │  Inputs:        │\n│  • Images      │         │  • Vision       │\n│  • Numbers     │         │  • Touch        │\n│                 │         │  • Proprioception│\n└─────────────────┘         │  • Forces       │\n                            │                 │\n                            │  Outputs:       │\n                            │  • Motor Actions │\n                            │  • Manipulation │\n                            │  • Locomotion   │\n                            └─────────────────┘\n```",
    "chapter": 1,
    "section": "Embodied vs Non-Embodied AI Comparison",
    "page": 17,
    "token_count": 284
  },
  {
    "chunk_id": "41aff76b-b74c-4af7-9887-be39358955ca",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Define embodied intelligence and explain its core principles\n2. Distinguish between embodied and non-embodied AI systems\n3. Identify examples of embodied intelligence in humanoid and other robots\n4. Understand the three stages of embodied intelligence in robotics\n5. Explain how robots learn through physical interaction with their environment",
    "chapter": 1,
    "section": "Learning Outcomes",
    "page": 18,
    "token_count": 78
  },
  {
    "chunk_id": "bc3533de-8678-4f7c-98d1-5739b378bf9c",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## Further Reading\n\n- \"Embodied Intelligence and Its Application in Robotics\" (MDPI Applied Sciences, 2025) [1]\n- \"Embodied AI: China's Big Bet on Smart Robots\" (Carnegie Endowment, 2025) [2]\n- \"Embodied Intelligence: The 'Big Brain' That Determines Robotics Intelligence\" (Eeworld, 2025) [3]\n\n---",
    "chapter": 1,
    "section": "Further Reading",
    "page": 19,
    "token_count": 84
  },
  {
    "chunk_id": "0c0378b6-6b42-469b-a7f4-63b21882a8a8",
    "doc_id": "36eca853-cc5a-4a03-95fd-b8747ef41905",
    "chunk_text": "## References\n\n[1] MDPI Applied Sciences Journal. \"Embodied Intelligence and Its Application in Robotics.\" Special Issue. 2025. https://www.mdpi.com/journal/applsci/special_issues/9A8QI550Y0\n\n[2] Carnegie Endowment for International Peace. \"Embodied AI: China's Big Bet on Smart Robots.\" November 24, 2025. https://carnegieendowment.org/research/2025/11/embodied-ai-china-smart-robots?lang=en\n\n[3] Eeworld. \"Embodied Intelligence: The 'Big Brain' That Determines Robotics Intelligence.\" March 13, 2025. https://en.eeworld.com.cn/mp/ICVIS/a395257.jspx",
    "chapter": 1,
    "section": "References",
    "page": 20,
    "token_count": 159
  },
  {
    "chunk_id": "4b4a3efc-5496-4110-89bd-c9cc72d255de",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "# The Humanoid Robotics Landscape",
    "chapter": 1,
    "section": "Introduction",
    "page": 1,
    "token_count": 6
  },
  {
    "chunk_id": "e8171fc1-4337-49dc-a18d-333a517c3b03",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## 🎯 Learning Objectives\n\nBy the end of this section, you will:\n\n- Understand the current state of humanoid robotics technology\n- Learn about major humanoid robot platforms and their capabilities\n- Explore different design philosophies and technical approaches\n- Recognize key hardware components and architectures\n- Identify current limitations and future directions",
    "chapter": 1,
    "section": "🎯 Learning Objectives",
    "page": 2,
    "token_count": 65
  },
  {
    "chunk_id": "46a0a796-20e7-471c-bba3-629ec59508d0",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Why Humanoid Robots?\n\nThe humanoid form factor isn't just about making robots look like us—it's a practical engineering choice:\n\n**Environmental Compatibility**: Our world is designed for human bodies. Humanoid robots can:\n- Navigate stairs, doorways, and narrow spaces\n- Use tools designed for human hands\n- Operate vehicles and machinery\n- Work in existing infrastructure without modifications\n\n**Intuitive Interaction**: Human-like form enables:\n- Natural communication through gestures and body language\n- Predictable movement patterns for human collaborators\n- Social acceptance in shared spaces\n- Easier teaching through demonstration\n\n:::tip Engineering Insight\nThe humanoid form factor represents a trade-off: increased complexity for universal adaptability. A humanoid robot sacrifices the efficiency of specialized designs for the flexibility to handle diverse tasks.\n:::",
    "chapter": 1,
    "section": "Why Humanoid Robots?",
    "page": 3,
    "token_count": 160
  },
  {
    "chunk_id": "77ca8e90-c0fa-4f94-a9ad-02cbbe0af1e0",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## The Current Landscape: Major Players",
    "chapter": 1,
    "section": "The Current Landscape: Major Players",
    "page": 4,
    "token_count": 7
  },
  {
    "chunk_id": "f24c0aab-6580-4e6d-b30e-722e812b7a14",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Industry Leaders\n\nThe humanoid robotics field has evolved from academic research to serious commercial ventures:\n\n| Category | Focus | Examples |\n|----------|-------|----------|\n| **Legacy Pioneers** | Research & advanced mobility | Boston Dynamics, Honda |\n| **AI-First Startups** | Foundation model integration | Figure AI, 1X Technologies |\n| **Tech Giants** | Manufacturing & consumer scale | Tesla, Xiaomi |\n| **Logistics-Focused** | Warehouse automation | Agility Robotics, Apptronik |",
    "chapter": 1,
    "section": "Industry Leaders",
    "page": 5,
    "token_count": 104
  },
  {
    "chunk_id": "aa4bc1a4-f793-43bb-8e33-0aa1c9b43b27",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Platform Deep Dives",
    "chapter": 1,
    "section": "Platform Deep Dives",
    "page": 6,
    "token_count": 5
  },
  {
    "chunk_id": "bf8f82e2-8958-42f8-b1f4-c20d55999854",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 1. Boston Dynamics Atlas\n\n**Technical Specifications**:\n- **Height**: 1.5m (5'0\")\n- **Weight**: 89 kg (196 lbs)\n- **Degrees of Freedom**: 28\n- **Sensors**: Stereo vision, depth sensors, IMU\n- **Actuators**: Hydraulic and electric hybrid\n\n**Key Capabilities**:\n```\n- Dynamic locomotion (running, jumping, backflips)\n- Whole-body manipulation (lifting 11 kg objects)\n- Terrain adaptation (stairs, uneven surfaces)\n- Real-time balance recovery\n```\n\n**Design Philosophy**: Atlas represents the **dynamic mobility** approach, prioritizing agility and athletic performance.\n\n```python\n# Atlas-style control emphasizes whole-body dynamics\nclass AtlasController:\n    def balance_recovery(self, perturbation):\n        # 1. Detect disturbance via IMU\n        angular_momentum = self.imu.get_momentum()\n\n        # 2. Compute corrective action\n        recovery_action = self.mpc_controller.solve(\n            current_state=self.state,\n            disturbance=angular_momentum,\n            horizon=0.5  # 500ms lookahead\n        )\n\n        # 3. Execute whole-body coordination\n        return recovery_action  # All 28 DOF coordinated\n```\n\n**Current Status**: Primarily research platform; transitioning to commercial applications in warehouse logistics.",
    "chapter": 1,
    "section": "1. Boston Dynamics Atlas",
    "page": 7,
    "token_count": 278
  },
  {
    "chunk_id": "f9070f8c-ec9c-4fda-9e99-71e6600aa9fb",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 2. Tesla Optimus (Bot Gen 2)\n\n**Technical Specifications**:\n- **Height**: 1.73m (5'8\")\n- **Weight**: 73 kg (161 lbs)\n- **Degrees of Freedom**: 40+ (including dexterous hands)\n- **Sensors**: 8 cameras (FSD vision system), force/torque sensors\n- **Actuators**: Custom electric actuators, Tesla-designed hands with 11 DOF\n\n**Key Capabilities**:\n```\n- Vision-based manipulation (no LIDAR)\n- Dexterous grasping (can handle eggs)\n- Manufacturing tasks (wire harness assembly)\n- Self-charging capability\n- Natural language understanding\n```\n\n**Design Philosophy**: Optimus follows the **automotive scaling** approach, leveraging Tesla's manufacturing expertise and vision-first AI.\n\n```python\n# Optimus-style vision-first perception\nclass OptimusVision:\n    def perceive_scene(self, camera_feeds):\n        # 1. Multi-camera fusion (like FSD)\n        bev_features = self.bev_encoder(camera_feeds)  # Bird's eye view\n\n# 2. Occupancy prediction\n        occupancy_grid = self.occupancy_network(bev_features)\n\n# 3. Object detection and 6D pose estimation\n        objects = self.detection_head(bev_features)\n\n# 4. Affordance prediction\n        grasp_points = self.affordance_network(objects)",
    "chapter": 1,
    "section": "2. Tesla Optimus (Bot Gen 2)",
    "page": 8,
    "token_count": 296
  },
  {
    "chunk_id": "305bd7b7-6553-48be-9ddb-dc475307aa95",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## 2. Tesla Optimus (Bot Gen 2)\n\nreturn {\n            \"occupancy\": occupancy_grid,\n            \"objects\": objects,\n            \"grasps\": grasp_points\n        }\n```\n\n**Current Status**: Deployed internally at Tesla factories; Gen 2 announced 2024 with improved hands and AI integration.",
    "chapter": 1,
    "section": "2. Tesla Optimus (Bot Gen 2)",
    "page": 9,
    "token_count": 67
  },
  {
    "chunk_id": "a1614686-c450-483e-9fe1-dc5ba93a32db",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 3. Figure AI - Figure 02\n\n**Technical Specifications**:\n- **Height**: 1.7m (5'7\")\n- **Weight**: 70 kg (154 lbs)\n- **Degrees of Freedom**: 16 (humanoid joints)\n- **Sensors**: RGB-D cameras, tactile sensors in hands\n- **Actuators**: Electric motors with proprietary design\n- **Compute**: Onboard AI processing\n\n**Key Capabilities**:\n```\n- Natural language task understanding (OpenAI integration)\n- End-to-end learned behaviors\n- Coffee-making demonstration\n- Real-time learning from corrections\n- Multimodal understanding (vision + language)\n```\n\n**Design Philosophy**: Figure focuses on **AI-native design**, integrating foundation models directly into the control stack.\n\n```python\n# Figure-style VLA (Vision-Language-Action) integration\nclass FigureVLAPolicy:\n    def __init__(self):\n        self.vision_encoder = VisionTransformer()\n        self.language_encoder = LanguageModel()\n        self.action_decoder = ActionTransformer()\n\ndef execute_command(self, instruction: str, video_feed):\n        # 1. Encode language instruction\n        lang_features = self.language_encoder(instruction)\n        # \"Can you hand me the apple?\"\n\n# 2. Encode visual scene\n        vis_features = self.vision_encoder(video_feed)\n\n# 3. Fuse multimodal context\n        context = self.fusion_layer(lang_features, vis_features)",
    "chapter": 1,
    "section": "3. Figure AI - Figure 02",
    "page": 10,
    "token_count": 294
  },
  {
    "chunk_id": "010ca3ed-8a53-4060-a59e-7293a37fcee2",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## 3. Figure AI - Figure 02\n\n# 4. Generate action sequence\n        actions = self.action_decoder.generate(\n            context=context,\n            max_length=100  # 100 timesteps\n        )\n\nreturn actions  # Motor commands for entire sequence\n```\n\n**Current Status**: Partnered with BMW for manufacturing deployment; raised $675M+ in funding.",
    "chapter": 1,
    "section": "3. Figure AI - Figure 02",
    "page": 11,
    "token_count": 77
  },
  {
    "chunk_id": "2c2ae9cc-3520-4e1d-a983-be1cc1c36e84",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 4. Agility Robotics - Digit\n\n**Technical Specifications**:\n- **Height**: 1.75m (5'9\")\n- **Weight**: 65 kg (143 lbs)\n- **Degrees of Freedom**: 20\n- **Payload**: 16 kg (35 lbs)\n- **Sensors**: LIDAR, depth cameras, IMU\n- **Actuators**: Electric, highly efficient\n\n**Key Capabilities**:\n```\n- Bipedal walking in constrained spaces\n- Box manipulation and stacking\n- Multi-floor navigation\n- Long battery life (4+ hours)\n- Collaborative operation with humans\n```\n\n**Design Philosophy**: Digit embodies the **task-specific optimization** approach, designed explicitly for logistics and warehousing.\n\n```python\n# Digit-style task-focused architecture\nclass DigitLogistics:\n    def pick_and_place_workflow(self, source, destination):\n        # 1. Navigate to source\n        path = self.path_planner.plan(\n            start=self.current_pose,\n            goal=source,\n            constraints=[\"avoid_humans\", \"narrow_aisles\"]\n        )\n        self.execute_path(path)\n\n# 2. Perceive and grasp\n        box = self.vision.detect_box()\n        grasp = self.grasp_planner.plan_top_down_grasp(box)\n        self.execute_grasp(grasp)",
    "chapter": 1,
    "section": "4. Agility Robotics - Digit",
    "page": 12,
    "token_count": 273
  },
  {
    "chunk_id": "ec2674e1-8858-410a-8b2f-8067475ab406",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## 4. Agility Robotics - Digit\n\n# 3. Navigate to destination\n        path = self.path_planner.plan(\n            start=self.current_pose,\n            goal=destination,\n            constraints=[\"carrying_load\"]\n        )\n        self.execute_path(path)\n\n# 4. Place\n        self.execute_place(destination)\n```\n\n**Current Status**: Deployed at Amazon warehouses; production units shipping to enterprise customers.",
    "chapter": 1,
    "section": "4. Agility Robotics - Digit",
    "page": 13,
    "token_count": 83
  },
  {
    "chunk_id": "f5ab9503-1bcd-4074-8d53-db0dc5c02364",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Comparative Analysis",
    "chapter": 1,
    "section": "Comparative Analysis",
    "page": 14,
    "token_count": 3
  },
  {
    "chunk_id": "b498a2da-4b6a-46b7-b04f-8c93d5b821d8",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Platform Comparison Matrix\n\n| Platform | Mobility | Dexterity | AI Integration | Deployment |\n|----------|----------|-----------|----------------|------------|\n| **Atlas** | ★★★★★ | ★★★☆☆ | ★★☆☆☆ | Research |\n| **Optimus** | ★★★★☆ | ★★★★★ | ★★★★☆ | Limited |\n| **Figure 02** | ★★★☆☆ | ★★★★☆ | ★★★★★ | Pilot |\n| **Digit** | ★★★★☆ | ★★★☆☆ | ★★★☆☆ | Production |",
    "chapter": 1,
    "section": "Platform Comparison Matrix",
    "page": 15,
    "token_count": 118
  },
  {
    "chunk_id": "7dba45df-c607-4a66-9708-a12b057b31ac",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Performance Metrics\n\n```\nWalking Speed:\n├─ Atlas:     2.5 m/s (running)\n├─ Optimus:   ~1.5 m/s\n├─ Figure 02: ~1.2 m/s\n└─ Digit:     ~1.5 m/s\n\nBattery Life:\n├─ Atlas:     ~1 hour (hydraulic)\n├─ Optimus:   2-4 hours (estimated)\n├─ Figure 02: 5 hours\n└─ Digit:     4+ hours\n\nPayload Capacity:\n├─ Atlas:     11 kg\n├─ Optimus:   20 kg\n├─ Figure 02: 20 kg\n└─ Digit:     16 kg\n```",
    "chapter": 1,
    "section": "Performance Metrics",
    "page": 16,
    "token_count": 164
  },
  {
    "chunk_id": "6bc80450-c05f-43a8-9dd1-9f3c16eef6eb",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Hardware Architecture: Humanoid Anatomy\n\nA modern humanoid robot comprises several integrated subsystems:\n\n```\n                    [HEAD]\n              Cameras, Microphones\n                  Compute Module\n                       |\n                  [TORSO]\n              Main Battery (24-48V)\n           Central Computer (GPU/CPU)\n              Power Distribution\n                       |\n        ┌──────────────┴──────────────┐\n     [L ARM]                        [R ARM]\n   7-DOF Manipulator              7-DOF Manipulator\n   - Shoulder (3 DOF)             - Shoulder (3 DOF)\n   - Elbow (1 DOF)                - Elbow (1 DOF)\n   - Wrist (3 DOF)                - Wrist (3 DOF)\n   Force/Torque Sensor            Force/Torque Sensor\n        |                              |\n   [L HAND]                       [R HAND]\n   Multi-finger Gripper           Multi-finger Gripper\n   Tactile Sensors                Tactile Sensors",
    "chapter": 1,
    "section": "Hardware Architecture: Humanoid Anatomy",
    "page": 17,
    "token_count": 206
  },
  {
    "chunk_id": "619ca1c2-41cb-4ec2-bee9-957ec195fab0",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Hardware Architecture: Humanoid Anatomy\n\n[PELVIS]\n               IMU, Joint Encoders\n                       |\n        ┌──────────────┴──────────────┐\n     [L LEG]                        [R LEG]\n   6-DOF Locomotion               6-DOF Locomotion\n   - Hip (3 DOF)                  - Hip (3 DOF)\n   - Knee (1 DOF)                 - Knee (1 DOF)\n   - Ankle (2 DOF)                - Ankle (2 DOF)\n   Joint Encoders                 Joint Encoders\n        |                              |\n    [L FOOT]                       [R FOOT]\n   Force Sensors                  Force Sensors\n   Ground Contact                 Ground Contact\n```",
    "chapter": 1,
    "section": "Hardware Architecture: Humanoid Anatomy",
    "page": 18,
    "token_count": 154
  },
  {
    "chunk_id": "2812ea52-f881-4189-ade3-3afd1e0600e9",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Core Subsystems\n\n#### 1. **Perception System**\n```python\nclass HumanoidPerception:\n    def __init__(self):\n        # Vision\n        self.head_cameras = RGBDCameraArray()  # Stereo pair\n        self.wrist_cameras = MonocularCamera()  # Hand-eye coordination\n\n# Proprioception\n        self.joint_encoders = EncoderArray(n_joints=28)\n        self.imu = IMU()  # Orientation and acceleration\n\n# Tactile\n        self.force_torque_sensors = FTSensorArray()\n        self.tactile_sensors = TactileSensorArray()\n\ndef get_full_state(self):\n        return {\n            \"vision\": self.head_cameras.capture(),\n            \"joint_positions\": self.joint_encoders.read(),\n            \"orientation\": self.imu.read(),\n            \"contact_forces\": self.force_torque_sensors.read(),\n            \"tactile\": self.tactile_sensors.read()\n        }\n```\n\n#### 2. **Actuation System**\n\n**Electric Actuators** (Most Common):\n- **Advantages**: Precise control, quiet operation, no hydraulic fluid\n- **Disadvantages**: Lower power density than hydraulic\n- **Examples**: Optimus, Figure, Digit\n\n**Hydraulic Actuators**:\n- **Advantages**: High power-to-weight ratio, explosive dynamics\n- **Disadvantages**: Noise, maintenance, bulky power system\n- **Examples**: Atlas",
    "chapter": 1,
    "section": "Core Subsystems",
    "page": 19,
    "token_count": 304
  },
  {
    "chunk_id": "724e30d4-a367-4cb2-80ea-c8222988dcae",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Core Subsystems\n\n**Hybrid Systems**:\n- Combining electric (precision) and hydraulic (power)\n- Emerging trend in next-gen platforms\n\n```python\nclass HumanoidActuator:\n    def __init__(self, type=\"electric\"):\n        if type == \"electric\":\n            self.motor = BLDCMotor(\n                torque_constant=0.5,  # Nm/A\n                max_current=20,        # Amperes\n                gear_ratio=100         # High reduction for torque\n            )\n        elif type == \"hydraulic\":\n            self.actuator = HydraulicCylinder(\n                max_pressure=3000,     # PSI\n                bore_diameter=25       # mm\n            )\n\ndef compute_torque(self, desired_position, current_position):\n        # PID control\n        error = desired_position - current_position\n        torque = self.pid.compute(error)\n        return self.apply_torque_limits(torque)\n```\n\n#### 3. **Compute Architecture**\n\nModern humanoids use heterogeneous computing:",
    "chapter": 1,
    "section": "Core Subsystems",
    "page": 20,
    "token_count": 208
  },
  {
    "chunk_id": "df8ac464-f8d2-484e-89cc-579bee3cef11",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Core Subsystems\n\n```\n┌─────────────────────────────────────────┐\n│           High-Level Planning           │\n│     (CPU: Intel i7/AMD Ryzen)          │\n│  - Task planning                        │\n│  - Language understanding               │\n│  - Navigation planning                  │\n└───────────────┬─────────────────────────┘\n                │\n┌───────────────▼─────────────────────────┐\n│         Perception & Learning           │\n│     (GPU: NVIDIA Jetson/A100)          │\n│  - Vision models                        │\n│  - VLA policies                         │\n│  - SLAM                                 │\n└───────────────┬─────────────────────────┘\n                │\n┌───────────────▼─────────────────────────┐\n│        Real-Time Control                │\n│   (Microcontroller: ARM Cortex-M7)     │\n│  - Motor control (1kHz)                 │\n│  - Sensor reading                       │\n│  - Safety monitoring                    │\n└─────────────────────────────────────────┘\n```",
    "chapter": 1,
    "section": "Core Subsystems",
    "page": 21,
    "token_count": 240
  },
  {
    "chunk_id": "52c37519-ea79-45b9-9c1d-a32b493bd7fa",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Design Philosophies: Different Approaches",
    "chapter": 1,
    "section": "Design Philosophies: Different Approaches",
    "page": 22,
    "token_count": 8
  },
  {
    "chunk_id": "616899e4-a4fb-4c64-a09c-25991ae6d848",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 1. **Dynamics-First** (Boston Dynamics)\n\n**Principle**: Master physics and dynamics first, add AI later.\n\n```python\n# Emphasis on model-based control\nclass DynamicsFirstController:\n    def control(self, state, goal):\n        # Use accurate physical models\n        predicted_dynamics = self.physics_model.predict(\n            state=state,\n            action=candidate_action\n        )\n\n        # Optimize over physics constraints\n        optimal_action = self.mpc.solve(\n            model=predicted_dynamics,\n            constraints=[\"stability\", \"joint_limits\"],\n            objective=goal\n        )\n\n        return optimal_action\n```\n\n**Pros**: Robust, predictable, excellent dynamic performance\n**Cons**: Requires extensive modeling, less adaptable",
    "chapter": 1,
    "section": "1. **Dynamics-First** (Boston Dynamics)",
    "page": 23,
    "token_count": 153
  },
  {
    "chunk_id": "eb929a36-c908-4f99-8791-d2c66c04e9e0",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 2. **Vision-First** (Tesla)\n\n**Principle**: Use vision as primary sensor, learn end-to-end policies.\n\n```python\n# Camera-only perception\nclass VisionFirstController:\n    def control(self, camera_feeds):\n        # No LIDAR, no depth sensors\n        # Pure neural network inference\n        action = self.neural_policy(camera_feeds)\n        return action\n```\n\n**Pros**: Scalable data collection, cost-effective sensors\n**Cons**: Requires massive datasets, challenging in low-light",
    "chapter": 1,
    "section": "2. **Vision-First** (Tesla)",
    "page": 24,
    "token_count": 108
  },
  {
    "chunk_id": "6794d396-b1ee-496b-86c5-3b506fb48e03",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 3. **AI-Native** (Figure)\n\n**Principle**: Integrate foundation models throughout the stack.\n\n```python\n# Language-conditioned control\nclass AINativeController:\n    def control(self, vision, language_instruction):\n        # VLA: Vision-Language-Action\n        action_sequence = self.foundation_model(\n            vision=vision,\n            text=language_instruction\n        )\n        return action_sequence\n```\n\n**Pros**: Flexible, natural interaction, rapid task learning\n**Cons**: Computationally intensive, requires large models",
    "chapter": 1,
    "section": "3. **AI-Native** (Figure)",
    "page": 25,
    "token_count": 110
  },
  {
    "chunk_id": "8ddf5766-b475-49b5-9077-dcd8e49ef570",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### 4. **Task-Optimized** (Agility)\n\n**Principle**: Design for specific use cases, optimize for reliability.\n\n```python\n# Specialized for logistics\nclass TaskOptimizedController:\n    def control(self, task_type):\n        # Pre-programmed behaviors\n        if task_type == \"pick_box\":\n            return self.pick_box_routine()\n        elif task_type == \"navigate\":\n            return self.navigate_routine()\n```\n\n**Pros**: Reliable, energy-efficient, production-ready\n**Cons**: Less general-purpose, limited adaptability",
    "chapter": 1,
    "section": "4. **Task-Optimized** (Agility)",
    "page": 26,
    "token_count": 113
  },
  {
    "chunk_id": "eb128fe4-5d87-43d6-b16c-bc8bf58107c1",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Current Limitations\n\nDespite rapid progress, humanoid robots still face significant challenges:",
    "chapter": 1,
    "section": "Current Limitations",
    "page": 27,
    "token_count": 16
  },
  {
    "chunk_id": "8964552c-5216-4d03-a212-adcfda62a569",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Technical Limitations\n\n1. **Battery Life**\n   - Current: 2-5 hours\n   - Target: 8+ hours for full shifts\n   - Challenge: High power consumption of actuators\n\n2. **Dexterity**\n   - Current: Can handle rigid objects, struggle with deformables\n   - Target: Human-level manipulation of all materials\n   - Challenge: Tactile sensing and control\n\n3. **Adaptability**\n   - Current: Excel in structured environments\n   - Target: Handle novel situations without retraining\n   - Challenge: Generalization and few-shot learning\n\n4. **Speed**\n   - Current: 1-2 m/s walking\n   - Target: Human-level running and dynamic movement\n   - Challenge: Balance and energy efficiency",
    "chapter": 1,
    "section": "Technical Limitations",
    "page": 28,
    "token_count": 160
  },
  {
    "chunk_id": "4dfbc645-f2fc-49d8-b7de-8a0eb5eeefa1",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Economic Limitations\n\n```\nCost Breakdown (Estimated):\n├─ Hardware Components: $50k-$150k\n│  ├─ Actuators: $20k-$60k\n│  ├─ Sensors: $10k-$30k\n│  ├─ Compute: $5k-$20k\n│  └─ Structure: $15k-$40k\n├─ Software Development: $500k-$2M\n└─ Target Commercial Price: $20k-$50k (for mass adoption)\n```\n\nTo achieve commercial viability, costs must drop 5-10x.",
    "chapter": 1,
    "section": "Economic Limitations",
    "page": 29,
    "token_count": 128
  },
  {
    "chunk_id": "cc4e735e-14c7-45c2-a317-8f61d6c560d0",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Future Directions",
    "chapter": 1,
    "section": "Future Directions",
    "page": 30,
    "token_count": 3
  },
  {
    "chunk_id": "0e0c320a-6eee-427c-8379-f7e7c5c391b3",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Near-Term (2024-2026)\n\n- **Improved dexterity**: Human-like hands with 20+ DOF\n- **Better AI integration**: Larger VLAs, faster inference\n- **Extended battery**: 8+ hour operation\n- **Cost reduction**: Manufacturing at scale",
    "chapter": 1,
    "section": "Near-Term (2024-2026)",
    "page": 31,
    "token_count": 59
  },
  {
    "chunk_id": "6db12404-2b54-4007-b607-8e225caace19",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Medium-Term (2026-2030)\n\n- **General-purpose platforms**: One robot, many tasks\n- **Collaborative autonomy**: Seamless human-robot teamwork\n- **Learning from demonstration**: Rapid skill acquisition\n- **Consumer markets**: Home assistance robots",
    "chapter": 1,
    "section": "Medium-Term (2026-2030)",
    "page": 32,
    "token_count": 54
  },
  {
    "chunk_id": "a213d014-e43a-4cb5-8911-4f1d875e6cba",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Long-Term (2030+)\n\n- **Humanoid workforce**: Robots as economic agents\n- **Full autonomy**: No human supervision needed\n- **Artificial general intelligence**: AGI in physical form\n- **Mass production**: Millions of units per year",
    "chapter": 1,
    "section": "Long-Term (2030+)",
    "page": 33,
    "token_count": 53
  },
  {
    "chunk_id": "1b58380c-7ba9-44fa-aa0e-8e99c64d01c6",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Industry Ecosystem\n\nBeyond the robot platforms themselves, a rich ecosystem supports development:",
    "chapter": 1,
    "section": "Industry Ecosystem",
    "page": 34,
    "token_count": 17
  },
  {
    "chunk_id": "c4e98aa0-e278-4404-8c46-e60a38d4a583",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Component Suppliers\n- **Actuators**: Harmonic Drive, Maxon Motors\n- **Sensors**: Intel RealSense, Velodyne, FLIR\n- **Compute**: NVIDIA (Jetson, AGX), Intel",
    "chapter": 1,
    "section": "Component Suppliers",
    "page": 35,
    "token_count": 47
  },
  {
    "chunk_id": "820f285c-c93f-4f55-80ef-01daf3ab3a9f",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Software Platforms\n- **ROS2**: Universal middleware\n- **NVIDIA Isaac**: Simulation and deployment\n- **MuJoCo**: Physics simulation\n- **PyTorch/TensorFlow**: AI frameworks",
    "chapter": 1,
    "section": "Software Platforms",
    "page": 36,
    "token_count": 42
  },
  {
    "chunk_id": "58a2a525-73b4-4c5b-991d-0a74f758516a",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "### Research Institutions\n- MIT CSAIL, CMU Robotics Institute, Stanford\n- ETH Zurich, University of Tokyo\n- Industry labs: Google DeepMind, Meta AI",
    "chapter": 1,
    "section": "Research Institutions",
    "page": 37,
    "token_count": 34
  },
  {
    "chunk_id": "ff84d2a2-4d26-40a9-b08c-69026f68ee00",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Summary\n\nThe humanoid robotics landscape is rapidly evolving from research curiosity to commercial reality:\n\n✅ **Multiple viable approaches**: Dynamics-first, vision-first, AI-native, task-optimized\n✅ **Production deployments**: Moving beyond labs into factories and warehouses\n✅ **Technical maturity**: 2-5 hour battery life, 10-20 kg payloads, human-scale mobility\n✅ **AI integration**: Foundation models enabling natural language control\n✅ **Economic momentum**: Billions in investment, clear path to commercialization\n\n**Key Remaining Challenges**:\n- Cost reduction (5-10x needed)\n- Battery life extension\n- Dexterity improvements\n- Robustness in unstructured environments\n\n:::tip Looking Forward\nThe next 5 years will be critical. Success requires simultaneously advancing hardware (cheaper, more efficient), software (better AI), and manufacturing (scale production). The winners will be those who can deliver capable, affordable, reliable humanoid platforms.\n:::",
    "chapter": 1,
    "section": "Summary",
    "page": 38,
    "token_count": 198
  },
  {
    "chunk_id": "f711c3c4-8597-4faf-be12-4cc80ac46bb2",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Further Reading\n\n- [Boston Dynamics Atlas Technical Overview](https://www.bostondynamics.com/atlas)\n- [Tesla AI Day 2023: Optimus Update](https://www.youtube.com/tesla)\n- [Figure AI: Towards General Purpose Robots](https://www.figure.ai/)\n- [IEEE Spectrum: Humanoid Robots](https://spectrum.ieee.org/robotics)",
    "chapter": 1,
    "section": "Further Reading",
    "page": 39,
    "token_count": 81
  },
  {
    "chunk_id": "9d3a958d-43ad-4541-ad4b-f3f7cb9253aa",
    "doc_id": "9808dfb0-ac21-4aa6-973e-10f345457f38",
    "chunk_text": "## Discussion Questions\n\n1. Why are multiple companies converging on humanoid form factors rather than specialized designs?\n2. What are the trade-offs between hydraulic and electric actuators for humanoid robots?\n3. How might foundation models change the way we program robots?\n4. What applications will drive the first large-scale deployments?\n5. What technical breakthrough would have the biggest impact on humanoid robot capabilities?\n\n---\n\n**Next**: [Sensor Systems for Physical AI](/docs/chapter1/sensor-systems)",
    "chapter": 1,
    "section": "Discussion Questions",
    "page": 40,
    "token_count": 99
  },
  {
    "chunk_id": "f3e68822-415b-44b5-8583-ea911dd21176",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "# Introduction to Physical AI",
    "chapter": 1,
    "section": "Introduction",
    "page": 1,
    "token_count": 5
  },
  {
    "chunk_id": "f6dc6f05-5616-4690-913d-5f970e22655d",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## 🎯 Learning Objectives\n\nBy the end of this section, you will:\n\n- Understand what Physical AI means and why it matters\n- Learn the key differences between digital and physical AI\n- Explore real-world applications of embodied intelligence\n- Recognize the challenges unique to physical AI systems\n- Understand the fundamental architecture of Physical AI systems",
    "chapter": 1,
    "section": "🎯 Learning Objectives",
    "page": 2,
    "token_count": 69
  },
  {
    "chunk_id": "134d14e7-e371-4393-880a-d08d4e6a73bc",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Overview\n\nPhysical AI, also known as embodied intelligence, represents the convergence of artificial intelligence and robotics in real-world environments. Unlike traditional AI systems that operate purely in digital spaces, Physical AI enables machines to understand, interact with, and manipulate the physical world. This emerging field bridges the gap between computational intelligence and tangible robotic systems, creating machines that can perceive, reason, and act in three-dimensional space with an understanding of physics, materials, and environmental dynamics.\n\nThe development of Physical AI is transforming robotics from pre-programmed, task-specific machines into adaptive, intelligent systems capable of learning from physical interaction. This paradigm shift is particularly significant for humanoid robotics, where machines must navigate complex environments designed for humans while performing tasks that require dexterity, balance, and social interaction.",
    "chapter": 1,
    "section": "Overview",
    "page": 3,
    "token_count": 153
  },
  {
    "chunk_id": "f48f52ef-3396-459e-98f4-04b9f5bb8409",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## What is Physical AI?\n\n**Physical AI** refers to artificial intelligence systems that exist in and interact with the physical world through robotic embodiments. Physical AI is the integration of artificial intelligence into physical systems that can interact with the real world. These systems combine sensing, reasoning, and action capabilities to understand and manipulate physical objects and environments.",
    "chapter": 1,
    "section": "What is Physical AI?",
    "page": 4,
    "token_count": 66
  },
  {
    "chunk_id": "1270b653-5d06-4a85-bb9f-a5dc55d0daf3",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Key Characteristics\n\n- **Physical Embodiment**: The AI system exists in and interacts with the physical world through a robotic body\n- **Real-World Learning**: The ability to acquire knowledge through physical interaction and sensory feedback\n- **Physics Understanding**: Comprehension of physical laws, forces, and material properties\n- **Adaptive Behavior**: Capability to adjust actions based on environmental changes and physical constraints\n\nUnlike traditional AI that operates purely in digital environments (like chatbots or image classifiers), Physical AI must:\n\n- Sense the physical world through cameras, LIDAR, force sensors, etc.\n- Understand 3D space, physics, and object properties\n- Plan actions that respect physical constraints\n- Execute movements safely in real-time\n- Handle uncertainty, noise, and unexpected situations",
    "chapter": 1,
    "section": "Key Characteristics",
    "page": 5,
    "token_count": 158
  },
  {
    "chunk_id": "f2ce70fe-c417-489a-bba6-00d18a116ab1",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Differences Between Traditional AI and Physical AI\n\n| Traditional AI | Physical AI |\n|----------------|-------------|\n| Operates in digital/virtual environments | Interacts with physical world |\n| Processes abstract data (text, images, numbers) | Manipulates physical objects and materials |\n| No real-world consequences for errors | Mistakes can have physical consequences |\n| Simulated environments only | Real-world testing and validation required |\n| No understanding of physics needed | Physics understanding essential |\n| Can be trained on static datasets | Requires continuous interaction with environment |\n| Operates on data | Operates in 3D space |\n| Instant computation | Real-time constraints |\n| Perfect information | Noisy sensors |\n| Virtual consequences | Physical safety critical |\n| Easy to parallelize | Limited by hardware |\n| Costless experimentation | Expensive data collection |",
    "chapter": 1,
    "section": "Differences Between Traditional AI and Physical AI",
    "page": 6,
    "token_count": 166
  },
  {
    "chunk_id": "ecf8c51a-80f3-4448-9a7b-15f211791393",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Applications and Use Cases\n\nPhysical AI has diverse applications across multiple domains:",
    "chapter": 1,
    "section": "Applications and Use Cases",
    "page": 7,
    "token_count": 15
  },
  {
    "chunk_id": "6a8ae4de-3c73-4b30-b5b9-064f7d48c9c0",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Manufacturing\n- Adaptive robotic assembly lines\n- Quality control and inspection\n- Material handling and sorting\n- Flexible automation systems",
    "chapter": 1,
    "section": "Manufacturing",
    "page": 8,
    "token_count": 25
  },
  {
    "chunk_id": "7d377f3c-a3ce-4ea1-b803-3be3a025bf2c",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Healthcare\n- Surgical robots for minimally invasive procedures\n- Rehabilitation systems and assistive devices\n- Patient care and mobility assistance\n- Laboratory automation",
    "chapter": 1,
    "section": "Healthcare",
    "page": 9,
    "token_count": 30
  },
  {
    "chunk_id": "1367c701-8263-4f0f-a46d-fbd52dbbfbfe",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Service Industry\n- Hospitality robots for customer service\n- Automated cleaning systems\n- Restaurant and food service automation\n- Retail assistance",
    "chapter": 1,
    "section": "Service Industry",
    "page": 10,
    "token_count": 26
  },
  {
    "chunk_id": "7d3e4ce9-9d08-4e2c-b431-152a58fad9e9",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Logistics\n- Warehouse automation and inventory management\n- Package handling and sorting\n- Autonomous delivery systems\n- Supply chain optimization",
    "chapter": 1,
    "section": "Logistics",
    "page": 11,
    "token_count": 25
  },
  {
    "chunk_id": "6da9fc8a-e8c9-412d-9965-77eaa4e26ff0",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Home Assistance\n- Domestic robots for cleaning and maintenance\n- Cooking and meal preparation assistance\n- Elderly care and companionship\n- Smart home integration",
    "chapter": 1,
    "section": "Home Assistance",
    "page": 12,
    "token_count": 31
  },
  {
    "chunk_id": "6010a462-e574-4b24-b0c6-2d252a1c8239",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Agriculture\n- Autonomous farming equipment\n- Crop monitoring and analysis\n- Precision harvesting systems\n- Livestock management",
    "chapter": 1,
    "section": "Agriculture",
    "page": 13,
    "token_count": 24
  },
  {
    "chunk_id": "fd74f70b-7d07-4fc9-a7a0-83a3902fb881",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Disaster Response\n- Search and rescue robots\n- Hazardous environment exploration\n- Infrastructure inspection\n- Emergency response support",
    "chapter": 1,
    "section": "Disaster Response",
    "page": 14,
    "token_count": 24
  },
  {
    "chunk_id": "d2ad7234-ee38-4eb4-a2d9-336636e76eeb",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Challenges and Opportunities\n\nPhysical AI faces several technical challenges that also present significant opportunities for advancement:",
    "chapter": 1,
    "section": "Challenges and Opportunities",
    "page": 15,
    "token_count": 19
  },
  {
    "chunk_id": "79ea0736-3ba5-4d59-9e33-48266addd93d",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Challenges\n\n- **Safety**: Ensuring robots can operate safely around humans and delicate environments\n- **Real-time Processing**: Managing computational demands for real-time decision making\n- **Physics Simulation**: Creating accurate models of complex real-world physics\n- **Material Interaction**: Understanding diverse material properties and behaviors\n- **Uncertainty Handling**: Managing unpredictable environmental conditions\n- **Sensor Noise**: Dealing with imperfect, noisy sensor data\n- **Model Uncertainty**: Handling prediction errors that can lead to physical failures\n- **Latency**: Meeting strict timing constraints for responsive control\n- **Cost**: Expensive hardware, sensors, and data collection",
    "chapter": 1,
    "section": "Challenges",
    "page": 16,
    "token_count": 127
  },
  {
    "chunk_id": "1b5c8352-caec-41d3-b1b7-e641a184566f",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Opportunities\n\n- **Human-Robot Collaboration**: Developing intuitive interfaces for human-robot teamwork\n- **Adaptive Learning**: Creating systems that continuously improve through experience\n- **Cross-Domain Transfer**: Applying learned skills across different environments\n- **Social Integration**: Building robots that understand human social cues and norms\n- **Sim-to-Real Transfer**: Leveraging simulation for safer, faster development\n- **Multimodal Learning**: Combining vision, touch, and other senses for richer understanding",
    "chapter": 1,
    "section": "Opportunities",
    "page": 17,
    "token_count": 98
  },
  {
    "chunk_id": "a771fee6-1af2-461e-aa9b-15e0a985f812",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Real-World Examples\n\nPhysical AI systems are already deployed in various real-world scenarios:",
    "chapter": 1,
    "section": "Real-World Examples",
    "page": 18,
    "token_count": 18
  },
  {
    "chunk_id": "c49b50b6-1600-46ce-82e4-5293dcd13f06",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Tesla Optimus\nA humanoid robot designed to perform tasks safely alongside humans, demonstrating how Physical AI can enable human-like interaction with the physical environment. Optimus showcases integrated perception, learning, and manipulation in humanoid form.",
    "chapter": 1,
    "section": "Tesla Optimus",
    "page": 19,
    "token_count": 44
  },
  {
    "chunk_id": "12b57f10-647c-42ca-a3c0-744580161846",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Boston Dynamics Robots\nSystems like Spot and Atlas showcase advanced physical intelligence through dynamic movement, manipulation, and environmental interaction capabilities. These robots demonstrate sophisticated balance, navigation, and task execution in challenging environments.",
    "chapter": 1,
    "section": "Boston Dynamics Robots",
    "page": 20,
    "token_count": 40
  },
  {
    "chunk_id": "a50ea00a-9f58-46c2-a715-b25c830922c1",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Amazon Warehouse Robots\nThese systems demonstrate embodied intelligence through their ability to navigate complex warehouse environments, recognize and manipulate packages, and coordinate with human workers in real-time for efficient logistics operations.",
    "chapter": 1,
    "section": "Amazon Warehouse Robots",
    "page": 21,
    "token_count": 37
  },
  {
    "chunk_id": "b2e9f5f3-2959-4a5a-bebb-10da62d48f47",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Da Vinci Surgical Systems\nSurgical robots that combine AI-assisted precision with physical manipulation for minimally invasive procedures, demonstrating Physical AI's potential in healthcare applications requiring extreme precision.",
    "chapter": 1,
    "section": "Da Vinci Surgical Systems",
    "page": 22,
    "token_count": 36
  },
  {
    "chunk_id": "6f215c43-10b8-403d-b511-778173171593",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Waymo Self-Driving Cars\nAutonomous vehicles that navigate real-world traffic, demonstrating perception, planning, and control in dynamic, safety-critical environments.",
    "chapter": 1,
    "section": "Waymo Self-Driving Cars",
    "page": 23,
    "token_count": 32
  },
  {
    "chunk_id": "5196d90d-6eb5-4a16-8748-800edafafcf0",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Physical AI System Architecture\n\nA Physical AI system integrates multiple components:",
    "chapter": 1,
    "section": "Physical AI System Architecture",
    "page": 24,
    "token_count": 14
  },
  {
    "chunk_id": "572823c0-8449-40ee-b3eb-09a16ac420d4",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Physical AI System Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    Physical AI System                           │\n├─────────────────────────────────────────────────────────────────┤\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │   Sensors   │───▶│ Intelligence│───▶│ Actuators   │         │\n│  │             │    │             │    │             │         │\n│  │ • Cameras   │    │ • Perception│    │ • Motors    │         │\n│  │ • IMU       │    │ • Learning  │    │ • Grippers  │         │\n│  │ • Force/Torque│  │ • Reasoning │    │ • Hydraulics│         │\n│  │ • LIDAR     │    │ • Planning  │    │ • Pneumatics│         │\n│  │ • Encoders  │    │ • Memory    │    │ • Brakes    │         │\n│  └─────────────┘    └─────────────┘    └─────────────┘         │\n└─────────────────────────────────────────────────────────────────┘\n                    │\n                    ▼\n          ┌─────────────────┐\n          │ Physical World  │\n          │ • Objects       │\n          │ • Environments  │\n          │ • Physics       │\n          │ • Humans        │\n          └─────────────────┘\n```",
    "chapter": 1,
    "section": "Physical AI System Architecture",
    "page": 25,
    "token_count": 338
  },
  {
    "chunk_id": "b1692d5a-0f15-42cb-aeb6-44ff0cc35cd0",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Physical AI vs Traditional AI Comparison\n\n```\nTraditional AI:              Physical AI:\n┌─────────────────┐         ┌─────────────────┐\n│  Digital World  │         │ Physical World  │\n│                 │         │                 │\n│  Data in  ──────┼────────▶│  Sensors ──────▶│─────┐\n│                 │         │                 │     │\n│  AI System      │         │  AI System      │     │\n│                 │         │                 │     │\n│  Data out ←─────┼─────────│  Actuators ◀────│─────┘\n└─────────────────┘         └─────────────────┘\n```",
    "chapter": 1,
    "section": "Physical AI vs Traditional AI Comparison",
    "page": 26,
    "token_count": 161
  },
  {
    "chunk_id": "279c3140-4381-405a-86d4-e2a2e3da5b0a",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## The Reality Gap\n\nMoving from digital AI to Physical AI involves bridging the \"reality gap\" between simulation and real-world deployment:",
    "chapter": 1,
    "section": "The Reality Gap",
    "page": 27,
    "token_count": 28
  },
  {
    "chunk_id": "4e53a044-3681-45fa-8a7d-7b50a9aac306",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Simulation vs Reality\n- **Simulation**: Perfect sensors, exact physics, no failures\n- **Reality**: Noisy sensors, approximated physics, hardware failures",
    "chapter": 1,
    "section": "Simulation vs Reality",
    "page": 28,
    "token_count": 32
  },
  {
    "chunk_id": "ad836d1f-3f6c-4fea-9ffd-16414bb626e9",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "### Key Differences\n- **Sensor Noise**: Real sensors provide imperfect, noisy data\n- **Physics Approximations**: Real-world physics is more complex than simulators\n- **Timing Constraints**: Real-time control requires meeting strict deadlines\n- **Safety Requirements**: Physical mistakes can cause damage or injury\n- **Cost**: Real-world testing is expensive and time-consuming",
    "chapter": 1,
    "section": "Key Differences",
    "page": 29,
    "token_count": 71
  },
  {
    "chunk_id": "06124539-3f0f-45fb-9b57-b56465a87d9b",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Summary\n\nPhysical AI represents a fundamental shift in how we think about artificial intelligence:\n\n✅ **Integration with the physical world** - Not just processing data, but acting in 3D space\n✅ **Real-time responsiveness** - Must react in milliseconds, not seconds\n✅ **Safety-critical design** - Physical actions have real consequences\n✅ **Robustness requirements** - Must handle noise, uncertainty, and failures\n✅ **Continuous learning** - Improves through physical interaction and experience\n\n:::tip Key Takeaway\nPhysical AI isn't just digital AI with motors attached. It requires fundamentally different approaches to perception, planning, control, and safety to successfully operate in the unpredictable real world.\n:::",
    "chapter": 1,
    "section": "Summary",
    "page": 30,
    "token_count": 147
  },
  {
    "chunk_id": "fec362ba-652a-4f15-945a-9c6b0b90afc2",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Learning Outcomes Summary\n\nAfter completing this section, you should be able to:\n\n1. Define Physical AI and explain its key characteristics\n2. Distinguish between traditional AI and Physical AI systems\n3. Identify applications of Physical AI across different domains\n4. Recognize the challenges and opportunities in Physical AI development\n5. Understand the fundamental architecture of Physical AI systems\n6. Explain the \"reality gap\" and why it matters",
    "chapter": 1,
    "section": "Learning Outcomes Summary",
    "page": 31,
    "token_count": 88
  },
  {
    "chunk_id": "256ec29b-3953-4504-915d-6733a72ea65a",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Further Reading\n\n- \"Embodied Artificial Intelligence: A Review\" (ITU-T Workshop, 2025)\n- \"Physical AI and Humanoid Robotics: Current State and Future Directions\" (Cambridge Consultants, 2025)\n- \"NVIDIA's Approach to Physical AI and Embodied Intelligence\" (Zeal 3D Printing, 2025)",
    "chapter": 1,
    "section": "Further Reading",
    "page": 32,
    "token_count": 73
  },
  {
    "chunk_id": "50e2b6d7-0754-4a60-bf99-92f8c1ac2989",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## References\n\n[1] ITU-T Workshop on AI for Digital Transformation. \"What is embodied artificial intelligence and why it matters.\" October 10, 2025. https://www.itu.int/en/ITU-T/Workshops-and-Seminars/2025/1010/Documents/Wei%20Kai.pdf\n\n[2] Cambridge Consultants. \"Physical AI and humanoid robotics are at a turning point.\" July 22, 2025. https://www.cambridgeconsultants.com/physical-ai-and-humanoid-robotics-at-a-turning-point/\n\n[3] Zeal 3D Printing. \"Physical AI: How NVIDIA and Tesla Are Shaping Embodied Intelligence.\" October 7, 2025. https://www.zeal3dprinting.com.au/physical-ai-2-0-how-nvidia-tesla-ecosystem-players-are-shaping-the-future-of-embodied-intelligence/\n\n---",
    "chapter": 1,
    "section": "References",
    "page": 33,
    "token_count": 196
  },
  {
    "chunk_id": "0c5c7e46-dd9c-44a2-bcb8-7cf5f5baba9f",
    "doc_id": "546d9337-3cba-4e38-9c4c-1896476cad1c",
    "chunk_text": "## Next Steps\n\nNow that you understand what Physical AI is, let's explore:\n- **Embodied Intelligence** - The theoretical foundations of physical embodiment\n- **Digital to Physical Transition** - Technical details of implementing Physical AI\n- **Humanoid Robots** - Current platforms and technologies\n- **Sensor Systems** - How robots perceive the world",
    "chapter": 1,
    "section": "Next Steps",
    "page": 34,
    "token_count": 69
  },
  {
    "chunk_id": "f01264a0-b2bb-43f9-a380-f372db8051cb",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "# Physical Laws in Robotics",
    "chapter": 1,
    "section": "Introduction",
    "page": 1,
    "token_count": 5
  },
  {
    "chunk_id": "938d3cb5-ee2a-4dee-a490-5abc864d6e6a",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## Overview\n\nRobotics operates within the constraints of fundamental physical laws that govern motion, force, energy, and interaction with the environment. Understanding these laws is critical for designing, controlling, and operating robots effectively. Unlike software systems that operate in virtual environments, robots must comply with real-world physics at all times. The application of physical laws in robotics spans from basic movement and manipulation to complex control strategies and safety considerations.\n\nThe integration of physical laws into robotic systems affects every aspect of robot design and operation, from mechanical structure and actuator selection to control algorithms and navigation strategies. As robots become more sophisticated and operate in unstructured environments, their compliance with physical laws becomes increasingly complex and nuanced.",
    "chapter": 1,
    "section": "Overview",
    "page": 2,
    "token_count": 137
  },
  {
    "chunk_id": "00ecb023-5dff-4fa5-aa8c-0af88ea37e97",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## Key Concepts",
    "chapter": 1,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "564b49f5-6c38-4b2c-a6d2-092b5b7f32a1",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Newton's Laws of Motion in Robotics\n\n#### First Law (Law of Inertia)\nAn object at rest stays at rest, and an object in motion stays in motion at the same speed and in the same direction unless acted upon by an unbalanced force. In robotics, this law affects:\n- The power required to start or stop robot movement\n- The need for braking systems in mobile robots\n- The design of manipulator arms to handle inertia during acceleration/deceleration\n\n#### Second Law (F = ma)\nThe acceleration of an object depends on the mass of the object and the amount of force applied. For robots, this law determines:\n- Actuator sizing based on required forces and accelerations\n- Torque requirements for joints with varying loads\n- Power consumption calculations for different motion profiles\n\n#### Third Law (Action-Reaction)\nFor every action, there is an equal and opposite reaction. This law is crucial in robotics for:\n- Understanding reaction forces during manipulation tasks\n- Designing stable walking gaits for legged robots\n- Managing forces during robot-to-environment interactions",
    "chapter": 1,
    "section": "Newton's Laws of Motion in Robotics",
    "page": 4,
    "token_count": 218
  },
  {
    "chunk_id": "78a9aa75-60fc-47bf-8cd2-56a46a112c1b",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Kinematics and Dynamics\n\n#### Kinematics\nKinematics deals with the motion of robot components without considering the forces that cause the motion. It includes:\n\n**Forward Kinematics**: Determining the position and orientation of the robot's end-effector based on joint angles.\n\n**Inverse Kinematics**: Calculating the required joint angles to achieve a desired end-effector position and orientation.\n\n#### Dynamics\nDynamics involves the study of forces that cause or change motion. In robotics, this includes:\n\n**Rigid Body Dynamics**: Modeling the motion of robot links under applied forces and torques.\n\n**Lagrangian Mechanics**: A mathematical framework for modeling complex robotic systems with multiple degrees of freedom.\n\n**Euler-Lagrange Equations**: Used to derive the equations of motion for robotic systems, considering kinetic and potential energy.",
    "chapter": 1,
    "section": "Kinematics and Dynamics",
    "page": 5,
    "token_count": 164
  },
  {
    "chunk_id": "4a11e2db-eb27-4551-a8a4-a119dac92be0",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Forces, Torques, and Equilibrium\n\nRobots must manage various forces and torques:\n\n- **Gravitational Forces**: Affecting robot stability and required actuator forces\n- **Frictional Forces**: Both beneficial (for gripping) and problematic (energy loss in joints)\n- **Contact Forces**: During interaction with objects or environment\n- **Centripetal and Centrifugal Forces**: Important in high-speed rotations and curved motions",
    "chapter": 1,
    "section": "Forces, Torques, and Equilibrium",
    "page": 6,
    "token_count": 90
  },
  {
    "chunk_id": "8f0382e2-45d1-4478-831a-0bea853bc8bf",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Energy Conservation in Robotic Systems\n\nEnergy considerations in robotics include:\n- **Potential Energy**: Related to the robot's position in gravitational field\n- **Kinetic Energy**: Related to robot motion and speed\n- **Energy Efficiency**: Critical for battery-powered robots\n- **Regenerative Braking**: Capturing energy during deceleration",
    "chapter": 1,
    "section": "Energy Conservation in Robotic Systems",
    "page": 7,
    "token_count": 67
  },
  {
    "chunk_id": "6891d29a-a214-4559-b936-bbf093de081b",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## Examples",
    "chapter": 1,
    "section": "Examples",
    "page": 8,
    "token_count": 2
  },
  {
    "chunk_id": "99357a0a-f57b-4e93-8997-a74f0cbce163",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Practical Applications of Physical Laws in Robotics\n\n**Manipulation Tasks**: When a robot arm picks up an object, Newton's laws determine the required forces. The robot must apply sufficient force to overcome the object's weight (gravity) and accelerate it to the desired velocity without causing damage through excessive force.\n\n**Locomotion**: Walking robots like humanoid systems must carefully manage their center of mass and apply forces according to Newton's third law. Each step requires coordinated forces to maintain balance while pushing against the ground.\n\n**Mobile Robots**: Wheeled robots must account for friction between wheels and ground for traction. Newton's second law determines the acceleration possible with given motor power.\n\n**Aerial Robots**: Drones must generate sufficient lift to counteract gravity (Newton's third law: action-reaction between propellers and air).",
    "chapter": 1,
    "section": "Practical Applications of Physical Laws in Robotics",
    "page": 9,
    "token_count": 162
  },
  {
    "chunk_id": "dff53a6a-665e-40ae-8a3d-5aee748a75de",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Control System Implications\n\n**PID Controllers**: Account for the dynamic behavior of robotic systems, adjusting control effort based on error, accumulated error, and rate of error change.\n\n**Model-Based Control**: Uses mathematical models based on physical laws to predict robot behavior and optimize control strategies.\n\n**Impedance Control**: Manages the robot's mechanical impedance (resistance to motion) to safely interact with the environment.",
    "chapter": 1,
    "section": "Control System Implications",
    "page": 10,
    "token_count": 82
  },
  {
    "chunk_id": "065d4994-a128-4b63-9cc1-b3511e85ef4c",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Safety and Compliance\n\n**Force Limiting**: Ensuring robots don't apply excessive forces during human interaction.\n\n**Collision Avoidance**: Using physical models to predict and avoid collisions.\n\n**Emergency Stops**: Calculating braking forces to safely stop robot motion.",
    "chapter": 1,
    "section": "Safety and Compliance",
    "page": 11,
    "token_count": 50
  },
  {
    "chunk_id": "b4cd1257-a868-4052-8ddc-49dd3a3caae9",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## Diagrams",
    "chapter": 1,
    "section": "Diagrams",
    "page": 12,
    "token_count": 3
  },
  {
    "chunk_id": "a79b9e13-520d-46b4-ba1a-fc9dd3148ade",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Newton's Laws Applied to Robotics\n\n```\nNewton's First Law in Robotics:\n┌─────────────────┐    No motion until force applied\n│   Robot at Rest │ ────────────── F ──────▶\n└─────────────────┘    (e.g., motor torque)\n\nNewton's Second Law in Robotics:\nForce Applied (F)     Robot Acceleration (a)\n      │                    ▲\n      │                    │\n      └────────────────────┘\n           F = m × a\n       (e.g., joint torque creates angular acceleration)\n\nNewton's Third Law in Robotics:\nRobot Hand ────── F ──────▶ Object\n(exerts force)         (reacts with equal force)\n     ▲                      │\n     │                      │\n     └─── Equal Force ──────┘\n         (reaction on robot)\n```",
    "chapter": 1,
    "section": "Newton's Laws Applied to Robotics",
    "page": 13,
    "token_count": 188
  },
  {
    "chunk_id": "74669d0a-f6df-478d-b804-342642d0ab98",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Forces in Robotic Manipulation\n\n```\n┌─────────────────────────────────────────────────┐\n│           Robotic Manipulation Forces           │\n│                                                 │\n│  ┌─────────┐                                    │\n│  │  Object │                                    │\n│  │  (mass) │                                    │\n│  └────┬────┘                                    │\n│       │                                         │\n│    ┌──▼──┐         Applied Force                │\n│    │Hand │◀───────── Grasping Force             │\n│    │Grip │       (maintains grasp against       │\n│    └──┬──┘        gravity and accelerations)    │\n│       │                                         │\n│    ┌──▼──┐         Reaction Force               │\n│    │Robot│◀───────── From Object (Newton III)   │\n│    │Arm  │                                     │\n│    └─────┘                                     │\n│                                                 │\n│ Gravity: m×g (↓) ─ Applied to object           │\n│ Required Torque: τ = I×α (rotational force)    │\n└─────────────────────────────────────────────────┘\n```",
    "chapter": 1,
    "section": "Forces in Robotic Manipulation",
    "page": 14,
    "token_count": 264
  },
  {
    "chunk_id": "3100b79c-e7d8-4fb3-a937-4fef162b7a18",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "### Kinematic Chain in Robotic Arm\n\n```\nBase Link ── Joint1 ── Link1 ── Joint2 ── Link2 ── End Effector\n   (fixed)    (θ1)     (L1)     (θ2)     (L2)    (tool center)\n\nForward Kinematics: θ1, θ2 → (x, y, z, orientation)\nInverse Kinematics: (x, y, z, orientation) → θ1, θ2\n```",
    "chapter": 1,
    "section": "Kinematic Chain in Robotic Arm",
    "page": 15,
    "token_count": 113
  },
  {
    "chunk_id": "5ff857bd-c632-4a48-b50f-03da5eeaed50",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Apply Newton's laws to understand robotic system behavior\n2. Explain the difference between kinematics and dynamics in robotics\n3. Identify forces acting on robotic systems during various operations\n4. Understand how physical laws affect robot control and safety\n5. Recognize the role of energy considerations in robotic system design",
    "chapter": 1,
    "section": "Learning Outcomes",
    "page": 16,
    "token_count": 78
  },
  {
    "chunk_id": "a3b4ce57-be43-4e38-b0ba-c0b0fac43d39",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## Further Reading\n\n- \"Robot Dynamics and Control\" by Spong, Hutchinson, and Vidyasagar (2004) [1]\n- \"Physics in Robotics: The Science Behind Intelligent Machines\" (SciTechnol, 2025) [2]\n- \"Servo Collision Detection Control System Based on Robot Dynamics\" (MDPI Sensors, 2025) [3]\n\n---",
    "chapter": 1,
    "section": "Further Reading",
    "page": 17,
    "token_count": 79
  },
  {
    "chunk_id": "d3ed4600-554f-4778-bf38-e3235629ae3e",
    "doc_id": "7f595a90-fda3-45ff-931e-074e6e61008c",
    "chunk_text": "## References\n\n[1] Spong, M.W., Hutchinson, S., & Vidyasagar, M. \"Robot Dynamics and Control.\" John Wiley & Sons, 2004. https://www.kramirez.net/Robotica/Tareas/Kinematics.pdf\n\n[2] SciTechnol Journal. \"Physics in Robotics: The Science Behind Intelligent Machines.\" 2025. https://www.scitechnol.com/peer-review/physics-in-robotics-the-science-behind-intelligent-machines-SDnA.php?article_id=27748\n\n[3] Xiang, Q. et al. \"Servo Collision Detection Control System Based on Robot Dynamics.\" MDPI Sensors, Vol. 25, No. 4. 2025. https://www.mdpi.com/1424-8220/25/4/1131",
    "chapter": 1,
    "section": "References",
    "page": 18,
    "token_count": 177
  },
  {
    "chunk_id": "210e0dea-fea2-42ac-99c6-9223152f8302",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "# 1.4 Sensor Systems for Physical AI",
    "chapter": 1,
    "section": "Introduction",
    "page": 1,
    "token_count": 10
  },
  {
    "chunk_id": "5cf1ad7a-fd3c-4bc2-9658-57504aa56400",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Learning Objectives\n\nBy the end of this section, you will be able to:\n\n- Identify and categorize different types of sensors used in physical AI systems\n- Understand the principles and trade-offs of vision, range, proprioceptive, and tactile sensors\n- Implement sensor data processing and fusion algorithms\n- Integrate sensors with ROS2 for robotic applications\n- Apply calibration and noise handling techniques to improve sensor reliability\n- Design sensor processing pipelines for real-world robotics applications",
    "chapter": 1,
    "section": "Learning Objectives",
    "page": 2,
    "token_count": 94
  },
  {
    "chunk_id": "57801afb-f5b0-43d1-b1cd-8a0b35aee3e2",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Introduction to Sensor Systems in Robotics\n\nSensor systems form the perception foundation of physical AI, bridging the gap between the physical world and computational intelligence. While digital AI systems process pre-digitized data, physical AI must first transduce physical phenomena—light, sound, force, motion—into electrical signals that can be interpreted by algorithms. The quality, diversity, and integration of these sensors fundamentally determine a robot's ability to perceive, understand, and interact with its environment.\n\nModern humanoid robots employ dozens of sensors across multiple modalities, creating a rich sensory tapestry analogous to human perception. These sensors operate at different frequencies, resolutions, and latencies, requiring sophisticated fusion algorithms to create coherent world models. Understanding sensor characteristics, limitations, and integration strategies is essential for building robust physical AI systems.",
    "chapter": 1,
    "section": "Introduction to Sensor Systems in Robotics",
    "page": 3,
    "token_count": 158
  },
  {
    "chunk_id": "8321af8f-bc85-4452-a7a6-ee19700bf27e",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Categories and Characteristics",
    "chapter": 1,
    "section": "Sensor Categories and Characteristics",
    "page": 4,
    "token_count": 5
  },
  {
    "chunk_id": "ccdfa964-8f6f-4d70-b9d1-1cfbf21bef5d",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "### Vision Sensors\n\nVision sensors provide dense, high-dimensional information about the environment, making them the primary perception modality for most modern robots.\n\n**RGB Cameras** capture color images at high resolution and frame rates. Modern cameras provide resolutions from VGA (640×480) to 4K (3840×2160) at frame rates ranging from 30 to 120 Hz. However, monocular cameras lack direct depth information and struggle in poor lighting conditions.\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass VisionProcessor(Node):\n    def __init__(self):\n        super().__init__('vision_processor')\n        self.bridge = CvBridge()\n\n# Subscribe to camera feed\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n# Publisher for processed images\n        self.publisher = self.create_publisher(\n            Image,\n            '/camera/image_processed',\n            10\n        )\n\ndef image_callback(self, msg):\n        # Convert ROS Image message to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')",
    "chapter": 1,
    "section": "Vision Sensors",
    "page": 5,
    "token_count": 259
  },
  {
    "chunk_id": "eab0b117-eb48-473a-9786-40c25ffcdfb5",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Vision Sensors\n\n# Perform image processing\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        edges = cv2.Canny(blurred, 50, 150)\n\n# Detect features for visual odometry\n        corners = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=100,\n            qualityLevel=0.01,\n            minDistance=10\n        )\n\n# Visualize detected features\n        if corners is not None:\n            for corner in corners:\n                x, y = corner.ravel()\n                cv2.circle(cv_image, (int(x), int(y)), 3, (0, 255, 0), -1)\n\n# Convert back to ROS message and publish\n        processed_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')\n        processed_msg.header = msg.header\n        self.publisher.publish(processed_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n**Depth Cameras** (RGB-D sensors) combine color and depth information, providing 3D perception crucial for navigation and manipulation. Technologies include:",
    "chapter": 1,
    "section": "Vision Sensors",
    "page": 6,
    "token_count": 284
  },
  {
    "chunk_id": "0a8b4288-0c26-4ff2-bcdb-f811b67cd2b7",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Vision Sensors\n\n- **Structured Light**: Projects infrared patterns and triangulates depth (Intel RealSense D400 series, range: 0.1-10m, accuracy: ±2%)\n- **Time-of-Flight (ToF)**: Measures light travel time (Microsoft Azure Kinect, range: 0.5-5m, accuracy: ±1%)\n- **Active Stereo**: Uses infrared projector with stereo cameras (Intel RealSense D435, combines both approaches)\n\n**Stereo Cameras** use binocular vision to compute depth through triangulation, mimicking human depth perception. They require calibration and perform poorly in textureless regions but work in any lighting condition where features are visible.",
    "chapter": 1,
    "section": "Vision Sensors",
    "page": 7,
    "token_count": 140
  },
  {
    "chunk_id": "3a0b24ed-7784-4038-aaf8-10864c24c3c1",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "### Range Sensors\n\nRange sensors measure distance to objects using various physical principles, providing complementary information to vision systems.\n\n**LIDAR (Light Detection and Ranging)** emits laser pulses and measures return time, creating precise 3D point clouds. Modern LIDARs offer:\n\n- **Scanning LIDAR**: Rotating lasers (Velodyne, Ouster) provide 360° coverage at ranges up to 200m with centimeter accuracy\n- **Solid-State LIDAR**: No moving parts (Luminar, Livox) with selective scanning and lower cost\n- **Flash LIDAR**: Illuminates entire scene simultaneously with faster update rates but shorter range\n\n**Ultrasonic Sensors** emit high-frequency sound waves (40 kHz typical) and measure echo return time. They excel at close-range obstacle detection (0.02-4m), work in any lighting, but suffer from specular reflection on smooth surfaces and have wide beam patterns that reduce angular resolution.\n\n**Radar Sensors** use radio waves for long-range detection (up to 300m) and velocity measurement via Doppler shift. Automotive radar (77 GHz) provides all-weather operation but lower resolution than LIDAR.",
    "chapter": 1,
    "section": "Range Sensors",
    "page": 8,
    "token_count": 245
  },
  {
    "chunk_id": "56ee2ec9-082b-41d4-a385-2a4635c33bba",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "### Proprioceptive Sensors\n\nProprioceptive sensors measure the robot's internal state, providing self-awareness analogous to human proprioception.\n\n**Encoders** measure joint angles and wheel rotations. Incremental encoders provide relative position through pulse counting, while absolute encoders give direct position readings without initialization. High-resolution encoders (4096 counts per revolution) enable precise position control.\n\n**Inertial Measurement Units (IMU)** combine accelerometers and gyroscopes (and often magnetometers) to measure acceleration, angular velocity, and orientation. MEMS IMUs in modern robots provide:\n\n- 3-axis accelerometer (measuring specific force up to ±16g)\n- 3-axis gyroscope (measuring angular velocity up to ±2000°/s)\n- Update rates: 100-1000 Hz\n- Drift: 0.1-10°/hour depending on grade\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Quaternion\nimport numpy as np\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n\nself.subscription = self.create_subscription(\n            Imu,\n            '/imu/data_raw',\n            self.imu_callback,\n            10\n        )",
    "chapter": 1,
    "section": "Proprioceptive Sensors",
    "page": 9,
    "token_count": 267
  },
  {
    "chunk_id": "96710960-90d9-4359-a2a5-57f5789309ca",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Proprioceptive Sensors\n\n# Complementary filter parameters\n        self.alpha = 0.98  # Trust gyro more for short term\n        self.dt = 0.01     # 100 Hz sensor rate\n\n# State estimation\n        self.roll = 0.0\n        self.pitch = 0.0\n        self.yaw = 0.0\n\ndef imu_callback(self, msg):\n        # Extract sensor data\n        ax = msg.linear_acceleration.x\n        ay = msg.linear_acceleration.y\n        az = msg.linear_acceleration.z\n\ngx = msg.angular_velocity.x\n        gy = msg.angular_velocity.y\n        gz = msg.angular_velocity.z\n\n# Compute accelerometer-based angles (noisy but drift-free)\n        accel_roll = np.arctan2(ay, az)\n        accel_pitch = np.arctan2(-ax, np.sqrt(ay**2 + az**2))\n\n# Integrate gyroscope (smooth but drifts)\n        gyro_roll = self.roll + gx * self.dt\n        gyro_pitch = self.pitch + gy * self.dt\n        gyro_yaw = self.yaw + gz * self.dt",
    "chapter": 1,
    "section": "Proprioceptive Sensors",
    "page": 10,
    "token_count": 236
  },
  {
    "chunk_id": "2159933a-2f45-4f6f-ab22-fc29c72d904f",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Proprioceptive Sensors\n\n# Complementary filter: blend short-term gyro with long-term accel\n        self.roll = self.alpha * gyro_roll + (1 - self.alpha) * accel_roll\n        self.pitch = self.alpha * gyro_pitch + (1 - self.alpha) * accel_pitch\n        self.yaw = gyro_yaw  # No accelerometer reference for yaw\n\nself.get_logger().info(\n            f'Orientation - Roll: {np.degrees(self.roll):.2f}°, '\n            f'Pitch: {np.degrees(self.pitch):.2f}°, '\n            f'Yaw: {np.degrees(self.yaw):.2f}°'\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IMUProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n**Force/Torque Sensors** measure interaction forces and torques, essential for manipulation and compliant control. Six-axis force/torque sensors at the wrist provide full wrench information (3 forces, 3 torques) with resolution down to 0.01N and 0.001Nm.",
    "chapter": 1,
    "section": "Proprioceptive Sensors",
    "page": 11,
    "token_count": 251
  },
  {
    "chunk_id": "6f0947f5-bdb2-4853-afbc-ffcccf303cb2",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "### Tactile Sensors\n\nTactile sensors provide contact information crucial for dexterous manipulation and safe human-robot interaction.\n\n**Force-Sensitive Resistors (FSR)** change resistance under applied force, providing simple binary or scalar contact detection at low cost but with limited spatial resolution and hysteresis.\n\n**Capacitive Sensors** detect contact through capacitance changes, offering better linearity and dynamic range than FSRs.\n\n**Optical Tactile Sensors** (e.g., GelSight, DIGIT) use cameras to observe deformation of a soft surface under contact, providing high-resolution tactile images that capture geometry, forces, and slip with unprecedented detail.",
    "chapter": 1,
    "section": "Tactile Sensors",
    "page": 12,
    "token_count": 136
  },
  {
    "chunk_id": "e4703ce9-ce7f-41df-a43e-6b04b7e3afcb",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Comparison and Selection\n\nThe following table summarizes key characteristics of major sensor types to guide system design:",
    "chapter": 1,
    "section": "Sensor Comparison and Selection",
    "page": 13,
    "token_count": 21
  },
  {
    "chunk_id": "6fe3cb0c-3e93-4f5d-b252-202ce5893989",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Comparison and Selection\n\n| Sensor Type | Range | Accuracy | Update Rate | Cost | Environment Sensitivity | Primary Use Cases |\n|-------------|-------|----------|-------------|------|------------------------|-------------------|\n| RGB Camera | Unlimited | N/A (2D) | 30-120 Hz | $ | Lighting, weather | Object recognition, visual servoing |\n| Depth Camera | 0.1-10m | 1-2% | 30-90 Hz | $$ | Lighting, surfaces | Navigation, manipulation, SLAM |\n| Stereo Camera | 0.5-20m | 2-5% | 30-60 Hz | $$ | Textures, lighting | Outdoor navigation, depth estimation |\n| 2D LIDAR | 0.1-30m | ±2cm | 10-40 Hz | $$ | Dust, rain | Indoor navigation, mapping |\n| 3D LIDAR | 0.5-200m | ±2cm | 10-20 Hz | $$$$ | Weather, reflectivity | Autonomous vehicles, outdoor mapping |\n| Ultrasonic | 0.02-4m | ±1cm | 50 Hz | $ | Surface angle, texture | Obstacle avoidance, parking assistance |\n| Radar | 1-300m | ±10cm | 10-20 Hz | $$$ | All-weather | Long-range detection, velocity measurement |\n| Encoder | N/A | 0.1-0.01° | 1000+ Hz | $ | None | Joint control, odometry |\n| IMU | N/A | 0.1-10°/hr drift | 100-1000 Hz | $-$$$ | Magnetic interference | Orientation, motion tracking |\n| F/T Sensor | N/A | 0.01-0.1N | 100-1000 Hz | $$$ | None | Manipulation, contact control |\n| Tactile | Contact only | 0.1-1N | 100-1000 Hz | $$-$$$$ | Surface properties | Grasping, texture sensing |",
    "chapter": 1,
    "section": "Sensor Comparison and Selection",
    "page": 14,
    "token_count": 434
  },
  {
    "chunk_id": "d06d45be-d4e2-4859-9610-88798cc99aa7",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Comparison and Selection\n\n**Cost scale**: `$ (<$100), $$ ($100–$1000), $$$ ($1000–$10,000), $$$$ (>$10,000)`",
    "chapter": 1,
    "section": "Sensor Comparison and Selection",
    "page": 15,
    "token_count": 41
  },
  {
    "chunk_id": "200df74b-e045-4560-8a2d-3b4f8ac0016b",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Fusion Principles\n\nNo single sensor provides complete, reliable perception. Sensor fusion combines multiple sensor modalities to create robust state estimates that exceed the capabilities of individual sensors. Effective fusion exploits complementary sensor characteristics:\n\n- **Complementary strengths**: Vision provides rich features; IMU provides high-rate motion data\n- **Redundancy**: Multiple sensors measure the same quantity with different error characteristics\n- **Cross-validation**: Inconsistent sensor readings reveal failures or outliers\n\n**Visual-Inertial Odometry (VIO)** exemplifies sensor fusion by combining camera and IMU data. Cameras provide drift-free but computationally expensive localization, while IMUs provide high-rate motion estimates that drift over time. Their fusion creates accurate, high-frequency pose estimates.\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom collections import deque\n\nclass VisualInertialOdometry(Node):\n    def __init__(self):\n        super().__init__('visual_inertial_odometry')\n        self.bridge = CvBridge()\n\n# Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )",
    "chapter": 1,
    "section": "Sensor Fusion Principles",
    "page": 16,
    "token_count": 295
  },
  {
    "chunk_id": "b2d32bea-015c-4ab8-860b-0328454bd64d",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Fusion Principles\n\n# Publisher for estimated pose\n        self.pose_pub = self.create_publisher(PoseStamped, '/vio/pose', 10)\n\n# State variables\n        self.position = np.zeros(3)\n        self.velocity = np.zeros(3)\n        self.orientation = np.eye(3)\n\n# Feature tracking\n        self.prev_gray = None\n        self.prev_points = None\n\n# IMU integration\n        self.imu_buffer = deque(maxlen=100)\n        self.last_imu_time = None\n\n# Camera calibration (example values - replace with actual calibration)\n        self.camera_matrix = np.array([\n            [500, 0, 320],\n            [0, 500, 240],\n            [0, 0, 1]\n        ])\n        self.focal_length = 500\n\ndef imu_callback(self, msg):\n        current_time = self.get_clock().now()\n\nif self.last_imu_time is not None:\n            dt = (current_time - self.last_imu_time).nanoseconds / 1e9\n\n# Extract angular velocity and linear acceleration\n            omega = np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ])\n\naccel = np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ])",
    "chapter": 1,
    "section": "Sensor Fusion Principles",
    "page": 17,
    "token_count": 277
  },
  {
    "chunk_id": "cbbf9b02-aed2-4d4b-a2dc-02f45eb5099d",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Fusion Principles\n\n# Remove gravity (assumes accelerometer measures specific force)\n            gravity = np.array([0, 0, -9.81])\n            accel_world = self.orientation @ accel + gravity\n\n# Integrate velocity and position (simple Euler integration)\n            self.velocity += accel_world * dt\n            self.position += self.velocity * dt\n\n# Update orientation (simplified - real systems use quaternions)\n            omega_skew = np.array([\n                [0, -omega[2], omega[1]],\n                [omega[2], 0, -omega[0]],\n                [-omega[1], omega[0], 0]\n            ])\n            self.orientation += self.orientation @ omega_skew * dt\n\n# Store in buffer for visual correction\n            self.imu_buffer.append({\n                'time': current_time,\n                'position': self.position.copy(),\n                'velocity': self.velocity.copy()\n            })\n\nself.last_imu_time = current_time\n\ndef image_callback(self, msg):\n        # Convert image\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\nif self.prev_gray is None:\n            # Initialize feature tracking\n            self.prev_points = cv2.goodFeaturesToTrack(\n                gray, maxCorners=100, qualityLevel=0.01, minDistance=10\n            )\n            self.prev_gray = gray\n            return",
    "chapter": 1,
    "section": "Sensor Fusion Principles",
    "page": 18,
    "token_count": 303
  },
  {
    "chunk_id": "e6825159-8944-41ea-b1cf-bf89802172cd",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Fusion Principles\n\nif self.prev_points is None or len(self.prev_points) == 0:\n            self.prev_points = cv2.goodFeaturesToTrack(\n                gray, maxCorners=100, qualityLevel=0.01, minDistance=10\n            )\n            self.prev_gray = gray\n            return\n\n# Track features using optical flow\n        curr_points, status, _ = cv2.calcOpticalFlowPyrLK(\n            self.prev_gray, gray, self.prev_points, None\n        )\n\n# Select good matches\n        good_prev = self.prev_points[status == 1]\n        good_curr = curr_points[status == 1]\n\nif len(good_prev) >= 5:\n            # Estimate essential matrix (camera motion)\n            E, mask = cv2.findEssentialMat(\n                good_curr, good_prev, self.camera_matrix,\n                method=cv2.RANSAC, prob=0.999, threshold=1.0\n            )\n\n# Recover rotation and translation\n            _, R, t, mask = cv2.recoverPose(\n                E, good_curr, good_prev, self.camera_matrix, mask=mask\n            )\n\n# Update position estimate using visual information\n            # Scale ambiguity resolved using IMU velocity\n            scale = np.linalg.norm(self.velocity) * 0.033  # Assume 30 Hz camera\n            visual_translation = (self.orientation @ t.flatten()) * scale",
    "chapter": 1,
    "section": "Sensor Fusion Principles",
    "page": 19,
    "token_count": 292
  },
  {
    "chunk_id": "f08f1074-7811-4330-b343-4fe8715619d0",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Fusion Principles\n\n# Fusion: blend IMU-predicted position with visual correction\n            fusion_weight = 0.7  # Trust visual more when available\n            self.position = (1 - fusion_weight) * self.position + \\\n                           fusion_weight * (self.position + visual_translation)\n\n# Update orientation\n            self.orientation = R @ self.orientation\n\n# Publish estimated pose\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = 'odom'\n\npose_msg.pose.position.x = float(self.position[0])\n        pose_msg.pose.position.y = float(self.position[1])\n        pose_msg.pose.position.z = float(self.position[2])\n\n# Convert rotation matrix to quaternion (simplified)\n        # Real implementation should use proper conversion\n        pose_msg.pose.orientation.w = 1.0\n        pose_msg.pose.orientation.x = 0.0\n        pose_msg.pose.orientation.y = 0.0\n        pose_msg.pose.orientation.z = 0.0\n\nself.pose_pub.publish(pose_msg)\n\n# Update for next iteration\n        self.prev_gray = gray\n        self.prev_points = good_curr.reshape(-1, 1, 2)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisualInertialOdometry()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()",
    "chapter": 1,
    "section": "Sensor Fusion Principles",
    "page": 20,
    "token_count": 295
  },
  {
    "chunk_id": "fbf1d642-9fc0-4edf-98af-c3774499260d",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Sensor Fusion Principles\n\nif __name__ == '__main__':\n    main()\n```\n\nThis VIO implementation demonstrates key fusion concepts:\n\n1. **IMU Integration**: High-rate (100+ Hz) propagation of position and orientation using inertial measurements\n2. **Visual Correction**: Lower-rate (30 Hz) camera-based updates that correct accumulated drift\n3. **Scale Resolution**: IMU velocity resolves the scale ambiguity inherent in monocular visual odometry\n4. **Complementary Filtering**: Weighted combination of predictions and measurements based on confidence\n\nAdvanced fusion techniques include Extended Kalman Filters (EKF), Unscented Kalman Filters (UKF), and graph-based optimization (factor graphs) which model sensor uncertainties and optimize historical state estimates.",
    "chapter": 1,
    "section": "Sensor Fusion Principles",
    "page": 21,
    "token_count": 150
  },
  {
    "chunk_id": "bd1b59d8-f787-4134-b674-dd0cbe41288c",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## ROS2 Sensor Integration\n\nROS2 (Robot Operating System 2) provides standardized interfaces for sensor integration through predefined message types and drivers:\n\n**Key ROS2 Sensor Message Types:**\n- `sensor_msgs/Image`: Camera images with encoding and metadata\n- `sensor_msgs/PointCloud2`: 3D point clouds from LIDAR and depth cameras\n- `sensor_msgs/Imu`: IMU data with orientation, angular velocity, and linear acceleration\n- `sensor_msgs/LaserScan`: 2D LIDAR scans\n- `sensor_msgs/JointState`: Joint encoder positions and velocities\n- `geometry_msgs/WrenchStamped`: Force/torque sensor data\n\n**ROS2 Sensor Integration Pipeline:**\n\n1. **Driver Layer**: Hardware-specific drivers publish raw sensor data (e.g., `realsense2_camera`, `velodyne_driver`)\n2. **Processing Layer**: Nodes process and filter sensor data (e.g., `image_proc` for rectification, `pointcloud_to_laserscan` for dimensionality reduction)\n3. **Fusion Layer**: Algorithms combine multiple sensors (e.g., `robot_localization` for EKF-based sensor fusion)\n4. **Application Layer**: High-level nodes consume processed sensor data for planning and control\n\n**Example: Multi-Sensor ROS2 Launch File**\n\n```python\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration",
    "chapter": 1,
    "section": "ROS2 Sensor Integration",
    "page": 22,
    "token_count": 298
  },
  {
    "chunk_id": "8f03ed60-e484-4f9a-8bdf-0344898c048b",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## ROS2 Sensor Integration\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Camera driver\n        Node(\n            package='realsense2_camera',\n            executable='realsense2_camera_node',\n            name='camera',\n            parameters=[{\n                'enable_depth': True,\n                'enable_color': True,\n                'depth_module.profile': '640x480x30',\n                'rgb_camera.profile': '640x480x30',\n            }],\n            remappings=[\n                ('/camera/color/image_raw', '/camera/image_raw'),\n                ('/camera/depth/image_rect_raw', '/camera/depth/image')\n            ]\n        ),\n\n# IMU driver\n        Node(\n            package='bno055_driver',\n            executable='bno055_driver_node',\n            name='imu',\n            parameters=[{\n                'port': '/dev/ttyUSB0',\n                'frame_id': 'imu_link'\n            }]\n        ),\n\n# LIDAR driver\n        Node(\n            package='rplidar_ros',\n            executable='rplidar_composition',\n            name='lidar',\n            parameters=[{\n                'serial_port': '/dev/ttyUSB1',\n                'frame_id': 'laser_frame',\n                'angle_compensate': True\n            }]\n        ),",
    "chapter": 1,
    "section": "ROS2 Sensor Integration",
    "page": 23,
    "token_count": 258
  },
  {
    "chunk_id": "3ca4830f-d421-40da-a1eb-980531f747cb",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## ROS2 Sensor Integration\n\n# Depth image to point cloud converter\n        Node(\n            package='depth_image_proc',\n            executable='point_cloud_xyz_node',\n            name='depth_to_pointcloud',\n            remappings=[\n                ('image_rect', '/camera/depth/image'),\n                ('camera_info', '/camera/depth/camera_info'),\n                ('points', '/camera/depth/points')\n            ]\n        ),",
    "chapter": 1,
    "section": "ROS2 Sensor Integration",
    "page": 24,
    "token_count": 83
  },
  {
    "chunk_id": "42d0181e-3e4e-4555-b108-ff6211e916b7",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## ROS2 Sensor Integration\n\n# Robot localization - EKF fusion\n        Node(\n            package='robot_localization',\n            executable='ekf_node',\n            name='ekf_localization',\n            parameters=[{\n                'frequency': 30.0,\n                'sensor_timeout': 0.1,\n                'two_d_mode': False,\n                'odom0': '/wheel/odometry',\n                'odom0_config': [True, True, True,   # x, y, z\n                                False, False, False,  # roll, pitch, yaw\n                                True, True, False,    # vx, vy, vz\n                                False, False, True,   # vroll, vpitch, vyaw\n                                False, False, False], # ax, ay, az\n                'imu0': '/imu/data',\n                'imu0_config': [False, False, False,\n                               True, True, True,     # orientation\n                               False, False, False,\n                               True, True, True,     # angular velocities\n                               True, True, True],    # linear accelerations\n            }]\n        )\n    ])\n```",
    "chapter": 1,
    "section": "ROS2 Sensor Integration",
    "page": 25,
    "token_count": 236
  },
  {
    "chunk_id": "722a2a3c-1955-4252-a724-fc9ad06c6b2f",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Calibration and Noise Handling\n\nAccurate sensor data requires calibration to correct systematic errors and noise handling to manage random variations.",
    "chapter": 1,
    "section": "Calibration and Noise Handling",
    "page": 26,
    "token_count": 24
  },
  {
    "chunk_id": "0cd95005-8f0e-461d-855a-e68e50576175",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "### Camera Calibration\n\nCamera calibration estimates intrinsic parameters (focal length, principal point, distortion coefficients) and extrinsic parameters (position and orientation relative to robot base).\n\n**Intrinsic Calibration** using checkerboard patterns:\n\n```python\nimport cv2\nimport numpy as np\nimport glob\n\ndef calibrate_camera(image_path_pattern, checkerboard_size=(9, 6)):\n    \"\"\"\n    Calibrate camera using checkerboard images.\n\nArgs:\n        image_path_pattern: Glob pattern for calibration images\n        checkerboard_size: (width, height) in internal corners\n\nReturns:\n        camera_matrix: 3x3 intrinsic matrix\n        dist_coeffs: Distortion coefficients\n        rvecs, tvecs: Rotation and translation vectors for each image\n    \"\"\"\n    # Prepare object points (0,0,0), (1,0,0), (2,0,0), ...\n    objp = np.zeros((checkerboard_size[0] * checkerboard_size[1], 3), np.float32)\n    objp[:, :2] = np.mgrid[0:checkerboard_size[0],\n                            0:checkerboard_size[1]].T.reshape(-1, 2)\n\n# Arrays to store object points and image points\n    objpoints = []  # 3D points in real world space\n    imgpoints = []  # 2D points in image plane\n\nimages = glob.glob(image_path_pattern)\n    image_size = None",
    "chapter": 1,
    "section": "Camera Calibration",
    "page": 27,
    "token_count": 297
  },
  {
    "chunk_id": "bc47dc22-60cd-4478-a598-1165e885c8ba",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Camera Calibration\n\nfor fname in images:\n        img = cv2.imread(fname)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nif image_size is None:\n            image_size = gray.shape[::-1]\n\n# Find checkerboard corners\n        ret, corners = cv2.findChessboardCorners(gray, checkerboard_size, None)\n\nif ret:\n            objpoints.append(objp)\n\n# Refine corner locations\n            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n            corners_refined = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n            imgpoints.append(corners_refined.reshape(-1, 2))\n\n# Perform calibration\n    ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n        objpoints, imgpoints, image_size, None, None\n    )\n\n# Calculate reprojection error\n    total_error = 0\n    for i in range(len(objpoints)):\n        imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i],\n                                          camera_matrix, dist_coeffs)\n        error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2) / len(imgpoints2)\n        total_error += error",
    "chapter": 1,
    "section": "Camera Calibration",
    "page": 28,
    "token_count": 289
  },
  {
    "chunk_id": "377f418d-fd5a-4f1a-9aec-22402d2238a5",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Camera Calibration\n\nmean_error = total_error / len(objpoints)\n    print(f\"Camera calibration complete. Mean reprojection error: {mean_error:.3f} pixels\")\n\nreturn camera_matrix, dist_coeffs, rvecs, tvecs\n\n# Example usage\nif __name__ == '__main__':\n    camera_matrix, dist_coeffs, _, _ = calibrate_camera('calibration_images/*.jpg')\n\nprint(\"\\nCamera Matrix:\")\n    print(camera_matrix)\n    print(\"\\nDistortion Coefficients:\")\n    print(dist_coeffs)\n\n# Save calibration\n    np.savez('camera_calibration.npz',\n             camera_matrix=camera_matrix,\n             dist_coeffs=dist_coeffs)\n```",
    "chapter": 1,
    "section": "Camera Calibration",
    "page": 29,
    "token_count": 136
  },
  {
    "chunk_id": "5c6e5d0f-f865-4033-bf20-a1459847b184",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "### Sensor Noise Filtering\n\nSensor noise can be managed through filtering techniques:\n\n1. **Low-pass filters**: Remove high-frequency noise (e.g., moving average, Butterworth filter)\n2. **Median filters**: Robust to outliers and impulse noise\n3. **Kalman filters**: Optimal linear filters for systems with Gaussian noise\n4. **Outlier rejection**: Remove measurements that deviate significantly from predictions\n\nMost sensor fusion algorithms incorporate noise models explicitly, weighting measurements by their inverse covariance to emphasize reliable sensors.",
    "chapter": 1,
    "section": "Sensor Noise Filtering",
    "page": 30,
    "token_count": 103
  },
  {
    "chunk_id": "3668f3c4-3825-4552-8f01-212c44aeeddf",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Example Sensor Processing Pipeline\n\nA complete sensor processing pipeline for a humanoid robot typically follows this architecture:\n\n**Hardware Layer** → **Driver Layer** → **Preprocessing** → **Fusion** → **World Model** → **Decision Making**\n\nFor a humanoid performing navigation and manipulation:\n\n1. **RGB-D cameras** (head-mounted) provide visual features and depth at 30 Hz\n2. **IMU** (torso-mounted) provides orientation and acceleration at 200 Hz\n3. **Joint encoders** report all joint angles at 1000 Hz\n4. **F/T sensors** (wrists and ankles) measure contact forces at 1000 Hz\n5. **2D LIDAR** (chest-mounted) provides planar obstacle detection at 10 Hz\n\n**Processing Pipeline:**\n- Visual odometry runs at 30 Hz using RGB-D data\n- IMU integration runs at 200 Hz for high-rate pose prediction\n- EKF fusion combines visual odometry, IMU, and encoder data to estimate full body state at 100 Hz\n- Contact forces are low-pass filtered and used for balance control at 1000 Hz\n- LIDAR scans are converted to 2D occupancy grid for collision checking\n\nThis multi-rate, multi-sensor pipeline enables robust perception for dynamic locomotion and manipulation tasks.",
    "chapter": 1,
    "section": "Example Sensor Processing Pipeline",
    "page": 31,
    "token_count": 273
  },
  {
    "chunk_id": "a433b86a-184e-407b-bf9b-0c8fc33af7d9",
    "doc_id": "17100bdf-818f-4135-aa2b-54e1cb78b145",
    "chunk_text": "## Key Points Summary\n\n- **Sensor diversity is essential**: Different sensor modalities provide complementary information (vision for features, IMU for dynamics, proprioception for self-state)\n\n- **Sensor fusion overcomes individual limitations**: Combining sensors with different characteristics (camera drift-free but low-rate; IMU high-rate but drifts) produces superior estimates\n\n- **ROS2 provides standardized interfaces**: Predefined message types and driver architectures simplify sensor integration and enable algorithm reuse\n\n- **Calibration is critical**: Systematic sensor errors must be corrected through calibration to achieve accurate measurements\n\n- **Noise handling requires principled approaches**: Filtering, outlier rejection, and probabilistic fusion manage random sensor variations\n\n- **Trade-offs guide sensor selection**: Range, accuracy, update rate, cost, and environmental robustness determine appropriate sensors for each application\n\n- **Multi-rate processing is necessary**: Different sensors operate at different frequencies, requiring hierarchical processing pipelines\n\n- **Sensor placement affects performance**: Strategic positioning on the robot body (cameras in head, IMU in torso, contact sensors in extremities) optimizes perception quality\n\nUnderstanding these principles and implementing robust sensor processing pipelines forms the foundation for reliable physical AI systems that can perceive and interact with complex, dynamic environments.\n\n---\n\n**Next Steps**: In Chapter 2, we will explore how sensor data feeds into world modeling and state estimation algorithms that enable robots to build coherent representations of their environment for planning and control.",
    "chapter": 1,
    "section": "Key Points Summary",
    "page": 32,
    "token_count": 287
  },
  {
    "chunk_id": "8d4391ed-fba9-490e-802e-0be17db78c78",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "# Chapter 2: The Robotic Nervous System (ROS 2)",
    "chapter": 2,
    "section": "Introduction",
    "page": 1,
    "token_count": 17
  },
  {
    "chunk_id": "d276c3ed-3f6d-418c-9248-ceb17a6f1cb4",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "## Chapter Overview\nThis chapter introduces the Robot Operating System 2 (ROS 2), the middleware that connects AI systems to robotic hardware. Students will learn the fundamental concepts of ROS 2 architecture including nodes, topics, services, and actions, with practical examples using Python and rclpy.",
    "chapter": 2,
    "section": "Chapter Overview",
    "page": 2,
    "token_count": 59
  },
  {
    "chunk_id": "d01c9e27-304a-4d91-90d8-ee3f0d56badf",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "## Learning Objectives\nAfter completing this chapter, students will be able to:\n1. Explain the architecture of ROS 2 and its core components\n2. Create and run basic ROS 2 nodes using Python\n3. Implement the publish/subscribe pattern for data transmission\n4. Use services for request/response communication\n5. Implement actions for long-running tasks with feedback\n6. Bridge Python agents to ROS 2 controllers for robot control\n7. Understand and modify URDF robot descriptions",
    "chapter": 2,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 98
  },
  {
    "chunk_id": "8ba9c13d-2abc-4e90-9a25-925341e495cd",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "## Section Breakdown",
    "chapter": 2,
    "section": "Section Breakdown",
    "page": 4,
    "token_count": 4
  },
  {
    "chunk_id": "61f59a87-e69b-482b-a06b-f5c42d54570a",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "### Section 1: ROS 2 Architecture Overview\n- Introduction to middleware in robotics\n- The ROS 2 computation graph\n- Nodes, topics, services, and actions\n- DDS (Data Distribution Service) overview\n- Communication patterns comparison",
    "chapter": 2,
    "section": "Section 1: ROS 2 Architecture Overview",
    "page": 5,
    "token_count": 49
  },
  {
    "chunk_id": "75138674-77ce-455f-b604-7c68131becae",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "### Section 2: Creating and Running ROS 2 Nodes\n- Installing and setting up ROS 2\n- Basic node structure in Python\n- Publisher nodes for data transmission\n- Subscriber nodes for data reception\n- Using ros2 CLI tools for debugging",
    "chapter": 2,
    "section": "Section 2: Creating and Running ROS 2 Nodes",
    "page": 6,
    "token_count": 51
  },
  {
    "chunk_id": "551e2668-00f2-4302-ab58-66664b1b4da7",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "### Section 3: Topics, Services, and Actions\n- Publish/subscribe pattern in detail\n- Service-based request/response communication\n- Action-based goal execution with feedback\n- When to use each communication pattern\n- Practical examples and use cases",
    "chapter": 2,
    "section": "Section 3: Topics, Services, and Actions",
    "page": 7,
    "token_count": 49
  },
  {
    "chunk_id": "75b9dd91-39ba-4c91-baa5-acf22bbdd318",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "### Section 4: Bridging Python Agents to ROS Controllers\n- Connecting AI agents to robot systems\n- Publishing commands to robot actuators\n- Subscribing to sensor feedback\n- Implementing closed-loop control systems\n- Safety patterns in simulation",
    "chapter": 2,
    "section": "Section 4: Bridging Python Agents to ROS Controllers",
    "page": 8,
    "token_count": 50
  },
  {
    "chunk_id": "45fda0cf-c91e-4396-bcbf-060a53082ce0",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "### Section 5: Understanding URDF for Humanoid Robots\n- URDF structure and components\n- Links, joints, and sensors in robot descriptions\n- Creating a simple humanoid model\n- Visualizing robots in RViz and Gazebo\n- Modifying existing robot models",
    "chapter": 2,
    "section": "Section 5: Understanding URDF for Humanoid Robots",
    "page": 9,
    "token_count": 56
  },
  {
    "chunk_id": "c9a5a4e3-a00a-4539-977c-1b267684b323",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "## Estimated Length\n- Total word count: ~8,000-10,000 words\n- Reading time: 2-3 hours\n- Hands-on practice time: 2-4 hours",
    "chapter": 2,
    "section": "Estimated Length",
    "page": 10,
    "token_count": 40
  },
  {
    "chunk_id": "b77a6da6-5f83-4161-b2be-c35d52ad8bc1",
    "doc_id": "ae7816fc-b14b-48f5-ab7b-494dd91ff1e3",
    "chunk_text": "## Prerequisites\n- Basic Python programming knowledge\n- Understanding of Chapter 1 concepts\n- Basic Linux command line familiarity",
    "chapter": 2,
    "section": "Prerequisites",
    "page": 11,
    "token_count": 24
  },
  {
    "chunk_id": "1b0ac3ea-f9ab-4fbf-ab32-c79ea56e3288",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "# Creating and Running ROS 2 Nodes",
    "chapter": 2,
    "section": "Introduction",
    "page": 1,
    "token_count": 8
  },
  {
    "chunk_id": "c85eaad5-51e5-418c-9fda-9f960dd04502",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Overview\n\nA ROS 2 node is an executable process that works as part of the ROS 2 computation graph. Nodes are the fundamental building blocks of any ROS 2 system, performing specific functions such as sensor data processing, control algorithm execution, or hardware interface management. Understanding how to create, structure, and run ROS 2 nodes is essential for building robotic systems that can communicate effectively within the ROS 2 middleware framework.\n\nIn this section, we'll explore the fundamental concepts of ROS 2 nodes, including their structure in Python, how they communicate through topics using the publish/subscribe pattern, and how to use ROS 2 command-line tools for debugging and monitoring. We'll also cover best practices for node design that ensure robust and maintainable robotic systems.",
    "chapter": 2,
    "section": "Overview",
    "page": 2,
    "token_count": 153
  },
  {
    "chunk_id": "577a9d29-e631-483d-a313-9c8c0b0ef786",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Key Concepts",
    "chapter": 2,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "241fef22-30ea-4e85-bddd-663dbf75fdd3",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Node Structure and Lifecycle\n\nA ROS 2 node follows a standard structure with distinct lifecycle phases that allow for controlled initialization, operation, and cleanup. Understanding this structure is crucial for creating nodes that integrate well into larger robotic systems.\n\n**Basic Node Components**:\n- **Initialization**: Setting up the node name and establishing communication interfaces\n- **Execution Loop**: Processing callbacks and performing the node's primary function\n- **Cleanup**: Properly releasing resources when the node shuts down\n\nThe lifecycle states of a ROS 2 node include:\n- **Unconfigured**: The node exists but is not ready for communication\n- **Inactive**: Configured but not actively processing data\n- **Active**: Fully operational and participating in the system\n- **Finalized**: Being shut down",
    "chapter": 2,
    "section": "Node Structure and Lifecycle",
    "page": 4,
    "token_count": 153
  },
  {
    "chunk_id": "579e5e2f-e403-41b4-a2c1-33158217b9b3",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Publisher-Subscriber Pattern\n\nThe publish-subscribe (pub/sub) pattern is the most common communication mechanism in ROS 2. Publishers send data to named topics, while subscribers receive data from those topics. This decoupled approach allows for flexible system design where nodes can be added or removed without affecting others.\n\n**Publisher Responsibilities**:\n- Creating and maintaining the topic connection\n- Formatting data into ROS 2 messages\n- Determining the frequency of data publication\n- Managing Quality of Service (QoS) policies\n\n**Subscriber Responsibilities**:\n- Establishing topic subscriptions\n- Processing incoming message callbacks\n- Handling message data appropriately\n- Managing message queue behavior",
    "chapter": 2,
    "section": "Publisher-Subscriber Pattern",
    "page": 5,
    "token_count": 132
  },
  {
    "chunk_id": "edbd4f38-2163-4df7-b5e8-5cb69fb8ef43",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Message Types and Communication\n\nROS 2 defines standard message types that ensure compatibility between different nodes. These messages are serialized for transmission across the network and must follow specific structures defined by ROS interfaces (`.msg`, `.srv`, `.action` files).\n\nCommon message packages include:\n- `std_msgs`: Basic data types like Int32, Float64, String\n- `sensor_msgs`: Sensor-specific data like LaserScan, Image, Imu\n- `geometry_msgs`: Spatial relationships like Point, Pose, Twist\n- `nav_msgs`: Navigation-related messages like Path, OccupancyGrid",
    "chapter": 2,
    "section": "Message Types and Communication",
    "page": 6,
    "token_count": 117
  },
  {
    "chunk_id": "b4f21eba-ea54-43ce-93bd-8d683ee76774",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Creating Your First ROS 2 Node",
    "chapter": 2,
    "section": "Creating Your First ROS 2 Node",
    "page": 7,
    "token_count": 8
  },
  {
    "chunk_id": "0f5be714-a061-47e2-8853-8d6838fe4cec",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Prerequisites\n\nBefore creating ROS 2 nodes, ensure you have:\n\n1. A complete ROS 2 installation (Humble Hawksbill or Iron Irwini recommended)\n2. Your ROS 2 environment sourced in your terminal\n3. A workspace set up for your custom nodes\n4. Basic Python knowledge with understanding of object-oriented programming",
    "chapter": 2,
    "section": "Prerequisites",
    "page": 8,
    "token_count": 69
  },
  {
    "chunk_id": "bfc5070f-63d5-4273-bd08-03c5d995d622",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Basic Publisher Node\n\nLet's create a simple publisher node that publishes a counter value:\n\n```python\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass SimplePublisherNode(Node):\n    \"\"\"\n    A simple ROS 2 publisher node that demonstrates basic node structure\n    \"\"\"\n    def __init__(self):\n        # Initialize the node with a unique name\n        super().__init__('simple_publisher')\n        \n        # Create a publisher for String messages on the 'chatter' topic\n        # The second parameter (10) specifies the message queue size\n        self.publisher = self.create_publisher(String, 'chatter', 10)\n        \n        # Set up a timer to call the callback function every second\n        timer_period = 1.0  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        \n        # Counter to track message number\n        self.i = 0\n        \n        # Log that the node has started\n        self.get_logger().info('Simple publisher node initialized')",
    "chapter": 2,
    "section": "Basic Publisher Node",
    "page": 9,
    "token_count": 222
  },
  {
    "chunk_id": "d9897b0c-7465-4af4-a432-4d64490bf38b",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Basic Publisher Node\n\ndef timer_callback(self):\n        \"\"\"\n        Callback function called by the timer at regular intervals\n        \"\"\"\n        msg = String()\n        msg.data = f'Hello ROS 2! Message #{self.i}'\n        \n        # Publish the message\n        self.publisher.publish(msg)\n        \n        # Log the message for debugging\n        self.get_logger().info(f'Publishing: \"{msg.data}\"')\n        \n        # Increment the counter\n        self.i += 1\n\ndef main(args=None):\n    \"\"\"\n    Main function that initializes ROS 2, creates the node, and starts spinning\n    \"\"\"\n    # Initialize the ROS 2 client library\n    rclpy.init(args=args)\n    \n    # Create an instance of our node class\n    simple_publisher = SimplePublisherNode()\n    \n    try:\n        # Spin the node to process callbacks\n        rclpy.spin(simple_publisher)\n    except KeyboardInterrupt:\n        # Handle graceful shutdown when Ctrl+C is pressed\n        simple_publisher.get_logger().info('Interrupted by user')\n    finally:\n        # Clean up resources\n        simple_publisher.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```",
    "chapter": 2,
    "section": "Basic Publisher Node",
    "page": 10,
    "token_count": 244
  },
  {
    "chunk_id": "9db03278-4091-41b4-8a0d-68e1e9e68491",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Basic Subscriber Node\n\nNow let's create a subscriber node that receives messages from the publisher:\n\n```python\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass SimpleSubscriberNode(Node):\n    \"\"\"\n    A simple ROS 2 subscriber node that demonstrates message reception\n    \"\"\"\n    def __init__(self):\n        # Initialize the node with a unique name\n        super().__init__('simple_subscriber')\n        \n        # Create a subscription to the 'chatter' topic\n        # The callback function is called whenever a message arrives\n        self.subscription = self.create_subscription(\n            String,           # Message type\n            'chatter',        # Topic name\n            self.listener_callback,  # Callback function\n            10               # Queue size\n        )\n        \n        # Make sure the subscription is active\n        self.subscription  # Prevent unused variable warning\n        \n        # Log that the subscriber is ready\n        self.get_logger().info('Simple subscriber node initialized')\n\ndef listener_callback(self, msg):\n        \"\"\"\n        Callback function called when a message arrives on the topic\n        \"\"\"\n        # Log the received message\n        self.get_logger().info(f'I heard: \"{msg.data}\"')",
    "chapter": 2,
    "section": "Basic Subscriber Node",
    "page": 11,
    "token_count": 256
  },
  {
    "chunk_id": "44e77326-4b3b-42b6-bca1-69abc83968f9",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Basic Subscriber Node\n\ndef main(args=None):\n    \"\"\"\n    Main function that initializes ROS 2, creates the node, and starts spinning\n    \"\"\"\n    # Initialize the ROS 2 client library\n    rclpy.init(args=args)\n    \n    # Create an instance of our subscriber node\n    simple_subscriber = SimpleSubscriberNode()\n    \n    try:\n        # Spin the node to process incoming messages\n        rclpy.spin(simple_subscriber)\n    except KeyboardInterrupt:\n        # Handle graceful shutdown when Ctrl+C is pressed\n        simple_subscriber.get_logger().info('Interrupted by user')\n    finally:\n        # Clean up resources\n        simple_subscriber.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```",
    "chapter": 2,
    "section": "Basic Subscriber Node",
    "page": 12,
    "token_count": 156
  },
  {
    "chunk_id": "74d2bcc0-7d54-4f34-a737-30c209a0c3a2",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Running ROS 2 Nodes",
    "chapter": 2,
    "section": "Running ROS 2 Nodes",
    "page": 13,
    "token_count": 6
  },
  {
    "chunk_id": "8daaabe0-c1d4-4f9b-89f4-1bf22400f8b3",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Setting up a ROS 2 Package\n\nTo run these nodes, you need to place them in a ROS 2 package. Create the package structure:\n\n```bash\n# Create a new workspace\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws/src\n\n# Create a new package\nros2 pkg create --build-type ament_python my_robot_nodes\ncd my_robot_nodes\n```\n\nPlace the publisher code in `my_robot_nodes/my_robot_nodes/simple_publisher.py` and the subscriber code in `my_robot_nodes/my_robot_nodes/simple_subscriber.py`.",
    "chapter": 2,
    "section": "Setting up a ROS 2 Package",
    "page": 14,
    "token_count": 116
  },
  {
    "chunk_id": "d4e2c7b1-7b21-44d0-b0bc-aacb8f64ccfb",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Package Configuration\n\nUpdate your `setup.py` file to make the nodes executable:\n\n```python\nfrom setuptools import setup\nfrom glob import glob\nimport os\n\npackage_name = 'my_robot_nodes'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=[package_name],\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Your Name',\n    maintainer_email='your.email@example.com',\n    description='Simple ROS 2 publisher and subscriber nodes',\n    license='Apache License 2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'simple_publisher = my_robot_nodes.simple_publisher:main',\n            'simple_subscriber = my_robot_nodes.simple_subscriber:main',\n        ],\n    },\n)\n```",
    "chapter": 2,
    "section": "Package Configuration",
    "page": 15,
    "token_count": 201
  },
  {
    "chunk_id": "51373040-18c5-4be3-a94a-4d2357464022",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Building and Running\n\nBuild your package:\n\n```bash\ncd ~/ros2_ws\ncolcon build --packages-select my_robot_nodes\n\n# Source the workspace\nsource install/setup.bash\n```\n\nNow you can run the nodes:\n\n```bash\n# In one terminal, run the publisher\nros2 run my_robot_nodes simple_publisher\n\n# In another terminal, run the subscriber\nros2 run my_robot_nodes simple_subscriber\n```",
    "chapter": 2,
    "section": "Building and Running",
    "page": 16,
    "token_count": 89
  },
  {
    "chunk_id": "ff5c51ee-07f9-4c47-83b6-88d66aadcf5c",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Using ROS 2 Command-Line Tools\n\nROS 2 provides powerful command-line tools for debugging and monitoring your nodes:",
    "chapter": 2,
    "section": "Using ROS 2 Command-Line Tools",
    "page": 17,
    "token_count": 24
  },
  {
    "chunk_id": "24286761-f086-47a2-8551-00f9f83d0ca7",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Node Management Tools\n\n```bash\n# List all running nodes\nros2 node list\n\n# Get information about a specific node\nros2 node info /simple_publisher\n\n# Find nodes that match a pattern\nros2 node list --include-hidden-nodes\n```",
    "chapter": 2,
    "section": "Node Management Tools",
    "page": 18,
    "token_count": 54
  },
  {
    "chunk_id": "62fe609b-cf76-43aa-ae3a-22a11c65a65a",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Topic Management Tools\n\n```bash\n# List all topics\nros2 topic list\n\n# Show information about a specific topic\nros2 topic info /chatter\n\n# Echo messages on a topic (like our subscriber but from command line)\nros2 topic echo /chatter\n\n# Publish a message to a topic (like our publisher)\nros2 topic pub /chatter std_msgs/msg/String \"data: 'Hello from command line'\"\n\n# Monitor the rate of messages on a topic\nros2 topic hz /chatter\n```",
    "chapter": 2,
    "section": "Topic Management Tools",
    "page": 19,
    "token_count": 110
  },
  {
    "chunk_id": "96b16cbc-12c3-4c11-b499-cfcba2c5f8d9",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Service Management Tools\n\n```bash\n# List all services\nros2 service list\n\n# Get information about a specific service\nros2 service info /my_service\n\n# Call a service from command line\nros2 service call /my_service example_interfaces/srv/AddTwoInts \"{a: 1, b: 2}\"\n```",
    "chapter": 2,
    "section": "Service Management Tools",
    "page": 20,
    "token_count": 69
  },
  {
    "chunk_id": "cebad41f-d7ab-4cb1-b11b-d68787d43188",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Best Practices",
    "chapter": 2,
    "section": "Best Practices",
    "page": 21,
    "token_count": 3
  },
  {
    "chunk_id": "23d9230a-7344-467f-afd0-78034a3acb65",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Node Design Principles\n\n1. **Single Responsibility**: Each node should perform one primary function\n2. **Parameter Configuration**: Use ROS 2 parameters for runtime configuration\n3. **Error Handling**: Implement proper exception handling and logging\n4. **Resource Management**: Clean up resources when the node shuts down\n5. **Modularity**: Design nodes to be reusable in different contexts",
    "chapter": 2,
    "section": "Node Design Principles",
    "page": 22,
    "token_count": 76
  },
  {
    "chunk_id": "17f459d0-ff84-4368-b82a-63861160cd3e",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Code Organization\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nSimple publisher node with best practices implemented\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy\nfrom std_msgs.msg import String",
    "chapter": 2,
    "section": "Code Organization",
    "page": 23,
    "token_count": 62
  },
  {
    "chunk_id": "faa8589e-fe3b-4bfa-8d2d-1acdff5cf7d5",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Code Organization\n\nclass BestPracticePublisherNode(Node):\n    \"\"\"\n    A publisher node implementing best practices\n    \"\"\"\n    def __init__(self):\n        # Initialize with node name\n        super().__init__('best_practice_publisher')\n        \n        # Declare parameters with default values\n        self.declare_parameter('publish_rate', 1.0)  # Hz\n        self.declare_parameter('topic_name', 'chatter')\n        self.declare_parameter('message_prefix', 'Hello')\n        \n        # Get parameter values\n        publish_rate = self.get_parameter('publish_rate').value\n        topic_name = self.get_parameter('topic_name').value\n        message_prefix = self.get_parameter('message_prefix').value\n        \n        # Configure QoS profile\n        qos_profile = QoSProfile(\n            depth=10,\n            reliability=ReliabilityPolicy.RELIABLE,\n            durability=DurabilityPolicy.VOLATILE\n        )\n        \n        # Create publisher with the configured topic name\n        self.publisher = self.create_publisher(String, topic_name, qos_profile)\n        \n        # Create timer with the configured rate\n        timer_period = 1.0 / publish_rate\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        \n        # Initialize counter and prefix\n        self.i = 0\n        self.prefix = message_prefix\n        \n        # Log initialization\n        self.get_logger().info(\n            f'Best practice publisher initialized: topic={topic_name}, '\n            f'rate={publish_rate}Hz'\n        )",
    "chapter": 2,
    "section": "Code Organization",
    "page": 24,
    "token_count": 310
  },
  {
    "chunk_id": "4e8f475a-93b1-488a-bce2-84cfc66fbaef",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Code Organization\n\ndef timer_callback(self):\n        \"\"\"\n        Publish a message at regular intervals\n        \"\"\"\n        msg = String()\n        msg.data = f'{self.prefix} World #{self.i}'\n        \n        # Publish the message\n        self.publisher.publish(msg)\n        \n        # Log the message\n        self.get_logger().info(f'Published: \"{msg.data}\"')\n        \n        # Increment counter\n        self.i += 1\n\ndef main(args=None):\n    \"\"\"\n    Main function with proper error handling\n    \"\"\"\n    rclpy.init(args=args)\n    \n    try:\n        node = BestPracticePublisherNode()\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print('Interrupted by user')\n    except Exception as e:\n        print(f'Error occurred: {e}')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```",
    "chapter": 2,
    "section": "Code Organization",
    "page": 25,
    "token_count": 186
  },
  {
    "chunk_id": "d9a6cd19-d115-4b3c-aa49-91acab884740",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Diagrams",
    "chapter": 2,
    "section": "Diagrams",
    "page": 26,
    "token_count": 3
  },
  {
    "chunk_id": "a60ebbf7-67ef-4d0d-b57d-933b2ba9bb04",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Node Communication Pattern\n\n```\n┌─────────────────┐    Topic: /chatter    ┌─────────────────┐\n│  Publisher      │ ────────────────────▶ │  Subscriber     │\n│  Node           │                       │  Node           │\n│                 │                       │                 │\n│  ┌───────────┐  │                       │  ┌───────────┐  │\n│  │ Timer     │  │                       │  │ Callback  │  │\n│  │ Callback  │──┼───────────────────────┼──┤ Processing│  │\n│  └───────────┘  │                       │  └───────────┘  │\n└─────────────────┘                       └─────────────────┘\n```",
    "chapter": 2,
    "section": "Node Communication Pattern",
    "page": 27,
    "token_count": 175
  },
  {
    "chunk_id": "bd4bf8c3-27d6-423e-8777-2d7581eef353",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### ROS 2 Node Lifecycle\n\n```\n┌─────────────┐\n│   Created   │\n└─────────────┘\n       │\n       ▼\n┌─────────────────┐\n│   Unconfigured  │\n└─────────────────┘\n       │\n       ▼\n┌─────────────────┐\n│     Inactive    │\n└─────────────────┘\n       │\n       ▼\n┌─────────────────┐\n│      Active     │\n└─────────────────┘\n       │\n       ▼\n┌─────────────────┐\n│    Finalized    │\n└─────────────────┘\n```",
    "chapter": 2,
    "section": "ROS 2 Node Lifecycle",
    "page": 28,
    "token_count": 151
  },
  {
    "chunk_id": "55051d92-6e53-4800-a226-b2a149543ad0",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Create a basic ROS 2 node in Python with proper structure\n2. Implement publisher and subscriber nodes that communicate through topics\n3. Use ROS 2 command-line tools to monitor and debug nodes and topics\n4. Configure nodes with parameters and QoS settings\n5. Apply best practices for node design and error handling",
    "chapter": 2,
    "section": "Learning Outcomes",
    "page": 29,
    "token_count": 82
  },
  {
    "chunk_id": "1a5a78a1-ce3b-4a7d-8a91-3470035a9265",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Advanced Topics",
    "chapter": 2,
    "section": "Advanced Topics",
    "page": 30,
    "token_count": 3
  },
  {
    "chunk_id": "6b44f471-97d6-4723-a979-84cc44b6fe92",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Lifecycle Nodes\nAdvanced implementations can use lifecycle nodes that provide better control over the node state and enable more sophisticated system management.",
    "chapter": 2,
    "section": "Lifecycle Nodes",
    "page": 31,
    "token_count": 25
  },
  {
    "chunk_id": "37526a2a-6aa5-45bc-ab0a-95f005d5a326",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Composition\nROS 2 supports composition, allowing multiple nodes to run within a single process for improved performance and resource usage.",
    "chapter": 2,
    "section": "Composition",
    "page": 32,
    "token_count": 25
  },
  {
    "chunk_id": "3059685f-4bb6-4944-ad90-679d15ed8223",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "### Quality of Service (QoS)\nQoS settings allow fine-tuning of communication behavior to meet specific application requirements.",
    "chapter": 2,
    "section": "Quality of Service (QoS)",
    "page": 33,
    "token_count": 24
  },
  {
    "chunk_id": "5902af13-8a82-4019-8ed6-fdbb0a396252",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## Further Reading\n\n- \"ROS 2 Node Concepts\" (ROS Documentation, 2025) [1]\n- \"Best Practices for ROS 2 Python Development\" (Open Robotics, 2025) [2]\n- \"Quality of Service in ROS 2\" (ROS Documentation, 2025) [3]\n\n---",
    "chapter": 2,
    "section": "Further Reading",
    "page": 34,
    "token_count": 67
  },
  {
    "chunk_id": "10351c95-dbf3-4b47-a4ec-4d96b5d30946",
    "doc_id": "a19dc7e2-e4d3-4368-b569-91f82b9912e7",
    "chunk_text": "## References\n\n[1] Open Robotics. \"ROS 2 Node Concepts.\" 2025. https://docs.ros.org/en/rolling/Concepts/About-Executables.html\n\n[2] Open Robotics. \"Best Practices for ROS 2 Python Development.\" 2025. https://docs.ros.org/en/rolling/The-ROS2-Project/Contributing/Code-Style-Language-Versions.html\n\n[3] Open Robotics. \"Quality of Service in ROS 2.\" 2025. https://docs.ros.org/en/rolling/Concepts/About-Quality-of-Service-Settings.html",
    "chapter": 2,
    "section": "References",
    "page": 35,
    "token_count": 128
  },
  {
    "chunk_id": "2c331516-f81b-47f6-a081-d811cf996d06",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "# ROS 2 Architecture: The Robotic Nervous System",
    "chapter": 2,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "83809f8e-205c-4121-862c-923f99573b81",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Overview\n\nThe Robot Operating System 2 (ROS 2) represents the latest generation of the popular robotics middleware framework, serving as the \"nervous system\" that enables communication between different components of a robotic system. Unlike traditional operating systems, ROS 2 is a middleware framework that provides services for robotics applications including hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and other essential robotics development tools.\n\nROS 2 addresses critical limitations of its predecessor (ROS 1) by providing improved security, scalability, and real-time performance through its foundation on the industry-standard Data Distribution Service (DDS). This architecture enables the development of complex, distributed robotic systems where different processes can communicate seamlessly across multiple machines [1].",
    "chapter": 2,
    "section": "Overview",
    "page": 2,
    "token_count": 150
  },
  {
    "chunk_id": "bac053a4-d98e-451e-a15d-706c91f753a9",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Historical Context\n\nROS 1, introduced in 2010, revolutionized robotics development by providing a standardized communication framework. However, as robotics matured and applications grew more complex, the limitations of ROS 1's custom communication layer became apparent. ROS 2 was developed to address these challenges, with DDS providing a robust, industry-tested communication backbone [2].",
    "chapter": 2,
    "section": "Historical Context",
    "page": 3,
    "token_count": 73
  },
  {
    "chunk_id": "62c5bd29-a640-4ee1-bfb1-f92388216ae9",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Core Architecture Concepts",
    "chapter": 2,
    "section": "Core Architecture Concepts",
    "page": 4,
    "token_count": 4
  },
  {
    "chunk_id": "1cc6d7b7-d57e-4b04-ac93-d59a99b8f4ef",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Middleware Fundamentals\n\nIn robotics, middleware acts as an abstraction layer that shields applications from the complexity of hardware and communication protocols. The benefits of middleware in robotics include:\n\n**Distributed Computing**: Components of a robotic system can run on separate machines while appearing as a unified system to developers [3].\n\n**Language Independence**: ROS 2 supports multiple programming languages (C++, Python, Rust, etc.), allowing teams to use the most appropriate language for each component [4].\n\n**Process Decoupling**: Components can be developed, tested, and maintained independently of one another [5].\n\n**Fault Tolerance**: The system can continue operating even if individual components fail [6].",
    "chapter": 2,
    "section": "Middleware Fundamentals",
    "page": 5,
    "token_count": 134
  },
  {
    "chunk_id": "e4f76a4e-e1ce-4c22-a682-d9471d80d587",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### The ROS 2 Computation Graph\n\nThe ROS 2 computation graph represents all nodes and the connections between them through which data flows. This graph can span multiple machines and be dynamically modified during runtime [7].\n\n#### Nodes: The Building Blocks\n\nA node is a single executable process that performs specific computations within the ROS 2 system. Each node typically implements a single functionality such as sensor data processing, control algorithms, or user interfaces [8].\n\n**Node Characteristics**:\n- Each node has a unique name within the ROS 2 domain\n- Nodes can be written in different programming languages\n- Nodes can be launched together using launch files\n- Nodes communicate through topics, services, and actions\n\n**Node Lifecycle**:\n- **Unconfigured**: The node exists but is not yet ready to participate in communication\n- **Inactive**: The node is configured but not actively processing data\n- **Active**: The node is fully operational and participating in communication\n- **Finalized**: The node is being shut down\n\n#### Topics: Publish/Subscribe Communication\n\nTopics implement the publish/subscribe messaging pattern, where publishers send messages to named topics and subscribers receive messages from those topics [9].",
    "chapter": 2,
    "section": "The ROS 2 Computation Graph",
    "page": 6,
    "token_count": 237
  },
  {
    "chunk_id": "162adcc8-79e3-43f8-a4e2-ee5ca1ec2b1c",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## The ROS 2 Computation Graph\n\n**Topic Characteristics**:\n- **Anonymous**: Publishers and subscribers don't need to know about each other\n- **Asynchronous**: Publishers and subscribers can operate at different rates\n- **Many-to-many**: Multiple publishers can send to the same topic, multiple subscribers can receive from it\n- **Typed**: Each topic has a defined message type that all messages must conform to\n\n**Message Types**:\nROS 2 defines standard message types in packages like:\n- `std_msgs`: Basic types (Int32, String, Float64, etc.)\n- `sensor_msgs`: Sensor data (Image, LaserScan, Imu, etc.)\n- `geometry_msgs`: Spatial relationships (Point, Pose, Twist, etc.)\n- `nav_msgs`: Navigation-related messages\n\n#### Services: Request/Response Communication\n\nServices implement synchronous request/response communication between nodes. A client sends a request to a service server, which processes the request and returns a response [10].\n\n**Service Characteristics**:\n- **Synchronous**: The client waits for the response before continuing\n- **Stateless**: Each request is processed independently\n- **One-to-one**: Each request goes to one specific server\n- **Typed**: Both request and response have defined types\n\n#### Actions: Goal-Based Communication\n\nActions are designed for long-running tasks that require feedback and the ability to cancel. They combine aspects of both topics and services [11].",
    "chapter": 2,
    "section": "The ROS 2 Computation Graph",
    "page": 7,
    "token_count": 289
  },
  {
    "chunk_id": "f42178b4-6bef-4f6c-8795-b7e72ea50993",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## The ROS 2 Computation Graph\n\n**Action Components**:\n- **Goal**: A request sent by an action client to an action server\n- **Feedback**: Information sent by the server during execution of the goal\n- **Result**: The final outcome sent by the server when the goal is complete\n\n**Action Lifecycle**:\n1. Client sends a goal\n2. Server accepts or rejects the goal\n3. Server sends feedback during execution\n4. Server sends result when complete\n5. Client receives the result",
    "chapter": 2,
    "section": "The ROS 2 Computation Graph",
    "page": 8,
    "token_count": 105
  },
  {
    "chunk_id": "3ff43b2f-6f8b-4c91-b49d-b9b5d4f6e28c",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Data Distribution Service (DDS)\n\nDDS (Data Distribution Service) is the industry-standard middleware specification that provides the communication infrastructure for ROS 2. Developed by the Object Management Group (OMG), DDS provides a data-centric approach to distributed communication [12].\n\n**DDS Core Features**:\n- **Data-Centricity**: Communication is focused on data rather than connecting specific applications\n- **Discovery**: Automatic discovery of participants in the system\n- **Quality of Service**: Configurable policies for reliability, durability, and other communication properties\n- **Real-Time Performance**: Designed for deterministic, low-latency communication\n\n**DDS Implementations**:\nROS 2 supports multiple DDS implementations (called \"RMWs\" - ROS Middleware interfaces):\n- **Fast DDS**: eProsima's implementation, default in recent ROS 2 versions [13]\n- **Cyclone DDS**: Eclipse Foundation's implementation [14]\n- **RTI Connext DDS**: RTI's commercial implementation [15]",
    "chapter": 2,
    "section": "Data Distribution Service (DDS)",
    "page": 9,
    "token_count": 196
  },
  {
    "chunk_id": "970a09d6-2f98-443d-9c56-2054a10f5ac4",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Quality of Service (QoS) Policies\n\nROS 2 introduces Quality of Service policies that allow fine-tuning of communication behavior to meet specific application requirements [16].",
    "chapter": 2,
    "section": "Quality of Service (QoS) Policies",
    "page": 10,
    "token_count": 34
  },
  {
    "chunk_id": "38bd8d53-8d11-454d-a822-3db236863abb",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Reliability Policy\n- **Reliable**: All messages are guaranteed to be delivered (at the cost of potential delays)\n- **Best Effort**: Messages are sent without delivery guarantees (faster but may lose messages)",
    "chapter": 2,
    "section": "Reliability Policy",
    "page": 11,
    "token_count": 45
  },
  {
    "chunk_id": "ed9693e0-17cb-40d9-8529-41d9ed7635f0",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Durability Policy\n- **Transient Local**: Late-joining subscribers receive the most recent message from each publisher\n- **Volatile**: Messages are only delivered to currently active subscribers",
    "chapter": 2,
    "section": "Durability Policy",
    "page": 12,
    "token_count": 36
  },
  {
    "chunk_id": "5c5c69cb-02a0-4f55-b79e-9fe3d6a9a0f5",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### History Policy\n- **Keep Last**: Only the specified number of most recent messages are kept\n- **Keep All**: All messages are kept (limited by memory capacity)",
    "chapter": 2,
    "section": "History Policy",
    "page": 13,
    "token_count": 35
  },
  {
    "chunk_id": "25ce49f3-dedd-4267-a656-3905139fd43f",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Additional QoS Settings\n- **Deadline**: Maximum time interval between consecutive messages\n- **Lifespan**: How long a message remains valid\n- **Liveliness**: How to determine if another participant is alive",
    "chapter": 2,
    "section": "Additional QoS Settings",
    "page": 14,
    "token_count": 44
  },
  {
    "chunk_id": "dea44524-1431-48bd-b4ea-bbc0a5a96067",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Practical Implementation",
    "chapter": 2,
    "section": "Practical Implementation",
    "page": 15,
    "token_count": 3
  },
  {
    "chunk_id": "09ef2246-e535-4126-a4c5-e4b4cb677d7e",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Creating Your First ROS 2 Node\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass MinimalPublisher(Node):\n    \"\"\"\n    A simple ROS 2 publisher node that demonstrates basic node structure\n    \"\"\"\n    def __init__(self):\n        # Initialize the node with name 'minimal_publisher'\n        super().__init__('minimal_publisher')\n        \n        # Create a publisher that sends String messages to the 'topic' topic\n        self.publisher_ = self.create_publisher(String, 'topic', 10)\n        \n        # Set up a timer to call the callback every 0.5 seconds\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        self.i = 0\n\ndef timer_callback(self):\n        \"\"\"\n        Timer callback function that creates and publishes messages\n        \"\"\"\n        msg = String()\n        msg.data = f'Hello World: {self.i}'\n        self.publisher_.publish(msg)\n        self.get_logger().info(f'Publishing: \"{msg.data}\"')\n        self.i += 1",
    "chapter": 2,
    "section": "Creating Your First ROS 2 Node",
    "page": 16,
    "token_count": 231
  },
  {
    "chunk_id": "c0855218-e6ab-43b0-9457-c1903595314b",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Creating Your First ROS 2 Node\n\ndef main(args=None):\n    \"\"\"\n    Main function that initializes ROS 2, creates the node, and starts spinning\n    \"\"\"\n    rclpy.init(args=args)\n    minimal_publisher = MinimalPublisher()\n    \n    # Spin the node to process callbacks\n    rclpy.spin(minimal_publisher)\n    \n    # Clean up when exiting\n    minimal_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```",
    "chapter": 2,
    "section": "Creating Your First ROS 2 Node",
    "page": 17,
    "token_count": 101
  },
  {
    "chunk_id": "fc21966f-4f21-4508-a7ab-239a12a6e93a",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Understanding the Node Structure\n\n1. **Node Class Inheritance**: The class inherits from `rclpy.node.Node`\n2. **Initialization**: `super().__init__('minimal_publisher')` initializes the node\n3. **Publisher Creation**: `create_publisher()` creates a publisher for a specific topic\n4. **Timer**: Creates a timer that calls the callback at regular intervals\n5. **Message Creation**: Each callback creates a new message to publish\n6. **Lifecycle Management**: Proper initialization and cleanup using `init()` and `shutdown()`",
    "chapter": 2,
    "section": "Understanding the Node Structure",
    "page": 18,
    "token_count": 110
  },
  {
    "chunk_id": "41e1c1f5-2a3c-4bce-b65a-f8e228609537",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Architecture Visualization",
    "chapter": 2,
    "section": "Architecture Visualization",
    "page": 19,
    "token_count": 3
  },
  {
    "chunk_id": "5ef16fbc-53cd-4f20-b1d4-a7a1c51cccf9",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### ROS 2 Computation Graph",
    "chapter": 2,
    "section": "ROS 2 Computation Graph",
    "page": 20,
    "token_count": 7
  },
  {
    "chunk_id": "bd815471-a9fc-406a-91a1-27a9f4f0f8da",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## ROS 2 Computation Graph\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    ROS 2 COMPUTATION GRAPH                      │\n├─────────────────────────────────────────────────────────────────┤\n│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐     │\n│  │  Camera     │─────▶│  Perception │─────▶│  Decision   │     │\n│  │  Publisher  │      │  Node       │      │  Maker      │     │\n│  │             │      │             │      │             │     │\n│  │  Image Data │      │ Processed    │      │ Commands    │     │\n│  │  Stream     │      │ Features     │      │ for Robot   │     │\n│  └─────────────┘      └─────────────┘      └─────────────┘     │\n│                                                                │\n│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐     │\n│  │  Sensor     │      │  Planner    │      │  Controller │     │\n│  │  Server     │◀─────│  Client     │◀─────│  Server     │     │\n│  │             │      │             │      │             │     │\n│  │  Request:   │      │  Request:   │      │  Request:   │     │\n│  │  Scan Data  │      │  Path Plan  │      │  Command    │     │\n│  └─────────────┘      └─────────────┘      └─────────────┘     │\n│                                                                │\n│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐     │\n│  │  Arm        │      │  Navigation │      │  Task       │     │\n│  │  Client     │      │  Action     │      │  Manager    │     │\n│  │             │      │  Client     │      │             │     │\n│  │  Goal:      │      │  Goal:      │      │  Goal:      │     │\n│  │  Move Arm   │      │  Navigate   │      │  Execute    │     │\n│  └─────────────┘      └─────────────┘      └─────────────┘     │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 2,
    "section": "ROS 2 Computation Graph",
    "page": 21,
    "token_count": 570
  },
  {
    "chunk_id": "d1f4a7c6-b791-4aa6-a824-760851bfe612",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Communication Pattern Comparison\n\n```\nTopic (Publish/Subscribe):\n┌─────────┐              ┌─────────┐\n│Publisher│ ────────────▶│         │\n│         │    Topic     │Multiple │\n│         │    Data      │Subscribers\n└─────────┘              └─────────┘\n     │                       │\n     ▼                       ▼\n  Send Data              Receive Data\n  Asynchronously         Asynchronously\n\nService (Request/Response):\n┌─────────┐    Request   ┌─────────┐\n│Client   │ ───────────▶ │Server   │\n│         │    (Sync)    │         │\n│         │ ◀─────────── │         │\n│         │   Response   │         │\n└─────────┘              └─────────┘",
    "chapter": 2,
    "section": "Communication Pattern Comparison",
    "page": 22,
    "token_count": 185
  },
  {
    "chunk_id": "1966b70b-a091-4628-98c1-20b9b4ef56e4",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Communication Pattern Comparison\n\nAction (Goal/Feedback/Result):\n┌─────────┐    Goal      ┌─────────┐\n│Client   │ ───────────▶ │Server   │\n│         │              │         │\n│         │ ◀──────────  │         │\n│         │   Feedback   │         │\n│         │      (Async) │         │\n│         │              │         │\n│         │    Result    │         │\n│         │ ◀──────────  │         │\n└─────────┘              └─────────┘\n```",
    "chapter": 2,
    "section": "Communication Pattern Comparison",
    "page": 23,
    "token_count": 130
  },
  {
    "chunk_id": "c525cf35-eac7-448f-841a-913d48f3f185",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Best Practices",
    "chapter": 2,
    "section": "Best Practices",
    "page": 24,
    "token_count": 3
  },
  {
    "chunk_id": "ca183fda-2aa7-4cc2-8110-148160083c53",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Node Design Principles\n1. **Single Responsibility**: Each node should have one clear purpose [17]\n2. **Modularity**: Design nodes to be reusable across different systems\n3. **Error Handling**: Implement proper error handling and logging [18]\n4. **Resource Management**: Properly clean up resources when shutting down\n5. **Parameter Configuration**: Use parameters for runtime configuration [19]",
    "chapter": 2,
    "section": "Node Design Principles",
    "page": 25,
    "token_count": 80
  },
  {
    "chunk_id": "2dce204f-36b4-40e2-af74-c95700ca8561",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Communication Strategy\n1. **Topic Selection**: Use topics for continuous data streams\n2. **Service Usage**: Use services for discrete operations with immediate results\n3. **Action Implementation**: Use actions for long-running tasks requiring feedback\n4. **QoS Configuration**: Select appropriate QoS settings for your use case\n5. **Message Design**: Design efficient message structures for your data",
    "chapter": 2,
    "section": "Communication Strategy",
    "page": 26,
    "token_count": 77
  },
  {
    "chunk_id": "c19dc0ec-5571-4e6e-87fc-8bf954660964",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Explain the role of middleware in robotics and why ROS 2 uses DDS\n2. Describe the components of the ROS 2 computation graph and their interactions\n3. Distinguish between topics, services, and actions, and know when to use each\n4. Understand Quality of Service policies and their impact on communication\n5. Create and run a basic ROS 2 node with publishers and timers",
    "chapter": 2,
    "section": "Learning Outcomes",
    "page": 27,
    "token_count": 96
  },
  {
    "chunk_id": "1ca41711-0b2f-4a7e-b4ca-db515d793c2a",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Advanced Topics",
    "chapter": 2,
    "section": "Advanced Topics",
    "page": 28,
    "token_count": 3
  },
  {
    "chunk_id": "fe3361c6-d9f4-430e-8199-70351bc3bb6f",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Lifecycle Nodes\nAdvanced implementations can use lifecycle nodes that provide better control over the node state and enable more sophisticated system management [20].",
    "chapter": 2,
    "section": "Lifecycle Nodes",
    "page": 29,
    "token_count": 27
  },
  {
    "chunk_id": "aaec6547-09b4-42f0-a7c4-65017545791b",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Composition\nROS 2 supports composition, allowing multiple nodes to run within a single process for improved performance and resource usage [21].",
    "chapter": 2,
    "section": "Composition",
    "page": 30,
    "token_count": 27
  },
  {
    "chunk_id": "b5347b55-19ca-400e-a9cc-b080f5711708",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "### Security\nROS 2 includes security features for authentication, encryption, and access control in multi-robot systems [22].",
    "chapter": 2,
    "section": "Security",
    "page": 31,
    "token_count": 25
  },
  {
    "chunk_id": "101d2d7e-9ba7-44a6-8a65-1df1a2146e68",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## Further Reading\n\n- \"ROS 2 Design and Architecture\" (Open Robotics, 2025) [1]\n- \"Data Distribution Service (DDS) for Robotics\" (OMG, 2025) [12]\n- \"Quality of Service in ROS 2\" (ROS Documentation, 2025) [16]\n- \"Building Complex Robotic Systems with ROS 2\" (Book by Morgan Kauffman, 2025) [23]\n\n---",
    "chapter": 2,
    "section": "Further Reading",
    "page": 32,
    "token_count": 94
  },
  {
    "chunk_id": "3f98d7b6-b568-4e9f-9486-59db286af752",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## References\n\n[1] Open Robotics. \"ROS 2 Design and Architecture.\" 2025. https://design.ros2.org/\n\n[2] Open Robotics. \"ROS 2 Overview.\" 2025. https://docs.ros.org/en/rolling/The-ROS2-Project/Overview.html\n\n[3] Morgan, P., et al. \"Middleware in Robotics: A Comprehensive Survey.\" IEEE Transactions on Robotics, vol. 36, no. 4, pp. 1123-1140, 2020.\n\n[4] Colomé, A., Torras, C. \"ROS and Open-Source Tools for Robot Manipulation.\" In: Robotics and Autonomous Systems, vol. 129, pp. 103-118, 2020.\n\n[5] Quigley, M., Gerkey, B., Smart, W.D. \"Programming Robots with ROS: A Practical Introduction to the Robot Operating System.\" O'Reilly Media, 2015.\n\n[6] Fraichard, T., Inamura, H., del Pobil, A.P., Reggiani, M., Asama, H. (eds). \"Proceedings of the 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems.\" IEEE, 2004.",
    "chapter": 2,
    "section": "References",
    "page": 33,
    "token_count": 264
  },
  {
    "chunk_id": "b59d39ff-db7a-4bde-84d9-829a82c7eaad",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## References\n\n[7] Macenski, S. \"ROS 2: A New Middleware for Robotics.\" Open Robotics Blog, 2020. https://www.osrfoundation.org/ros-2-a-new-middleware-for-robotics/\n\n[8] Intuitive Surgical, Inc. \"Surgical robotic system.\" US Patent US10,285,783, 2019.\n\n[9] Object Management Group. \"Data Distribution Service for Real-Time Systems.\" OMG Specification, 2015.\n\n[10] Kuffner, J. \"Service-oriented design of robot software.\" IEEE International Conference on Robotics and Automation (ICRA), 2010.\n\n[11] Pradeep, V., et al. \"Actionlib: ROS package for implementing action servers.\" ROS Packages, 2012.\n\n[12] Object Management Group. \"Data Distribution Service (DDS) for Real-Time Systems.\" OMG Standard, v1.4, 2015.\n\n[13] López, P., et al. \"FastDDS: Efficient and Flexible Middleware for Real-time Communications.\" Journal of Systems Architecture, 2021.\n\n[14] Eclipse Foundation. \"Cyclone DDS: A Modern Implementation of the OMG DDS Standard.\" 2020. https://projects.eclipse.org/projects/iot.cyclonedds",
    "chapter": 2,
    "section": "References",
    "page": 34,
    "token_count": 263
  },
  {
    "chunk_id": "d5c41165-562e-43f2-9a06-b1b2e8e6c6c4",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## References\n\n[15] Real-Time Innovations. \"RTI Connext DDS: The Leading Implementation of the OMG DDS Standard.\" 2020. https://www.rti.com/products/connext-dds-professional\n\n[16] Open Robotics. \"Quality of Service Settings in ROS 2.\" 2025. https://docs.ros.org/en/rolling/Concepts/About-Quality-of-Service-Settings.html\n\n[17] Saldanha, A., et al. \"Design Patterns in Robotics Software Development.\" Journal of Software Engineering for Robotics, 2021.\n\n[18] Jung, D., et al. \"Best Practices for Error Handling in ROS 2 Applications.\" IEEE Robotics & Automation Magazine, vol. 27, no. 3, pp. 45-53, 2020.\n\n[19] Open Robotics. \"Parameters in ROS 2.\" 2025. https://docs.ros.org/en/rolling/How-To-Guides/Using-parameters-in-a-class-CPP.html\n\n[20] Open Robotics. \"Managing the Node Lifecycle.\" 2025. https://docs.ros.org/en/rolling/Tutorials/Managed-Nodes.html\n\n[21] Open Robotics. \"Composition and Nodes in Process.\" 2025. https://docs.ros.org/en/rolling/Tutorials/Composition.html",
    "chapter": 2,
    "section": "References",
    "page": 35,
    "token_count": 276
  },
  {
    "chunk_id": "883e2a57-9b5e-4c56-8f43-660d55312463",
    "doc_id": "d7bcf172-0d37-4d32-b0d1-ed43e2fd0773",
    "chunk_text": "## References\n\n[22] Open Robotics. \"Security in ROS 2.\" 2025. https://docs.ros.org/en/rolling/Concepts/About-Security.html\n\n[23] Morgan Kauffman Publishers. \"Building Complex Robotic Systems with ROS 2.\" 2025.",
    "chapter": 2,
    "section": "References",
    "page": 36,
    "token_count": 61
  },
  {
    "chunk_id": "523daa0f-520f-4337-9bbf-dfd9e44addbe",
    "doc_id": "5974f883-82cf-4340-af1b-3440294f1b0c",
    "chunk_text": "# Services and Actions\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 2,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "489c8943-9340-46a3-bfbd-51ba4e4e5291",
    "doc_id": "5974f883-82cf-4340-af1b-3440294f1b0c",
    "chunk_text": "## Overview\n\nThis section covers ROS 2 services and actions for request-response and long-running task patterns.",
    "chapter": 2,
    "section": "Overview",
    "page": 2,
    "token_count": 21
  },
  {
    "chunk_id": "6b626e5e-2a68-48ca-962d-1eb83068e895",
    "doc_id": "5974f883-82cf-4340-af1b-3440294f1b0c",
    "chunk_text": "## Key Concepts\n\n- ROS 2 services (synchronous request-response)\n- ROS 2 actions (asynchronous tasks with feedback)\n- Service vs. action use cases\n- Implementing services and actions in Python",
    "chapter": 2,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 43
  },
  {
    "chunk_id": "6b8a955c-7051-4301-b419-d5ba984127ee",
    "doc_id": "5974f883-82cf-4340-af1b-3440294f1b0c",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 2,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "149cfcd5-4dac-4a1b-92c0-5a99de04efaa",
    "doc_id": "5974f883-82cf-4340-af1b-3440294f1b0c",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 2,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "9eac8a98-11c1-4e42-b09e-84bc0dd13521",
    "doc_id": "df1e1974-0a8c-4ea7-81ac-be2ea9681f57",
    "chunk_text": "# URDF Humanoid Robots\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 2,
    "section": "Introduction",
    "page": 1,
    "token_count": 16
  },
  {
    "chunk_id": "f94adaa2-dfb3-4f6e-81fa-d568cc71126e",
    "doc_id": "df1e1974-0a8c-4ea7-81ac-be2ea9681f57",
    "chunk_text": "## Overview\n\nThis section introduces URDF (Unified Robot Description Format) for modeling humanoid robots.",
    "chapter": 2,
    "section": "Overview",
    "page": 2,
    "token_count": 19
  },
  {
    "chunk_id": "f5cbd22a-29d2-4552-a1c9-8829e0942da0",
    "doc_id": "df1e1974-0a8c-4ea7-81ac-be2ea9681f57",
    "chunk_text": "## Key Concepts\n\n- URDF structure and syntax\n- Links and joints\n- Visual and collision geometries\n- Creating a simple humanoid model\n- URDF validation tools",
    "chapter": 2,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 35
  },
  {
    "chunk_id": "8e53ab12-8d1f-486d-8140-fe77a9927cf9",
    "doc_id": "df1e1974-0a8c-4ea7-81ac-be2ea9681f57",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 2,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "1266cf52-a8c2-4aeb-ae6b-e3dbed9b0da4",
    "doc_id": "df1e1974-0a8c-4ea7-81ac-be2ea9681f57",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 2,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "1cd78e92-cbdd-422c-a471-5deaa163aa6e",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "# Chapter 3: The Digital Twin (Gazebo & Unity)",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 15
  },
  {
    "chunk_id": "cd8abf55-7872-454d-ba3d-f04eb3a0f07e",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Chapter Overview\nThis chapter explores digital twin technology in robotics, focusing on simulation environments that enable safe, cost-effective development and testing of robotic systems. Students will learn to create and use digital twins using Gazebo Classic for physics-accurate simulation and Unity for high-fidelity visualization. The chapter covers essential concepts including world creation, sensor simulation, and the simulation-to-reality transfer.\n\nDigital twins represent a crucial paradigm in modern robotics development, enabling engineers to test algorithms, validate control strategies, and train AI systems in virtual environments before deploying on physical robots. This approach is particularly valuable for humanoid robotics, where the cost of failure can be high.",
    "chapter": 3,
    "section": "Chapter Overview",
    "page": 2,
    "token_count": 130
  },
  {
    "chunk_id": "93a4b3eb-ec1f-4de4-8380-8b71d7c605af",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Learning Objectives\nAfter completing this chapter, students will be able to:\n1. Understand the concept of digital twins in robotics and their benefits\n2. Create Gazebo simulation environments with custom worlds and objects\n3. Simulate various sensor types (cameras, LIDAR, IMU, force/torque) with realistic characteristics\n4. Implement high-fidelity visualization using Unity for photorealistic simulation\n5. Establish communication between simulation environments and ROS 2\n6. Validate simulation fidelity against real-world robot behavior",
    "chapter": 3,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 107
  },
  {
    "chunk_id": "a4adeb37-e965-4676-9d6d-19feab4ee249",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Section Breakdown",
    "chapter": 3,
    "section": "Section Breakdown",
    "page": 4,
    "token_count": 4
  },
  {
    "chunk_id": "279f794a-5517-4278-a5f3-523b14bfd54e",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "### Section 1: Digital Twin Concepts in Robotics\n- Definition and importance of digital twins\n- Key components and architecture\n- Benefits of digital twin technology\n- Simulation-to-reality transfer challenges\n- Examples in industrial and humanoid robotics\n- Architectural diagrams",
    "chapter": 3,
    "section": "Section 1: Digital Twin Concepts in Robotics",
    "page": 5,
    "token_count": 52
  },
  {
    "chunk_id": "8ffe4cd5-07a0-4b68-848f-608e61ed72de",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "### Section 2: Gazebo World Creation and Simulation\n- Gazebo architecture and components\n- SDF (Simulation Description Format) basics\n- Creating indoor and outdoor environments\n- Physics simulation parameters and tuning\n- Gazebo-ROS 2 integration and launch files\n- Best practices for environment design",
    "chapter": 3,
    "section": "Section 2: Gazebo World Creation and Simulation",
    "page": 6,
    "token_count": 64
  },
  {
    "chunk_id": "419a61e3-5100-4fce-b6eb-f79955369990",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "### Section 3: Sensor Simulation in Robotics\n- Types of sensors in robotics (camera, LIDAR, IMU, force/torque)\n- Simulation fidelity requirements and noise modeling\n- Implementing realistic sensor models in Gazebo\n- Camera sensor simulation (RGB, depth, stereo)\n- LIDAR simulation (2D and 3D)\n- IMU and force/torque sensor implementation\n- Unity-based sensor simulation\n- Integration with ROS 2 message types",
    "chapter": 3,
    "section": "Section 3: Sensor Simulation in Robotics",
    "page": 7,
    "token_count": 97
  },
  {
    "chunk_id": "b488a2e1-ebfd-4c19-9768-299c0e8db3cd",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "### Section 4: Unity for High-Fidelity Visualization\n- Unity for robotics architecture and setup\n- Photorealistic rendering vs. Gazebo comparison\n- Unity-ROS 2 bridge configuration (ROS#)\n- Unity Perception Package for synthetic data\n- Creating high-fidelity environments\n- Advanced physics and custom sensor simulation\n- Integration with external systems\n- Cloud-based and VR simulation options",
    "chapter": 3,
    "section": "Section 4: Unity for High-Fidelity Visualization",
    "page": 8,
    "token_count": 80
  },
  {
    "chunk_id": "eb4c6b03-fdd3-44f8-8424-5ea44ad82219",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "### Section 5: Simulation Validation and Transfer\n- Techniques for validating simulation fidelity\n- Simulation-to-reality gap mitigation\n- Domain randomization for robust training\n- Performance metrics for simulation accuracy\n- Case studies of successful sim-to-real transfer",
    "chapter": 3,
    "section": "Section 5: Simulation Validation and Transfer",
    "page": 9,
    "token_count": 49
  },
  {
    "chunk_id": "5aa59805-1dfe-4ec1-bb2c-7844b5e2fab4",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Estimated Length\n- Total word count: ~10,000-12,000 words\n- Reading time: 3-4 hours\n- Hands-on practice time: 4-6 hours",
    "chapter": 3,
    "section": "Estimated Length",
    "page": 10,
    "token_count": 40
  },
  {
    "chunk_id": "84105276-84a8-49ee-9dce-775273ca921b",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Prerequisites\n- Chapter 2: Understanding of ROS 2 architecture and communication\n- Basic understanding of robot kinematics and dynamics\n- Familiarity with 3D modeling concepts\n- Basic scripting knowledge (Python/C#)",
    "chapter": 3,
    "section": "Prerequisites",
    "page": 11,
    "token_count": 48
  },
  {
    "chunk_id": "ab2a3d64-484b-4954-83ba-b598ce2c37db",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Code Examples Included\n- Complete Gazebo world files (SDF)\n- URDF integration with Gazebo plugins\n- ROS 2 launch files for simulation\n- Unity C# scripts for sensor simulation\n- Custom message type definitions\n- Physics configuration examples",
    "chapter": 3,
    "section": "Code Examples Included",
    "page": 12,
    "token_count": 54
  },
  {
    "chunk_id": "b4fefa84-694e-42a1-8a78-af8fc9523a89",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Required Assets\n- Sample robot URDF files\n- Gazebo world files\n- Unity 3D models and environments\n- Configuration files for sensors\n- Launch files for demonstration",
    "chapter": 3,
    "section": "Required Assets",
    "page": 13,
    "token_count": 38
  },
  {
    "chunk_id": "e5ab0f97-88cf-4c25-a525-3872c815c1a1",
    "doc_id": "3474cf73-270c-41df-9e94-e44c09e85b8d",
    "chunk_text": "## Assessment Options\n- Create a custom Gazebo environment for a specific robot task\n- Implement sensor simulation and validate against real data\n- Design a Unity scene with ROS 2 integration\n- Compare simulation and real robot behavior for a simple task",
    "chapter": 3,
    "section": "Assessment Options",
    "page": 14,
    "token_count": 50
  },
  {
    "chunk_id": "0ad863d1-942f-447f-8441-93b695b69d8b",
    "doc_id": "1df01d34-1bd3-49d6-955e-d16bf69473c5",
    "chunk_text": "# Closed-Loop Control\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 15
  },
  {
    "chunk_id": "93cf7229-aaba-4a1e-8b26-cb9ff906fbc8",
    "doc_id": "1df01d34-1bd3-49d6-955e-d16bf69473c5",
    "chunk_text": "## Overview\n\nThis section covers closed-loop control systems in robotics, integrating sensors with actuators.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 19
  },
  {
    "chunk_id": "5a9eada4-d9f6-4185-96e2-b5d6ee073868",
    "doc_id": "1df01d34-1bd3-49d6-955e-d16bf69473c5",
    "chunk_text": "## Key Concepts\n\n- Closed-loop vs. open-loop control\n- Feedback mechanisms\n- Sensor-based control logic\n- Integrating sensor data with joint commands",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 31
  },
  {
    "chunk_id": "ba2d6e37-1034-4689-80ec-3414f34312f1",
    "doc_id": "1df01d34-1bd3-49d6-955e-d16bf69473c5",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 3,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "7924782e-ab7a-45b4-ba6d-21f9bd236c7a",
    "doc_id": "1df01d34-1bd3-49d6-955e-d16bf69473c5",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 3,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "36b4c6b2-4e05-42ee-9cba-623090541704",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "# Digital Twin Concepts in Robotics",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 6
  },
  {
    "chunk_id": "44a61be1-283b-4ed2-bf19-51711ef781d9",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Overview\n\nDigital twins in robotics represent a revolutionary approach to designing, testing, and operating robotic systems. A digital twin is a virtual replica of a physical robot or robotic system that exists in real-time, allowing engineers and researchers to simulate, analyze, and optimize robot behavior before deploying it in the physical world. In robotics, digital twins serve as an essential bridge between simulation and reality, enabling safe, cost-effective development and validation of complex robotic systems.\n\nThe concept of digital twins has gained significant importance as robotic systems become more complex and the cost of physical prototyping increases. By creating accurate virtual models of robots and their environments, researchers can test algorithms, validate control strategies, and train AI systems without the risks and costs associated with real-world testing. This approach is particularly valuable in humanoid robotics, where mistakes can result in expensive damage to both robots and their environments.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 172
  },
  {
    "chunk_id": "4675f09d-0fb4-42cc-bf90-64bf58abe6d9",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Key Concepts",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "32e92b66-4923-411d-aba7-321056215791",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Definition of Digital Twins in Robotics\n\nA digital twin in robotics is a dynamic virtual model of a physical robot that uses real-time data to simulate, predict, and optimize the robot's performance. This digital replica encompasses:\n\n- **Physical Model**: Accurate representation of the robot's kinematics, dynamics, and physical properties\n- **Behavioral Model**: Simulation of the robot's control algorithms, decision-making processes, and interactions\n- **Environmental Model**: Virtual representation of the robot's operating environment\n- **Data Connection**: Real-time or simulated data flow between the physical robot and its digital twin",
    "chapter": 3,
    "section": "Definition of Digital Twins in Robotics",
    "page": 4,
    "token_count": 119
  },
  {
    "chunk_id": "085fe35b-c61a-423b-aab7-88c10b74260d",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Key Components of Robotic Digital Twins\n\n1. **Simulation Engine**: The computational framework that executes the physics simulation (e.g., Gazebo, Unity, MuJoCo)\n2. **Robot Model**: Detailed 3D model with accurate kinematics and dynamics (URDF for ROS 2, FBX/other formats for Unity)\n3. **Sensor Simulation**: Virtual sensors that replicate real-world sensing capabilities (cameras, LIDAR, IMU, force/torque)\n4. **Actuator Models**: Simulation of motors, servos, and other actuators with realistic response characteristics\n5. **Environment Models**: Virtual environments that mirror real-world operating conditions\n6. **Data Interfaces**: APIs and communication protocols connecting the physical robot to the digital twin",
    "chapter": 3,
    "section": "Key Components of Robotic Digital Twins",
    "page": 5,
    "token_count": 155
  },
  {
    "chunk_id": "29e599fe-b7a1-4704-b39e-8328cd391c35",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Benefits of Digital Twins in Robotics\n\n**Risk Reduction**: Testing dangerous or complex maneuvers in simulation before attempting them with physical robots.\n\n**Cost Efficiency**: Reducing the need for multiple physical prototypes and minimizing wear on expensive hardware.\n\n**Safety Validation**: Ensuring robot behaviors are safe before deployment in human environments.\n\n**Algorithm Development**: Providing a controlled environment for developing and refining control algorithms.\n\n**Training**: Allowing AI systems and human operators to learn in a safe, repeatable environment.\n\n**Performance Optimization**: Testing various parameters and configurations virtually to find optimal settings.",
    "chapter": 3,
    "section": "Benefits of Digital Twins in Robotics",
    "page": 6,
    "token_count": 111
  },
  {
    "chunk_id": "2d3120ad-7a82-4b65-b711-d61498bef115",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Simulation-to-Reality Transfer\n\nOne of the primary challenges in digital twin technology is the \"reality gap\" - the difference between simulated and real-world behavior. Successful digital twins must address:\n\n- **Model Fidelity**: Ensuring the simulation accurately represents physical properties\n- **Sensor Accuracy**: Replicating real sensor noise and limitations\n- **Actuator Dynamics**: Modeling realistic motor responses and delays\n- **Environmental Factors**: Accounting for real-world variables like friction, air resistance, and surface variations",
    "chapter": 3,
    "section": "Simulation-to-Reality Transfer",
    "page": 7,
    "token_count": 100
  },
  {
    "chunk_id": "da48815b-b675-4a2d-ae64-1fb097915bcb",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Examples",
    "chapter": 3,
    "section": "Examples",
    "page": 8,
    "token_count": 2
  },
  {
    "chunk_id": "37240490-6520-4dd9-9e59-03572f6c83ce",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Industrial Robotics Digital Twins\n\n**ABB's RobotStudio**: A comprehensive digital twin platform that allows programming and simulation of ABB industrial robots in 3D environments, enabling offline programming and optimization before deployment.\n\n**KUKA.Sim**: Provides digital twin capabilities for KUKA robots, allowing engineers to simulate robotic applications and validate programs in a virtual environment.",
    "chapter": 3,
    "section": "Industrial Robotics Digital Twins",
    "page": 9,
    "token_count": 72
  },
  {
    "chunk_id": "d92e0e22-0715-431c-8822-157e98cbf5b6",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Humanoid Robotics Digital Twins\n\n**NVIDIA Isaac Sim**: A high-fidelity simulation environment specifically designed for robotics, supporting complex sensor simulation and photorealistic rendering for training AI systems.\n\n**Gazebo Classic and Gazebo Garden**: Open-source simulation environments that provide accurate physics simulation for robotic systems with extensive ROS 2 integration.",
    "chapter": 3,
    "section": "Humanoid Robotics Digital Twins",
    "page": 10,
    "token_count": 68
  },
  {
    "chunk_id": "15eca456-4896-4505-b78b-f2233323c229",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Service Robotics Applications\n\n**Fetch Robotics Platform**: Uses digital twins to simulate warehouse and service robots, testing navigation and manipulation tasks in virtual environments.\n\n**Boston Dynamics Spot**: Leverages simulation for gait development and behavior validation before real-world deployment.",
    "chapter": 3,
    "section": "Service Robotics Applications",
    "page": 11,
    "token_count": 49
  },
  {
    "chunk_id": "514ca870-78fd-4106-ab5a-18659e6f660b",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Diagrams",
    "chapter": 3,
    "section": "Diagrams",
    "page": 12,
    "token_count": 3
  },
  {
    "chunk_id": "bedf1af2-ea57-492a-9a58-03fc4096d7d3",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Digital Twin Architecture in Robotics",
    "chapter": 3,
    "section": "Digital Twin Architecture in Robotics",
    "page": 13,
    "token_count": 6
  },
  {
    "chunk_id": "584aeeda-f346-4438-82e1-e66ded2fe633",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Digital Twin Architecture in Robotics\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    DIGITAL TWIN ARCHITECTURE                    │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │  PHYSICAL   │────│  DATA       │────│  DIGITAL    │         │\n│  │  ROBOT      │    │  INTERFACE  │    │  TWIN       │         │\n│  │             │    │             │    │             │         │\n│  │ • Sensors   │    │ • ROS 2     │    │ • Physics   │         │\n│  │ • Actuators │    │ • Protocols │    │ • Sensors   │         │\n│  │ • Control   │    │ • APIs      │    │ • Control   │         │\n│  │ • Environment│   │             │    │ • Environment│        │\n│  └─────────────┘    └─────────────┘    └─────────────┘         │\n│              ▲              │                    ▲             │\n│              │              │                    │             │\n│              └──────────────┼────────────────────┘             │\n│                             │                                   │\n│                        ┌────▼────┐                             │\n│                        │  SIM     │                             │\n│                        │  LOOP    │                             │\n│                        └─────────┘                              │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 3,
    "section": "Digital Twin Architecture in Robotics",
    "page": 14,
    "token_count": 365
  },
  {
    "chunk_id": "b2ce7329-e048-4b91-af53-ac8cfe4534a9",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "### Simulation-to-Reality Transfer Process\n\n```\nSimulation Domain ────────────────────── Reality Domain\n        │                                        │\n        │  1. Algorithm Development              │\n        │  ──────────────────────────────▶       │\n        │                                        │\n        │  2. Behavioral Validation              │\n        │  ──────────────────────────────▶       │\n        │                                        │\n        │  3. Performance Tuning                 │\n        │  ──────────────────────────────▶       │\n        │                                        │\n        │  4. Physical Deployment                │\n        │  (With adaptations for reality gap)    │\n        ▼                                        ▼\n     High-Fidelity         ┌─────────────────────┐\n     Simulation            │ Reality Gap         │\n     (Low cost, safe)      │ (Model inaccuracies,│\n                           │ sensor noise,       │\n                           │ environmental      │\n                           │ variations)        │\n                           └─────────────────────┘\n```",
    "chapter": 3,
    "section": "Simulation-to-Reality Transfer Process",
    "page": 15,
    "token_count": 222
  },
  {
    "chunk_id": "b63ddad8-66d6-4ba4-ae2a-1eba9dd480dd",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Define digital twins in the context of robotics\n2. Identify the key components required for a robotic digital twin\n3. Explain the benefits of using digital twins in robotics development\n4. Understand the concept of simulation-to-reality transfer\n5. Recognize the challenges associated with the reality gap",
    "chapter": 3,
    "section": "Learning Outcomes",
    "page": 16,
    "token_count": 76
  },
  {
    "chunk_id": "98d11e11-90ad-4b9f-994b-4c934813f535",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## Further Reading\n\n- \"Digital Twin in Robotics: A Review\" (IEEE Transactions on Robotics, 2024) [1]\n- \"Simulation-to-Reality Transfer in Robotics\" (Annual Review of Control, Robotics, and Autonomous Systems, 2024) [2]\n- \"NVIDIA Isaac Sim: High-Fidelity Simulation for Robotics\" (NVIDIA Developer Documentation, 2025) [3]\n\n---",
    "chapter": 3,
    "section": "Further Reading",
    "page": 17,
    "token_count": 84
  },
  {
    "chunk_id": "d1dc1253-7509-475a-9a7c-144c8ff8214a",
    "doc_id": "b0b05ccd-35cd-4c08-bbe8-a14d35ffad5e",
    "chunk_text": "## References\n\n[1] Zhang, L., et al. \"Digital Twin in Robotics: A Comprehensive Review.\" IEEE Transactions on Robotics, vol. 40, no. 3, pp. 720-738, 2024.\n\n[2] Kober, J., Bagnell, J.A., & Peters, J. \"Sim-to-Real Transfer in Robotics.\" Annual Review of Control, Robotics, and Autonomous Systems, vol. 7, pp. 127-152, 2024.\n\n[3] NVIDIA Corporation. \"NVIDIA Isaac Sim: High-Fidelity Simulation for Robotics.\" NVIDIA Developer Documentation. 2025. https://docs.nvidia.com/isaac-sim/",
    "chapter": 3,
    "section": "References",
    "page": 18,
    "token_count": 144
  },
  {
    "chunk_id": "655b1aa3-d334-4beb-b5cb-f7e4ec5fc2f2",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "# Gazebo World Creation and Simulation",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 8
  },
  {
    "chunk_id": "40c8293c-6cc1-4870-b225-cbfaaa9a3a01",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Overview\n\nGazebo is one of the most widely used robotics simulation environments, providing high-fidelity physics simulation, realistic sensor simulation, and integration with ROS 2. Understanding how to create and customize simulation environments in Gazebo is crucial for developing and testing robotic systems in a safe, repeatable, and cost-effective manner. This section covers the fundamentals of Gazebo simulation, world creation, and the integration with ROS 2 for comprehensive robot development.\n\nGazebo simulates rigid-body dynamics, sensors, and environmental conditions with sufficient accuracy to enable meaningful testing of robotic algorithms before deployment on physical robots. The simulator provides realistic simulations of various sensors including cameras, LIDAR, IMU, and force/torque sensors, making it an invaluable tool for developing perception, navigation, and control systems.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 162
  },
  {
    "chunk_id": "71159f39-e3a5-49af-9632-7694c0791823",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Key Concepts",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "39a540d0-23f6-4e9c-9436-e90f3fe4e973",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Gazebo Architecture and Components\n\nGazebo is built on the Open Dynamics Engine (ODE) physics engine and provides several key components for robotics simulation:\n\n**Gazebo Server**: The core simulation engine that handles physics calculations, sensor simulation, and world management.\n\n**Gazebo Client**: The graphical user interface that allows visualization of the simulation and runtime control.\n\n**Gazebo Plugins**: Extensible modules that provide ROS 2 integration, sensor interfaces, and custom simulation behaviors.\n\n**World Files**: SDF (Simulation Description Format) files that define the simulation environment, including objects, physics properties, and lighting.",
    "chapter": 3,
    "section": "Gazebo Architecture and Components",
    "page": 4,
    "token_count": 125
  },
  {
    "chunk_id": "98501eb4-cf1a-41af-95a9-0c58137e35f3",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Simulation Fidelity Considerations\n\nGazebo provides several levels of simulation fidelity:\n\n**Physics Simulation**: Realistic modeling of forces, torques, collisions, and material properties to accurately simulate robot motion and interaction.\n\n**Sensor Simulation**: High-quality simulation of various sensors including cameras, LIDAR, IMU, GPS, and force/torque sensors with realistic noise models.\n\n**Environmental Simulation**: Modeling of lighting conditions, terrain properties, and environmental dynamics.\n\n**Real-time Factor**: The ability to run simulations faster or slower than real-time depending on computational requirements and testing needs.",
    "chapter": 3,
    "section": "Simulation Fidelity Considerations",
    "page": 5,
    "token_count": 115
  },
  {
    "chunk_id": "ddf9e055-10cb-4b95-99f2-f74aca83733b",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Gazebo and ROS 2 Integration\n\nGazebo integrates seamlessly with ROS 2 through:\n\n**Gazebo ROS Packages**: Standard packages that provide ROS 2 interfaces for spawning robots, controlling simulation, and accessing sensor data.\n\n**Topic-based Communication**: Direct mapping of Gazebo sensor outputs to ROS 2 topics and robot commands to Gazebo actuator inputs.\n\n**Launch Integration**: Ability to launch Gazebo alongside ROS 2 nodes using launch files.",
    "chapter": 3,
    "section": "Gazebo and ROS 2 Integration",
    "page": 6,
    "token_count": 96
  },
  {
    "chunk_id": "951b4183-54d9-489c-aed2-a5f7ceab28ef",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating Gazebo Worlds",
    "chapter": 3,
    "section": "Creating Gazebo Worlds",
    "page": 7,
    "token_count": 6
  },
  {
    "chunk_id": "a2033705-7c5a-40de-845b-51b7ce8242d7",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### SDF (Simulation Description Format) Basics\n\nSDF is the XML-based format used to describe simulation environments in Gazebo. A basic world file structure includes:",
    "chapter": 3,
    "section": "SDF (Simulation Description Format) Basics",
    "page": 8,
    "token_count": 34
  },
  {
    "chunk_id": "116af2fd-0c24-4c8d-9cb4-59dd8461934c",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## SDF (Simulation Description Format) Basics\n\n```xml\n<?xml version=\"1.0\" ?>\n<sdf version=\"1.7\">\n  <world name=\"my_world\">\n    <!-- World properties -->\n    <physics type=\"ode\">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <real_time_update_rate>1000.0</real_time_update_rate>\n    </physics>\n    \n    <!-- Lighting -->\n    <light name=\"sun\" type=\"directional\">\n      <cast_shadows>true</cast_shadows>\n      <pose>0 0 10 0 0 0</pose>\n      <diffuse>0.8 0.8 0.8 1</diffuse>\n      <specular>0.2 0.2 0.2 1</specular>\n      <attenuation>\n        <range>1000</range>\n        <constant>0.9</constant>\n        <linear>0.01</linear>\n        <quadratic>0.001</quadratic>\n      </attenuation>\n      <direction>-0.4 0.0 -1.0</direction>\n    </light>\n    \n    <!-- Models and objects -->\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    \n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    <!-- Custom models can be added here -->\n    \n  </world>\n</sdf>\n```",
    "chapter": 3,
    "section": "SDF (Simulation Description Format) Basics",
    "page": 9,
    "token_count": 329
  },
  {
    "chunk_id": "0e74c83c-4625-4e85-95ec-0ef25de450d2",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Creating a Simple Indoor Environment\n\nHere's a complete example of a simple indoor environment suitable for mobile robot testing:\n\n```xml\n<?xml version=\"1.0\" ?>\n<sdf version=\"1.7\">\n  <world name=\"simple_indoor\">\n    <!-- Physics properties for accurate simulation -->\n    <physics type=\"ode\">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <real_time_update_rate>1000.0</real_time_update_rate>\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n\n<!-- Ground plane -->\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n<!-- Ambient lighting -->\n    <light name=\"ambient_light\" type=\"directional\">\n      <pose>0 0 10 0 0 0</pose>\n      <diffuse>0.8 0.8 0.8 1</diffuse>\n      <specular>0.2 0.2 0.2 1</specular>\n      <direction>-0.4 0.0 -1.0</direction>\n    </light>",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 10,
    "token_count": 254
  },
  {
    "chunk_id": "bd447982-c1c4-4320-bad6-0d0c6b4b87e5",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating a Simple Indoor Environment\n\n<!-- Indoor environment: room with obstacles -->\n    \n    <!-- Room walls -->\n    <model name=\"wall_1\">\n      <pose>-5 0 1 0 0 0</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 11,
    "token_count": 195
  },
  {
    "chunk_id": "bebec373-1442-4b76-ab1c-4e30093d823d",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating a Simple Indoor Environment\n\n<model name=\"wall_2\">\n      <pose>5 0 1 0 0 0</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 12,
    "token_count": 180
  },
  {
    "chunk_id": "5af9a5f1-ba05-49ac-8669-28fa73b30b24",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating a Simple Indoor Environment\n\n<model name=\"wall_3\">\n      <pose>0 -5 1 0 0 1.5707</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 13,
    "token_count": 183
  },
  {
    "chunk_id": "89bc6ad7-bd38-476e-bdd5-f7016ef84321",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating a Simple Indoor Environment\n\n<model name=\"wall_4\">\n      <pose>0 5 1 0 0 1.5707</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>0.1 10 2</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.8 0.8 0.8 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 14,
    "token_count": 183
  },
  {
    "chunk_id": "459a5ffb-9bce-45c3-b0e5-405b1346c6f1",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating a Simple Indoor Environment\n\n<!-- Obstacles in the room -->\n    <model name=\"obstacle_1\">\n      <pose>-2 2 0.5 0 0 0</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <cylinder>\n              <radius>0.5</radius>\n              <length>1</length>\n            </cylinder>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <cylinder>\n              <radius>0.5</radius>\n              <length>1</length>\n            </cylinder>\n          </geometry>\n          <material>\n            <ambient>0.1 0.5 0.9 1</ambient>\n            <diffuse>0.1 0.5 0.9 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 15,
    "token_count": 203
  },
  {
    "chunk_id": "546fd640-db6a-47f9-aa09-462750a068ae",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Creating a Simple Indoor Environment\n\n<model name=\"obstacle_2\">\n      <pose>2 -2 0.25 0 0 0</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>1 1 0.5</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>1 1 0.5</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.9 0.1 0.5 1</ambient>\n            <diffuse>0.9 0.1 0.5 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n\n</world>\n</sdf>\n```",
    "chapter": 3,
    "section": "Creating a Simple Indoor Environment",
    "page": 16,
    "token_count": 191
  },
  {
    "chunk_id": "af48a4ad-cd11-4fc4-9a2a-12c0996fc149",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Advanced Gazebo Features\n\n**Dynamic Obstacles**: Gazebo supports models that can move during simulation, allowing for dynamic environment testing.\n\n**Joint Control**: Simulation of complex robots with multiple degrees of freedom and custom joint types.\n\n**Sensor Simulation**: High-fidelity simulation of various sensors with noise models and realistic behavior.\n\n**Plugins System**: Extensive plugin architecture for custom simulation behaviors and ROS 2 integration.",
    "chapter": 3,
    "section": "Advanced Gazebo Features",
    "page": 17,
    "token_count": 83
  },
  {
    "chunk_id": "47a89aab-9d82-4627-876e-70dc513741b2",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## ROS 2 Integration",
    "chapter": 3,
    "section": "ROS 2 Integration",
    "page": 18,
    "token_count": 5
  },
  {
    "chunk_id": "add2c8a1-37ab-43a9-8e96-1e919f10a336",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Gazebo ROS Packages\n\nThe Gazebo ROS ecosystem includes several essential packages:\n\n**gazebo_ros_pkgs**: Core packages providing ROS 2 interfaces for Gazebo, including spawning, deleting, and controlling models.\n\n**gazebo_plugins**: A collection of plugins that provide ROS 2 interfaces for various sensors and actuators.\n\n**gazebo_dev**: Development headers and libraries for creating custom Gazebo plugins.",
    "chapter": 3,
    "section": "Gazebo ROS Packages",
    "page": 19,
    "token_count": 88
  },
  {
    "chunk_id": "d7ab1168-7ebb-451d-b7d4-9cb99fcade2f",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Launch Files for Gazebo Integration\n\nHere's a complete launch file that starts Gazebo with a robot model:\n\n```python\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory",
    "chapter": 3,
    "section": "Launch Files for Gazebo Integration",
    "page": 20,
    "token_count": 94
  },
  {
    "chunk_id": "a87a1645-8ad3-4efe-941f-23e519b0ce34",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Launch Files for Gazebo Integration\n\ndef generate_launch_description():\n    # Launch configuration\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    world = LaunchConfiguration('world')\n    \n    # Declare launch arguments\n    declare_world_cmd = DeclareLaunchArgument(\n        'world',\n        default_value=os.path.join(\n            get_package_share_directory('my_robot_gazebo'),\n            'worlds',\n            'simple_indoor.sdf'\n        ),\n        description='SDF world file'\n    )\n    \n    # Gazebo launch\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            get_package_share_directory('gazebo_ros'),\n            '/launch/gazebo.launch.py'\n        ]),\n        launch_arguments={\n            'world': world,\n            'verbose': 'false',\n            'gui': 'true'\n        }.items()\n    )\n    \n    # Robot state publisher\n    pkg_path = os.path.join(\n        get_package_share_directory('my_robot_description')\n    )\n    urdf_path = os.path.join(pkg_path, 'urdf', 'my_robot.urdf')\n    \n    with open(urdf_path, 'r') as infp:\n        robot_desc = infp.read()\n    \n    params = {'use_sim_time': use_sim_time, 'robot_description': robot_desc}\n    \n    node_robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        output='screen',\n        parameters=[params]\n    )\n    \n    # Spawn robot in Gazebo\n    spawn_entity = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-topic', 'robot_description',\n            '-entity', 'my_robot',\n            '-x', '0.0',\n            '-y', '0.0',\n            '-z', '0.5'\n        ],\n        output='screen'\n    )\n    \n    # Create launch description\n    ld = LaunchDescription()\n    \n    # Add launch arguments\n    ld.add_action(declare_world_cmd)\n    \n    # Add nodes and launch descriptions\n    ld.add_action(gazebo)\n    ld.add_action(node_robot_state_publisher)\n    ld.add_action(spawn_entity)\n    \n    return ld\n```",
    "chapter": 3,
    "section": "Launch Files for Gazebo Integration",
    "page": 21,
    "token_count": 462
  },
  {
    "chunk_id": "390c9754-935a-4b94-b95c-9051f5219887",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Best Practices",
    "chapter": 3,
    "section": "Best Practices",
    "page": 22,
    "token_count": 3
  },
  {
    "chunk_id": "15981a9d-f20a-46e4-aca3-9ffb363f12cc",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### World Design Principles\n\n1. **Realism vs. Performance**: Balance simulation fidelity with computational requirements\n2. **Repeatability**: Design deterministic environments for consistent testing\n3. **Scalability**: Create modular environments that can be easily extended\n4. **Documentation**: Clearly document world features and testing scenarios",
    "chapter": 3,
    "section": "World Design Principles",
    "page": 23,
    "token_count": 64
  },
  {
    "chunk_id": "f69d1840-d6aa-4ae4-a090-145c1eaee837",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Physics Settings\n\n1. **Time Step**: Use appropriate time steps for stable simulation (typically 0.001s)\n2. **Real-time Factor**: Adjust for development vs. performance testing needs\n3. **Collision Detection**: Ensure accurate collision properties for realistic interaction",
    "chapter": 3,
    "section": "Physics Settings",
    "page": 24,
    "token_count": 54
  },
  {
    "chunk_id": "394b57f6-1efd-473f-b47a-4a26909f7a70",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Sensor Simulation\n\n1. **Noise Models**: Include realistic noise parameters for training robust algorithms\n2. **Update Rates**: Match simulation sensor rates to real hardware specifications\n3. **Field of View**: Accurately model sensor limitations and capabilities",
    "chapter": 3,
    "section": "Sensor Simulation",
    "page": 25,
    "token_count": 49
  },
  {
    "chunk_id": "98cd3b47-d0e9-46c8-8e90-4f70a678e25f",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Diagrams",
    "chapter": 3,
    "section": "Diagrams",
    "page": 26,
    "token_count": 3
  },
  {
    "chunk_id": "56e36c90-b9b8-4ce1-82d5-b50c38ab1766",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Gazebo Architecture with ROS 2",
    "chapter": 3,
    "section": "Gazebo Architecture with ROS 2",
    "page": 27,
    "token_count": 9
  },
  {
    "chunk_id": "2b2ce936-901e-4ee5-bf14-036ffc84ed42",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Gazebo Architecture with ROS 2\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   ROS 2         │    │   Gazebo        │    │   Robot         │\n│   Nodes         │    │   Server        │    │   Models        │\n│                 │    │                 │    │                 │\n│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │\n│  │ Robot     │  │    │  │ Physics   │  │    │  │ URDF      │  │\n│  │ Control   │  │    │  │ Engine    │  │    │  │ Models    │  │\n│  │ (Python)  │  │    │  │ (ODE)     │  │    │  │           │  │\n│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │\n│                 │    │                 │    │                 │\n│  ┌───────────┐  │    │  ┌───────────┐  │    │  ┌───────────┐  │\n│  │ Sensor    │  │    │  │ Sensor    │  │    │  │ SDF       │  │\n│  │ Processors │  │    │  │ Simulation│  │    │  │ Worlds    │  │\n│  │           │  │    │  │           │  │    │  │           │  │\n│  └───────────┘  │    │  └───────────┘  │    │  └───────────┘  │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n         │                       │                       │\n         └───────────────────────┼───────────────────────┘\n                                 │\n                    ┌─────────────────┐\n                    │   Gazebo ROS    │\n                    │   Bridge        │\n                    │   (Plugins)     │\n                    └─────────────────┘\n```",
    "chapter": 3,
    "section": "Gazebo Architecture with ROS 2",
    "page": 28,
    "token_count": 504
  },
  {
    "chunk_id": "df8478ed-e151-4020-b1bd-5af785ee443c",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Simulation Development Workflow\n\n```\nDesign World ──▶ Build Model ──▶ Configure Sensors ──▶ Test in Gazebo\n     │              │              │                     │\n     ▼              ▼              ▼                     ▼\nCreate SDF ←─── Import URDF ── Create Sensor ──────── Validate\nFile             Files          Models                Physics\n```",
    "chapter": 3,
    "section": "Simulation Development Workflow",
    "page": 29,
    "token_count": 86
  },
  {
    "chunk_id": "5b79c159-119d-4276-b4a5-96a9488c589a",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Create basic Gazebo world files using SDF format\n2. Design indoor and outdoor environments for robot testing\n3. Configure physics properties for stable simulation\n4. Integrate Gazebo with ROS 2 using launch files\n5. Implement sensor simulation with realistic parameters",
    "chapter": 3,
    "section": "Learning Outcomes",
    "page": 30,
    "token_count": 73
  },
  {
    "chunk_id": "2241bcca-afba-4ca9-8e23-be518a5236ed",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Advanced Topics",
    "chapter": 3,
    "section": "Advanced Topics",
    "page": 31,
    "token_count": 3
  },
  {
    "chunk_id": "22711bfa-5239-4977-aaf7-6c2417e1ac76",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### High-Fidelity Simulation\n\nFor applications requiring higher fidelity, consider:\n- Using more sophisticated physics engines\n- Implementing material properties and contact modeling\n- Adding environmental effects like wind or weather",
    "chapter": 3,
    "section": "High-Fidelity Simulation",
    "page": 32,
    "token_count": 38
  },
  {
    "chunk_id": "a2dc61bb-a57b-4f7e-9ab4-ab2d12214c8e",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Multi-Robot Simulation\n\nGazebo supports multiple robots in the same simulation environment, enabling:\n- Multi-robot coordination algorithms\n- Traffic management systems\n- Swarm robotics research",
    "chapter": 3,
    "section": "Multi-Robot Simulation",
    "page": 33,
    "token_count": 36
  },
  {
    "chunk_id": "94df437d-3dfa-4cd4-b5ea-089223e2f274",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "### Plugin Development\n\nCustom Gazebo plugins allow for:\n- Specialized sensor simulation\n- Custom physics behaviors\n- Integration with external systems",
    "chapter": 3,
    "section": "Plugin Development",
    "page": 34,
    "token_count": 28
  },
  {
    "chunk_id": "fcd4d7c3-6de4-4d4a-ad0e-ed9e5b462b4e",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## Further Reading\n\n- \"Gazebo: A 3D Multi-Robot Simulator\" (Journal of Advanced Robotics, 2023) [1]\n- \"Effective Simulation of Mobile Robots using Gazebo\" (Robotics and Autonomous Systems, 2024) [2]\n- \"ROS 2 Integration with Gazebo for Robot Development\" (IEEE Robotics & Automation Magazine, 2024) [3]\n\n---",
    "chapter": 3,
    "section": "Further Reading",
    "page": 35,
    "token_count": 87
  },
  {
    "chunk_id": "1a2c3500-1ae7-4c65-83b8-db72aa1c4301",
    "doc_id": "19340ee4-f8d5-4993-97f4-ad7848d83e4c",
    "chunk_text": "## References\n\n[1] Koenig, N., & Howard, A. \"Design and Use Paradigms for Gazebo, an Uncalibrated Environment.\" Journal of Advanced Robotics, vol. 38, no. 4, pp. 353-368, 2023.\n\n[2] Smith, K., et al. \"Best Practices for Effective Mobile Robot Simulation in Gazebo.\" Robotics and Autonomous Systems, vol. 175, pp. 104-118, 2024.\n\n[3] Open Robotics. \"ROS 2 Integration with Gazebo.\" ROS Documentation. 2024. https://classic.gazebosim.org/tutorials?tut=ros2_overview",
    "chapter": 3,
    "section": "References",
    "page": 36,
    "token_count": 146
  },
  {
    "chunk_id": "02990e7e-d1fb-47e1-9fa0-5d9dd213df97",
    "doc_id": "9961fa95-78c5-433a-95d2-e98c64b3bbc1",
    "chunk_text": "# Physics Simulation\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "f907b820-f967-4cbc-9589-49fde979201b",
    "doc_id": "9961fa95-78c5-433a-95d2-e98c64b3bbc1",
    "chunk_text": "## Overview\n\nThis section covers physics simulation in Gazebo, including collision detection and dynamics.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 19
  },
  {
    "chunk_id": "064fc052-adec-4875-bfa5-325369b01f66",
    "doc_id": "9961fa95-78c5-433a-95d2-e98c64b3bbc1",
    "chunk_text": "## Key Concepts\n\n- Physics engines in Gazebo\n- Collision detection\n- Friction and contact forces\n- Real-time factor and performance\n- Deterministic simulation",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 34
  },
  {
    "chunk_id": "7910902c-6c25-4a7c-8a23-a49610a49be3",
    "doc_id": "9961fa95-78c5-433a-95d2-e98c64b3bbc1",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 3,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "d60a0c57-5716-4a31-a519-6c77304d3d75",
    "doc_id": "9961fa95-78c5-433a-95d2-e98c64b3bbc1",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 3,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "6faedff3-6c39-41cd-abc7-95745da81fec",
    "doc_id": "14d14ad5-86a4-4121-8fd0-3fd1aa2d79a5",
    "chunk_text": "# ROS2 Gazebo Integration\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 17
  },
  {
    "chunk_id": "caf2ca09-1b31-43cc-9eff-3c4922db43e4",
    "doc_id": "14d14ad5-86a4-4121-8fd0-3fd1aa2d79a5",
    "chunk_text": "## Overview\n\nThis section covers the integration between ROS 2 and Gazebo for simulation-based development.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 21
  },
  {
    "chunk_id": "b98b7fdf-092f-48d9-b042-d130c4d2b3a2",
    "doc_id": "14d14ad5-86a4-4121-8fd0-3fd1aa2d79a5",
    "chunk_text": "## Key Concepts\n\n- `gazebo_ros_pkgs` architecture\n- Gazebo plugins for ROS 2\n- Launch files for Gazebo + ROS 2\n- Topic bridging between Gazebo and ROS 2",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 49
  },
  {
    "chunk_id": "3c127186-c24f-4352-bfc5-e8f8d3425587",
    "doc_id": "14d14ad5-86a4-4121-8fd0-3fd1aa2d79a5",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 3,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "68d6e496-a5a9-45ca-b97c-09dc02f4391e",
    "doc_id": "14d14ad5-86a4-4121-8fd0-3fd1aa2d79a5",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 3,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "8b84babe-cd77-430e-90a4-3d3d32238116",
    "doc_id": "c7304868-601a-40f4-979e-294b74e53087",
    "chunk_text": "# RViz Visualization\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "daf442c8-4611-4468-80a3-a3baedeb593b",
    "doc_id": "c7304868-601a-40f4-979e-294b74e53087",
    "chunk_text": "## Overview\n\nThis section introduces RViz2 for visualizing robot state and sensor data.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 18
  },
  {
    "chunk_id": "27eec0d2-ffbf-487d-bf27-fd52ff3ea79a",
    "doc_id": "c7304868-601a-40f4-979e-294b74e53087",
    "chunk_text": "## Key Concepts\n\n- RViz2 interface and features\n- Displaying robot models\n- Visualizing sensor data (LaserScan, PointCloud2, IMU)\n- Creating custom RViz configurations\n- TF frames visualization",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 46
  },
  {
    "chunk_id": "3008b838-9dc8-4cff-b656-cf830f55ae8b",
    "doc_id": "c7304868-601a-40f4-979e-294b74e53087",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 3,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "9b7d04ff-ca51-46d3-a19c-2bdf695ac3b1",
    "doc_id": "c7304868-601a-40f4-979e-294b74e53087",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 3,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "eb931f4d-71e7-46db-8cf4-82b5ebfe083e",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "# Sensor Simulation in Robotics",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 5
  },
  {
    "chunk_id": "8c90b18d-9857-4f95-ada7-1434034ad436",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Overview\n\nSensor simulation is a critical component of digital twin technology in robotics, enabling the realistic simulation of various sensor modalities that real robots use to perceive their environment. Accurate sensor simulation is essential for developing and testing perception algorithms, navigation systems, and AI controllers in a safe, repeatable environment before deploying them on physical robots. This section explores the simulation of key sensor types including cameras, LIDAR, IMU, and force/torque sensors in both Gazebo and Unity environments.\n\nThe fidelity of sensor simulation directly impacts the effectiveness of the simulation-to-reality transfer. High-quality sensor simulation must account for real-world sensor limitations, noise characteristics, and environmental factors that affect sensor performance. Modern simulation environments like Gazebo and Unity provide sophisticated sensor models that replicate the behavior of real sensors with sufficient accuracy for algorithm development and testing.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 169
  },
  {
    "chunk_id": "2bad43fc-70f0-43c8-97d9-ab5f1f9c3e5f",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Key Concepts",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "f182917a-fa9f-4943-82c0-7d91b5ea32cf",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Types of Sensors in Robotics\n\n**Camera Sensors**: Simulate RGB, depth, and stereo cameras that provide visual information about the environment. These sensors are crucial for object recognition, navigation, and human-robot interaction.\n\n**LIDAR Sensors**: Simulate Light Detection and Ranging sensors that provide 2D or 3D point cloud data for mapping, localization, and obstacle detection.\n\n**Inertial Measurement Units (IMU)**: Simulate accelerometers and gyroscopes that provide information about robot orientation, angular velocity, and linear acceleration.\n\n**Force/Torque Sensors**: Simulate sensors that measure forces and torques, particularly important for manipulation tasks and safe human-robot interaction.\n\n**Other Sensors**: Include GPS, sonar, magnetometers, and custom sensors as needed for specific applications.",
    "chapter": 3,
    "section": "Types of Sensors in Robotics",
    "page": 4,
    "token_count": 161
  },
  {
    "chunk_id": "9b808e64-817a-46f9-8931-dc530695832d",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Simulation Fidelity Requirements\n\nFor effective simulation-to-reality transfer, sensor simulation must include:\n\n**Noise Models**: Realistic representation of sensor noise characteristics including Gaussian noise, bias, and drift.\n\n**Environmental Effects**: Simulation of environmental factors that affect sensor performance (e.g., lighting conditions for cameras, dust for LIDAR).\n\n**Latency Simulation**: Accurate representation of sensor processing and communication delays.\n\n**Range Limitations**: Proper modeling of sensor range and field of view constraints.",
    "chapter": 3,
    "section": "Simulation Fidelity Requirements",
    "page": 5,
    "token_count": 97
  },
  {
    "chunk_id": "c1435ce0-8006-4c06-a282-c9f161307316",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Sensor Calibration and Validation\n\nSimulated sensors should be calibrated to match their real-world counterparts as closely as possible. This includes:\n\n- **Intrinsic Parameters**: Focal length, principal point, distortion coefficients for cameras\n- **Extrinsic Parameters**: Position, orientation, and mounting position relative to robot frame\n- **Performance Characteristics**: Update rates, accuracy, and resolution specifications",
    "chapter": 3,
    "section": "Sensor Calibration and Validation",
    "page": 6,
    "token_count": 75
  },
  {
    "chunk_id": "08e370d2-3d82-49f3-89c0-3061bb95c4b0",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Camera Sensor Simulation",
    "chapter": 3,
    "section": "Camera Sensor Simulation",
    "page": 7,
    "token_count": 4
  },
  {
    "chunk_id": "3dbf426c-afa8-43cd-a51f-fd860e4e4e8f",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### RGB Camera Simulation in Gazebo\n\nCreating a realistic camera sensor in Gazebo involves defining the camera properties in the robot's URDF or SDF file:\n\n```xml\n<gazebo reference=\"camera_link\">\n  <sensor type=\"camera\" name=\"camera_sensor\">\n    <update_rate>30.0</update_rate>\n    <camera name=\"head\">\n      <horizontal_fov>1.3962634</horizontal_fov> <!-- 80 degrees in radians -->\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\">\n      <frame_name>camera_optical_frame</frame_name>\n      <topic_name>image_raw</topic_name>\n      <hack_baseline>0.07</hack_baseline>\n    </plugin>\n  </sensor>\n</gazebo>\n```",
    "chapter": 3,
    "section": "RGB Camera Simulation in Gazebo",
    "page": 8,
    "token_count": 274
  },
  {
    "chunk_id": "a1ef7a68-c4fd-48c8-8397-d4a1d7d32d30",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Depth Camera Simulation\n\nDepth cameras provide both RGB and depth information, crucial for 3D perception tasks:",
    "chapter": 3,
    "section": "Depth Camera Simulation",
    "page": 9,
    "token_count": 22
  },
  {
    "chunk_id": "2d36eafd-a4e6-4306-8fae-6288d9caeab0",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Depth Camera Simulation\n\n```xml\n<gazebo reference=\"depth_camera_link\">\n  <sensor type=\"depth\" name=\"depth_camera_sensor\">\n    <update_rate>30.0</update_rate>\n    <camera name=\"depth_camera\">\n      <horizontal_fov>1.04719755</horizontal_fov> <!-- 60 degrees -->\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <depth_camera>\n        <clip>\n          <near>0.1</near>\n          <far>10.0</far>\n        </clip>\n      </depth_camera>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </camera>\n    <plugin name=\"depth_camera_controller\" filename=\"libgazebo_ros_openni_kinect.so\">\n      <baseline>0.2</baseline>\n      <always_on>true</always_on>\n      <update_rate>30.0</update_rate>\n      <camera_name>depth_camera</camera_name>\n      <image_topic_name>rgb/image_raw</image_topic_name>\n      <depth_image_topic_name>depth/image_raw</depth_image_topic_name>\n      <point_cloud_topic_name>depth/points</point_cloud_topic_name>\n      <camera_info_topic_name>rgb/camera_info</camera_info_topic_name>\n      <frame_name>depth_camera_optical_frame</frame_name>\n      <point_cloud_cropped>false</point_cloud_cropped>\n      <Cx>320.5</Cx>\n      <Cy>240.5</Cy>\n      <focal_length>320.0</focal_length>\n    </plugin>\n  </sensor>\n</gazebo>\n```",
    "chapter": 3,
    "section": "Depth Camera Simulation",
    "page": 10,
    "token_count": 391
  },
  {
    "chunk_id": "c6d4e055-cc50-4262-be5c-600c5e34d4e5",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Camera Simulation in Unity\n\nUnity provides advanced rendering capabilities for realistic camera simulation. Here's an example of a camera component setup:\n\n```csharp\nusing UnityEngine;\nusing System.Collections;",
    "chapter": 3,
    "section": "Camera Simulation in Unity",
    "page": 11,
    "token_count": 37
  },
  {
    "chunk_id": "1fc40dfd-a4b0-499b-85e5-9a02c9300d3e",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Camera Simulation in Unity\n\npublic class CameraSensor : MonoBehaviour\n{\n    [Header(\"Camera Settings\")]\n    public int imageWidth = 640;\n    public int imageHeight = 480;\n    public float fieldOfView = 60f;\n    public float nearClip = 0.1f;\n    public float farClip = 100f;\n    \n    [Header(\"Noise Settings\")]\n    public bool enableNoise = true;\n    public float noiseIntensity = 0.01f;\n    \n    private Camera cam;\n    private RenderTexture renderTexture;\n    \n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        SetupCamera();\n        CreateRenderTexture();\n    }\n    \n    void SetupCamera()\n    {\n        cam.fieldOfView = fieldOfView;\n        cam.nearClipPlane = nearClip;\n        cam.farClipPlane = farClip;\n    }\n    \n    void CreateRenderTexture()\n    {\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        cam.targetTexture = renderTexture;\n    }\n    \n    public Texture2D CaptureImage()\n    {\n        RenderTexture.active = renderTexture;\n        Texture2D image = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n        image.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        image.Apply();\n        \n        // Apply noise if enabled\n        if (enableNoise)\n        {\n            AddNoiseToImage(image);\n        }\n        \n        RenderTexture.active = null;\n        return image;\n    }\n    \n    void AddNoiseToImage(Texture2D image)\n    {\n        Color[] pixels = image.GetPixels();\n        \n        for (int i = 0; i < pixels.Length; i++)\n        {\n            float noise = Random.Range(-noiseIntensity, noiseIntensity);\n            pixels[i] = new Color(\n                Mathf.Clamp01(pixels[i].r + noise),\n                Mathf.Clamp01(pixels[i].g + noise),\n                Mathf.Clamp01(pixels[i].b + noise)\n            );\n        }\n        \n        image.SetPixels(pixels);\n        image.Apply();\n    }\n}\n```",
    "chapter": 3,
    "section": "Camera Simulation in Unity",
    "page": 12,
    "token_count": 441
  },
  {
    "chunk_id": "ed67e4b1-b5b5-45aa-b793-d154548aa0a5",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## LIDAR Sensor Simulation",
    "chapter": 3,
    "section": "LIDAR Sensor Simulation",
    "page": 13,
    "token_count": 6
  },
  {
    "chunk_id": "79e79a9b-f4f7-4240-84a8-980cd250a4f2",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### 2D LIDAR Simulation in Gazebo\n\n2D LIDAR sensors provide range measurements in a single plane, commonly used for 2D mapping and navigation:\n\n```xml\n<gazebo reference=\"laser_link\">\n  <sensor type=\"ray\" name=\"laser_sensor\">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>\n          <resolution>1.0</resolution>\n          <min_angle>-3.14159</min_angle>  <!-- -π radians -->\n          <max_angle>3.14159</max_angle>   <!-- π radians -->\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"laser_scan_publisher\" filename=\"libgazebo_ros_ray_sensor.so\">\n      <ros>\n        <namespace>/laser</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>laser_frame</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n```",
    "chapter": 3,
    "section": "2D LIDAR Simulation in Gazebo",
    "page": 14,
    "token_count": 289
  },
  {
    "chunk_id": "8bd22f5f-2444-4487-a8a2-4841af8ab9ad",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### 3D LIDAR Simulation\n\n3D LIDAR sensors provide full 3D point cloud data:",
    "chapter": 3,
    "section": "3D LIDAR Simulation",
    "page": 15,
    "token_count": 24
  },
  {
    "chunk_id": "a523389b-acab-4f4b-9eec-61ebbd3e7b37",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## 3D LIDAR Simulation\n\n```xml\n<gazebo reference=\"velodyne_link\">\n  <sensor type=\"ray\" name=\"velodyne_sensor\">\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>800</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>  <!-- -π -->\n          <max_angle>3.14159</max_angle>   <!-- π -->\n        </horizontal>\n        <vertical>\n          <samples>32</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.314159</min_angle> <!-- -18 degrees -->\n          <max_angle>0.244346</max_angle>  <!-- 14 degrees -->\n        </vertical>\n      </scan>\n      <range>\n        <min>0.2</min>\n        <max>100.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"velodyne_controller\" filename=\"libgazebo_ros_velodyne_laser.so\">\n      <topicName>points</topicName>\n      <frameName>velodyne_frame</frameName>\n      <min_range>0.2</min_range>\n      <max_range>100.0</max_range>\n      <gaussian_noise>0.008</gaussian_noise>\n    </plugin>\n  </sensor>\n</gazebo>\n```",
    "chapter": 3,
    "section": "3D LIDAR Simulation",
    "page": 16,
    "token_count": 319
  },
  {
    "chunk_id": "297d11a3-c255-412f-b7ff-aa6caf539fa3",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## IMU Sensor Simulation\n\nIMU sensors provide critical information about robot orientation and motion:",
    "chapter": 3,
    "section": "IMU Sensor Simulation",
    "page": 17,
    "token_count": 18
  },
  {
    "chunk_id": "9e55f502-3394-4688-bc74-73094b3669cf",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## IMU Sensor Simulation\n\n```xml\n<gazebo reference=\"imu_link\">\n  <sensor name=\"imu_sensor\" type=\"imu\">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name=\"imu_plugin\" filename=\"libgazebo_ros_imu.so\">\n      <bodyName>imu_link</bodyName>\n      <frameName>imu_link</frameName>\n      <topicName>imu</topicName>\n      <serviceName>imu_service</serviceName>\n      <gaussianNoise>0.0</gaussianNoise>\n      <updateRate>100.0</updateRate>\n    </plugin>\n  </sensor>\n</gazebo>\n```",
    "chapter": 3,
    "section": "IMU Sensor Simulation",
    "page": 18,
    "token_count": 610
  },
  {
    "chunk_id": "9538b923-05e9-447b-b2e2-5395574b1a79",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Force/Torque Sensor Simulation\n\nFor manipulation tasks, force/torque sensors are crucial:\n\n```xml\n<gazebo>\n  <plugin name=\"ft_sensor\" filename=\"libgazebo_ros_ft_sensor.so\">\n    <update_rate>100</update_rate>\n    <topic_name>wrench</topic_name>\n    <joint_name>wrist_joint</joint_name>\n  </plugin>\n</gazebo>\n```",
    "chapter": 3,
    "section": "Force/Torque Sensor Simulation",
    "page": 19,
    "token_count": 85
  },
  {
    "chunk_id": "cc82ce82-3810-421b-80bc-af9472557948",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Unity Sensor Implementation\n\nUnity offers advanced rendering capabilities for high-fidelity sensor simulation, particularly valuable for training AI systems:",
    "chapter": 3,
    "section": "Unity Sensor Implementation",
    "page": 20,
    "token_count": 24
  },
  {
    "chunk_id": "795c97e6-9f00-4fd7-9e8c-e29e93f84e99",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Unity Perception Package\n\nUnity's Perception package provides tools for generating synthetic sensor data:\n\n```csharp\nusing UnityEngine;\nusing Unity.Perception.GroundTruth;\nusing Unity.Simulation;\n\npublic class SensorSetup : MonoBehaviour\n{\n    void Start()\n    {\n        // Add semantic segmentation labels to objects\n        var labeler = gameObject.AddComponent<SemanticSegmentationLabeler>();\n        \n        // Configure camera for synthetic sensor data\n        var datasetCapture = gameObject.AddComponent<DatasetCapture>();\n        datasetCapture.captureOnPlay = true;\n        \n        // Add noise models to simulate real sensor characteristics\n        ConfigureSensorNoise();\n    }\n    \n    void ConfigureSensorNoise()\n    {\n        // Add realistic noise models that match physical sensors\n        // This includes quantization, blur, and other sensor-specific effects\n    }\n}\n```",
    "chapter": 3,
    "section": "Unity Perception Package",
    "page": 21,
    "token_count": 163
  },
  {
    "chunk_id": "169ca40d-8937-4865-b220-3bf81238c33d",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Sensor Integration with ROS 2",
    "chapter": 3,
    "section": "Sensor Integration with ROS 2",
    "page": 22,
    "token_count": 7
  },
  {
    "chunk_id": "687f2555-edbc-438d-9475-8298ba37a5ae",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Sensor Message Types\n\nROS 2 provides standardized message types for different sensor data:\n\n- `sensor_msgs/Image`: For camera images\n- `sensor_msgs/LaserScan`: For 2D LIDAR data\n- `sensor_msgs/PointCloud2`: For 3D point cloud data\n- `sensor_msgs/Imu`: For IMU data\n- `geometry_msgs/WrenchStamped`: For force/torque data",
    "chapter": 3,
    "section": "Sensor Message Types",
    "page": 23,
    "token_count": 86
  },
  {
    "chunk_id": "a2ca1eec-d337-482d-8dde-9a33673eac24",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Launch File Integration\n\nExample launch file for sensor simulation:\n\n```python\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Sensor processing nodes\n    camera_processor = Node(\n        package='image_proc',\n        executable='image_proc',\n        name='camera_processor'\n    )\n    \n    scan_processor = Node(\n        package='laser_filters',\n        executable='scan_to_scan_filter_chain',\n        name='scan_processor',\n        parameters=[\n            os.path.join(\n                get_package_share_directory('my_robot_description'),\n                'config',\n                'laser_filter.yaml'\n            )\n        ]\n    )\n    \n    imu_processor = Node(\n        package='imu_filter_madgwick',\n        executable='imu_filter_node',\n        name='imu_filter',\n        parameters=[{\n            'use_mag': False,\n            'publish_tf': False\n        }]\n    )\n    \n    return LaunchDescription([\n        camera_processor,\n        scan_processor,\n        imu_processor\n    ])\n```",
    "chapter": 3,
    "section": "Launch File Integration",
    "page": 24,
    "token_count": 218
  },
  {
    "chunk_id": "7564a5c4-bce8-46ad-8f87-8946eb4ccd97",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Best Practices for Sensor Simulation",
    "chapter": 3,
    "section": "Best Practices for Sensor Simulation",
    "page": 25,
    "token_count": 6
  },
  {
    "chunk_id": "813a4efb-0a50-489b-8e01-38931a1d3bb8",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Realism vs. Performance\n\nBalance simulation fidelity with computational requirements:\n- Use simplified physics models for distant objects\n- Implement level-of-detail systems for complex environments\n- Consider using faster algorithms during development",
    "chapter": 3,
    "section": "Realism vs. Performance",
    "page": 26,
    "token_count": 40
  },
  {
    "chunk_id": "2af998d8-c0af-44d4-bf35-84d12864f9c4",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Validation Approaches\n\n1. **Cross-validation**: Compare simulated sensor data with real sensor data\n2. **Statistical analysis**: Match noise characteristics and performance metrics\n3. **Functional testing**: Verify that control algorithms work equally well on both simulated and real data",
    "chapter": 3,
    "section": "Validation Approaches",
    "page": 27,
    "token_count": 53
  },
  {
    "chunk_id": "11c03c20-c68b-41b1-81eb-e9723c83fe08",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Noise Modeling\n\nInclude realistic noise models that match real sensors:\n- Gaussian noise for random measurement errors\n- Bias and drift for long-term sensor behavior\n- Environmental effects like lighting conditions for cameras",
    "chapter": 3,
    "section": "Noise Modeling",
    "page": 28,
    "token_count": 39
  },
  {
    "chunk_id": "d0b5bfd2-d0bf-4edc-a9a9-53704a49b57b",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Diagrams",
    "chapter": 3,
    "section": "Diagrams",
    "page": 29,
    "token_count": 3
  },
  {
    "chunk_id": "a63e83bc-3d54-44e7-8053-032ab04913c5",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Sensor Simulation Architecture",
    "chapter": 3,
    "section": "Sensor Simulation Architecture",
    "page": 30,
    "token_count": 4
  },
  {
    "chunk_id": "712b5f6a-9247-4b38-9e3c-2fb7e76f4f8c",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Sensor Simulation Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                   SENSOR SIMULATION ARCHITECTURE                │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │   PHYSICAL  │    │  SIMULATION │    │   ROS 2     │         │\n│  │   SENSORS   │    │   ENGINE    │    │  MESSAGES   │         │\n│  │             │    │             │    │             │         │\n│  │ • Cameras   │    │ • Physics   │    │ • sensor_msgs│         │\n│  │ • LIDAR     │◄──►│ • Lighting  │◄──►│ • geometry  │         │\n│  │ • IMU       │    │ • Materials │    │ • nav_msgs  │         │\n│  │ • Force/Torque│  │ • Noise     │    │             │         │\n│  └─────────────┘    │   Models    │    └─────────────┘         │\n│                     └─────────────┘                            │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                 GAZEBO PLUGINS                          │   │\n│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐      │   │\n│  │  │ Camera      │ │ LIDAR       │ │ IMU         │      │   │\n│  │  │ Plugin      │ │ Plugin      │ │ Plugin      │      │   │\n│  │  └─────────────┘ └─────────────┘ └─────────────┘      │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 3,
    "section": "Sensor Simulation Architecture",
    "page": 31,
    "token_count": 451
  },
  {
    "chunk_id": "25831899-0d82-449e-8f0b-e3cf42a11013",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Sensor Fusion in Simulation\n\n```\nIndividual Sensors ──────────────▶ Fused Perception\n     │                                    │\n     ├─ Camera: RGB & Depth               │\n     │  (Object Recognition, Depth)       ├─ 3D Object Detection\n     │                                    │   (Position, Class, Velocity)\n     ├─ LIDAR: Point Cloud              │\n     │  (Environment Mapping, Obstacles)  │\n     │                                    │\n     ├─ IMU: Orientation & Acceleration   │\n     │  (Pose Estimation, Motion)        │\n     │                                    │\n     └─ GPS: Position                     │\n        (Global Positioning)             │\n                                         │\n              ┌─────────────────┐        │\n              │   Ground Truth  │        │\n              │   (Simulation)  │        │\n              │   Available!    │        │\n              └─────────────────┘        │\n                                         ▼\n                              Integrated Robot Perception\n                              (Safe Navigation, Task Execution)\n```",
    "chapter": 3,
    "section": "Sensor Fusion in Simulation",
    "page": 32,
    "token_count": 221
  },
  {
    "chunk_id": "4bfca31e-de83-4ad1-ae52-24ef53ae0049",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Implement realistic camera sensor simulation in Gazebo and Unity\n2. Configure LIDAR sensors with appropriate noise models and parameters\n3. Simulate IMU and force/torque sensors with realistic characteristics\n4. Integrate simulated sensors with ROS 2 message publishing\n5. Validate sensor simulation fidelity against real hardware",
    "chapter": 3,
    "section": "Learning Outcomes",
    "page": 33,
    "token_count": 81
  },
  {
    "chunk_id": "ecd0254b-625e-4a5e-92d3-3ee93d291741",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Advanced Topics",
    "chapter": 3,
    "section": "Advanced Topics",
    "page": 34,
    "token_count": 3
  },
  {
    "chunk_id": "6a576dfd-8704-488f-b46a-464cd2683c8c",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### High-Fidelity Sensor Simulation\n\n- Physics-based rendering for photorealistic camera simulation\n- Advanced noise modeling including temperature and wear effects\n- Multi-sensor fusion simulation for enhanced perception",
    "chapter": 3,
    "section": "High-Fidelity Sensor Simulation",
    "page": 35,
    "token_count": 37
  },
  {
    "chunk_id": "5eb2cd57-f536-40e0-b400-e7d203f5b0c7",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Domain Randomization\n\n- Randomizing visual properties to improve transfer learning\n- Varying environmental conditions during training\n- Stochastic sensor parameter adjustment",
    "chapter": 3,
    "section": "Domain Randomization",
    "page": 36,
    "token_count": 30
  },
  {
    "chunk_id": "b0213402-16d0-4e44-83a1-59e725e4ba1e",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "### Sensor Placement Optimization\n\n- Strategic sensor placement for maximum environment coverage\n- Redundancy planning for fault tolerance\n- Field-of-view analysis for optimal positioning",
    "chapter": 3,
    "section": "Sensor Placement Optimization",
    "page": 37,
    "token_count": 31
  },
  {
    "chunk_id": "dcfb6283-95a9-42c3-9199-deccc13fc219",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## Further Reading\n\n- \"Realistic Sensor Simulation for Robotics\" (IEEE Robotics & Automation Magazine, 2024) [1]\n- \"Gazebo Sensor Plugins Development Guide\" (Open Robotics, 2024) [2]\n- \"Unity Perception Package for Synthetic Data Generation\" (Unity Technologies, 2024) [3]\n\n---",
    "chapter": 3,
    "section": "Further Reading",
    "page": 38,
    "token_count": 70
  },
  {
    "chunk_id": "b8fe1565-b83a-4289-a631-bdf03600ed05",
    "doc_id": "368ece06-5d43-4749-af6a-5254ac3929c6",
    "chunk_text": "## References\n\n[1] Johnson, A., et al. \"Realistic Sensor Simulation in Robotics: Balancing Fidelity and Performance.\" IEEE Robotics & Automation Magazine, vol. 31, no. 2, pp. 45-58, 2024.\n\n[2] Open Robotics. \"Gazebo Sensor Plugins Development Guide.\" Gazebo Documentation. 2024. https://classic.gazebosim.org/tutorials?tut=ros2_libraries\n\n[3] Unity Technologies. \"Unity Perception Package for Synthetic Data Generation.\" Unity Developer Documentation. 2024. https://docs.unity3d.com/Packages/com.unity.perception@latest",
    "chapter": 3,
    "section": "References",
    "page": 39,
    "token_count": 138
  },
  {
    "chunk_id": "6ec18a2d-f18c-4ad6-b7aa-2498d140849e",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "# Unity for High-Fidelity Visualization",
    "chapter": 3,
    "section": "Introduction",
    "page": 1,
    "token_count": 7
  },
  {
    "chunk_id": "9674b8a6-df61-4f5a-a0a0-4e4292107846",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Overview\n\nUnity is a powerful real-time 3D development platform that has emerged as a leading solution for creating high-fidelity digital twins in robotics. With its advanced rendering capabilities, physics simulation, and extensive asset ecosystem, Unity provides an ideal environment for creating photorealistic simulations that can be used for training AI systems, validating robotic algorithms, and creating compelling visualizations. Unity's integration with robotics frameworks like ROS 2 makes it an essential tool for the modern robotics development pipeline.\n\nUnity's strength lies in its ability to create highly realistic visual environments with sophisticated lighting, materials, and special effects that closely match real-world conditions. This photorealism is particularly important for training computer vision systems and other AI components that rely heavily on visual input. The platform's real-time rendering capabilities also make it suitable for interactive simulation and human-in-the-loop testing scenarios.",
    "chapter": 3,
    "section": "Overview",
    "page": 2,
    "token_count": 171
  },
  {
    "chunk_id": "f43d1d3b-82f7-40e7-b204-6299d573efa3",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Key Concepts",
    "chapter": 3,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "e8b38802-a8c9-4ed5-bb3b-17eac14c476d",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Unity for Robotics Architecture\n\nUnity's architecture for robotics includes several key components:\n\n**Unity Engine**: The core 3D engine that handles rendering, physics, input, and other core functions.\n\n**ROS# Bridge**: A communication layer that enables Unity to communicate with ROS 2 networks, allowing virtual sensors to publish to ROS topics and virtual actuators to receive commands.\n\n**Perception Package**: Unity's specialized toolkit for generating synthetic sensor data including camera images, depth maps, segmentation data, and point clouds.\n\n**Simulation Framework**: Tools and components specifically designed for robotics simulation, including robot models, environment assets, and physics configurations.",
    "chapter": 3,
    "section": "Unity for Robotics Architecture",
    "page": 4,
    "token_count": 126
  },
  {
    "chunk_id": "39f4bdfd-bc69-44a4-8843-541e67d66408",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Photorealistic Rendering Benefits\n\nUnity's high-quality rendering provides several advantages for robotics simulation:\n\n**Visual Fidelity**: Photorealistic environments help ensure that vision-based AI systems trained in simulation will perform well in real-world conditions.\n\n**Lighting Simulation**: Advanced lighting models accurately simulate real-world lighting conditions and their effect on sensor data.\n\n**Material Properties**: Realistic material rendering helps simulate how different surface properties affect sensor performance.\n\n**Atmospheric Effects**: Simulation of environmental conditions like fog, rain, or dust that affect sensor performance.",
    "chapter": 3,
    "section": "Photorealistic Rendering Benefits",
    "page": 5,
    "token_count": 107
  },
  {
    "chunk_id": "675eaec0-c8b2-44c3-8e22-fdc130a87e3b",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Unity vs. Gazebo Comparison\n\nWhile Gazebo excels at physics accuracy and real-time simulation performance, Unity offers superior visual quality and development tools:\n\n| Feature | Unity | Gazebo |\n|---------|-------|--------|\n| Visual Quality | Excellent (photorealistic) | Good (functional) |\n| Physics Accuracy | Good (game-oriented) | Excellent (scientific) |\n| Development Tools | Comprehensive IDE | Command-line focused |\n| Asset Library | Extensive marketplace | Limited model database |\n| Rendering Effects | Advanced PBR, lighting | Basic rendering |\n| AI Training | High-fidelity synthetic data | Standard sensor simulation |",
    "chapter": 3,
    "section": "Unity vs. Gazebo Comparison",
    "page": 6,
    "token_count": 130
  },
  {
    "chunk_id": "3a0652cd-fbfa-4b22-8477-6ca5755b1782",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Setting Up Unity for Robotics",
    "chapter": 3,
    "section": "Setting Up Unity for Robotics",
    "page": 7,
    "token_count": 6
  },
  {
    "chunk_id": "69e02bae-dec7-4b0e-8980-a6ce4ded6c24",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Unity Installation and Requirements\n\nTo set up Unity for robotics applications, you'll need a Unity Hub account and one of these Unity versions:\n- Unity 2021.3 LTS (recommended for stability)\n- Unity 2022.3 LTS (recommended for new projects)\n- Unity 2023.2 or later (latest features)\n\n**System Requirements**:\n- **OS**: Windows 10/11 (64-bit), macOS 10.15+, Ubuntu 20.04+\n- **CPU**: Intel i5/Ryzen 5 or better\n- **RAM**: 8GB minimum, 16GB+ recommended\n- **GPU**: DirectX 10 compatible with 1GB+ VRAM\n- **Storage**: 30GB+ for Unity installation and projects",
    "chapter": 3,
    "section": "Unity Installation and Requirements",
    "page": 8,
    "token_count": 157
  },
  {
    "chunk_id": "1936ecf0-5194-44ff-bc4b-64b8478e7b6e",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Installing Unity Robotics Tools\n\n1. **Unity Perception Package**: Adds tools for generating synthetic sensor data\n2. **ROS# Package**: Provides ROS 2 communication bridge\n3. **Oculus XR Package**: If using VR for immersive simulation\n4. **Bolt Visualization**: For creating custom UI and visualization tools",
    "chapter": 3,
    "section": "Installing Unity Robotics Tools",
    "page": 9,
    "token_count": 64
  },
  {
    "chunk_id": "a96fe267-6a92-48c5-a9f6-719600ed7054",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### ROS# Setup for Unity\n\nROS# is the primary bridge between Unity and ROS 2. Here's how to set it up:\n\n**Step 1**: Install ROS# from the Unity Asset Store or GitHub\n**Step 2**: Configure network settings in Unity\n**Step 3**: Set up message publishers/subscribers\n**Step 4**: Test communication with ROS 2 nodes",
    "chapter": 3,
    "section": "ROS# Setup for Unity",
    "page": 10,
    "token_count": 79
  },
  {
    "chunk_id": "7392dd83-18cb-4abc-b4ed-64baf4f4450e",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Unity Robotics Integration",
    "chapter": 3,
    "section": "Unity Robotics Integration",
    "page": 11,
    "token_count": 4
  },
  {
    "chunk_id": "34b8024e-b8f5-4d27-a958-f440d7156ad5",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Basic ROS# Implementation\n\nHere's an example of how to create a simple ROS publisher in Unity:\n\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\nusing Unity.Robotics.ROSTCPConnector.MessageGeneration;",
    "chapter": 3,
    "section": "Basic ROS# Implementation",
    "page": 12,
    "token_count": 59
  },
  {
    "chunk_id": "d2330c35-5547-442c-915d-0ccd478e267b",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Basic ROS# Implementation\n\npublic class UnityCameraPublisher : MonoBehaviour\n{\n    [Header(\"ROS Settings\")]\n    public string topicName = \"/unity_camera/image_raw\";\n    public string frameId = \"unity_camera_optical_frame\";\n    \n    [Header(\"Camera Settings\")]\n    public Camera cameraComponent;\n    public int imageWidth = 640;\n    public int imageHeight = 480;\n    private RenderTexture renderTexture;\n    private ROSConnection ros;\n    \n    void Start()\n    {\n        // Get reference to ROS TCP Connector\n        ros = ROSConnection.instance;\n        \n        // Create render texture for camera capture\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        cameraComponent.targetTexture = renderTexture;\n        \n        // Start publishing loop\n        InvokeRepeating(\"PublishCameraData\", 0.0f, 0.1f); // 10Hz publishing\n    }\n    \n    void PublishCameraData()\n    {\n        // Capture image from camera\n        RenderTexture.active = renderTexture;\n        Texture2D image = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n        image.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        image.Apply();\n        \n        // Convert to ROS message\n        var msg = new SensorMsgs.Image();\n        msg.header.stamp = new TimeStamp(Time.time);\n        msg.header.frame_id = frameId;\n        msg.height = (uint)imageHeight;\n        msg.width = (uint)imageWidth;\n        msg.encoding = \"rgb8\";\n        msg.is_bigendian = 0;\n        msg.step = (uint)(imageWidth * 3); // 3 bytes per pixel (RGB)\n        \n        // Convert image data to bytes\n        byte[] bytes = image.EncodeToPNG();\n        msg.data = bytes;\n        \n        // Publish message\n        ros.Publish(topicName, msg);\n        \n        // Clean up\n        DestroyImmediate(image);\n    }\n}\n```",
    "chapter": 3,
    "section": "Basic ROS# Implementation",
    "page": 13,
    "token_count": 413
  },
  {
    "chunk_id": "3e66695e-67a6-4b79-85f3-ff4c2dca3364",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Unity Perception Package Usage\n\nThe Unity Perception package enables synthetic data generation:\n\n```csharp\nusing UnityEngine;\nusing Unity.Perception.GroundTruth;\nusing Unity.Perception.GroundTruth.DataModel;\n\npublic class SensorSetup : MonoBehaviour\n{\n    [Header(\"Camera Configuration\")]\n    public Camera cameraComponent;\n    public int captureFrequency = 10; // Hz\n    private int frameCounter = 0;\n    \n    [Header(\"Sensor Labels\")]\n    public string sensorId = \"main_camera\";\n    public bool captureRgb = true;\n    public bool captureDepth = true;\n    public bool captureSegmentation = true;\n    \n    void Start()\n    {\n        ConfigureCameraSensor();\n    }\n    \n    void ConfigureCameraSensor()\n    {\n        // Add perception camera\n        var perceptionCamera = cameraComponent.gameObject.AddComponent<PerceptionCamera>();\n        perceptionCamera.captureRgbImages = captureRgb;\n        perceptionCamera.captureDepthImages = captureDepth;\n        perceptionCamera.captureSegmentationImages = captureSegmentation;\n        perceptionCamera.frequency = captureFrequency;\n    }\n    \n    void Update()\n    {\n        frameCounter++;\n        \n        // Additional sensor processing can happen here\n        ProcessSensorData();\n    }\n    \n    void ProcessSensorData()\n    {\n        // Process and publish sensor data\n        // This could include custom noise models or processing pipelines\n    }\n}\n```",
    "chapter": 3,
    "section": "Unity Perception Package Usage",
    "page": 14,
    "token_count": 271
  },
  {
    "chunk_id": "b9d3ac4d-df0f-44d5-bc23-68727b33898d",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Creating High-Fidelity Environments",
    "chapter": 3,
    "section": "Creating High-Fidelity Environments",
    "page": 15,
    "token_count": 7
  },
  {
    "chunk_id": "ac79e8b5-ae32-432d-baef-e613bfd61c74",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Material and Lighting Setup\n\nCreating photorealistic environments requires careful attention to materials and lighting:\n\n**High Dynamic Range (HDR) Lighting**:\n```csharp\nusing UnityEngine;\n\npublic class HDRLightingSetup : MonoBehaviour\n{\n    [Header(\"HDR Skybox\")]\n    public Material hdrSkybox;\n    \n    [Header(\"Light Sources\")]\n    public Light mainSunLight;\n    public float exposure = 1.0f;\n    public float gamma = 1.0f;\n    \n    void Start()\n    {\n        SetupHDR();\n    }\n    \n    void SetupHDR()\n    {\n        // Apply HDR skybox\n        RenderSettings.skybox = hdrSkybox;\n        \n        // Configure main directional light (sun)\n        mainSunLight.type = LightType.Directional;\n        mainSunLight.intensity = 1.0f;\n        mainSunLight.color = Color.white;\n        \n        // Configure rendering settings for HDR\n        QualitySettings.activeColorSpace = ColorSpace.Linear;\n    }\n}\n```",
    "chapter": 3,
    "section": "Material and Lighting Setup",
    "page": 16,
    "token_count": 203
  },
  {
    "chunk_id": "7b3201a3-b396-4a36-8e3f-567dd281a180",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Procedural Environment Generation\n\nUnity allows for procedural generation of complex environments:\n\n```csharp\nusing UnityEngine;\nusing System.Collections.Generic;",
    "chapter": 3,
    "section": "Procedural Environment Generation",
    "page": 17,
    "token_count": 28
  },
  {
    "chunk_id": "3742e715-af10-4b3c-a6b6-f9e70795fd4f",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Procedural Environment Generation\n\npublic class ProceduralEnvironment : MonoBehaviour\n{\n    [Header(\"Terrain Generation\")]\n    public int terrainWidth = 200;\n    public int terrainHeight = 200;\n    public float terrainScale = 20.0f;\n    \n    [Header(\"Building Parameters\")]\n    public GameObject buildingPrefab;\n    public int buildingCount = 50;\n    public float minBuildingSize = 5.0f;\n    public float maxBuildingSize = 15.0f;\n    \n    [Header(\"Obstacle Parameters\")]\n    public GameObject[] obstaclePrefabs;\n    public int obstacleCount = 100;\n    \n    void Start()\n    {\n        GenerateTerrain();\n        SpawnBuildings();\n        SpawnObstacles();\n    }\n    \n    void GenerateTerrain()\n    {\n        // Create a basic terrain using Unity's terrain system\n        Terrain terrain = Terrain.activeTerrain;\n        \n        if (terrain == null)\n        {\n            // Create new terrain if none exists\n            GameObject terrainObj = new GameObject(\"GeneratedTerrain\");\n            terrain = terrainObj.AddComponent<Terrain>();\n            TerrainCollider tc = terrainObj.AddComponent<TerrainCollider>();\n            \n            terrain.terrainData = new TerrainData();\n            terrain.terrainData.size = new Vector3(terrainWidth, 20, terrainHeight);\n            \n            // Generate heightmap\n            float[,] heights = new float[terrainWidth, terrainHeight];\n            for (int x = 0; x < terrainWidth; x++)\n            {\n                for (int z = 0; z < terrainHeight; z++)\n                {\n                    heights[x, z] = Mathf.PerlinNoise(\n                        x / terrainScale, \n                        z / terrainScale\n                    ) * 0.5f; // Height variation\n                }\n            }\n            \n            terrain.terrainData.SetHeights(0, 0, heights);\n        }\n    }\n    \n    void SpawnBuildings()\n    {\n        for (int i = 0; i < buildingCount; i++)\n        {\n            Vector3 position = new Vector3(\n                Random.Range(0, terrainWidth),\n                0,\n                Random.Range(0, terrainHeight)\n            );\n            \n            GameObject building = Instantiate(buildingPrefab, position, Quaternion.identity);\n            \n            // Randomize building size\n            float size = Random.Range(minBuildingSize, maxBuildingSize);\n            building.transform.localScale = new Vector3(size, size * Random.Range(1.5f, 3.0f), size);\n        }\n    }\n    \n    void SpawnObstacles()\n    {\n        for (int i = 0; i < obstacleCount; i++)\n        {\n            if (obstaclePrefabs.Length > 0)\n            {\n                GameObject prefab = obstaclePrefabs[Random.Range(0, obstaclePrefabs.Length)];\n                \n                Vector3 position = new Vector3(\n                    Random.Range(0, terrainWidth),\n                    0,\n                    Random.Range(0, terrainHeight)\n                );\n                \n                Instantiate(prefab, position, Quaternion.identity);\n            }\n        }\n    }\n}\n```",
    "chapter": 3,
    "section": "Procedural Environment Generation",
    "page": 18,
    "token_count": 625
  },
  {
    "chunk_id": "1928d724-929c-4cf2-a91a-09327f128381",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Advanced Unity Features for Robotics",
    "chapter": 3,
    "section": "Advanced Unity Features for Robotics",
    "page": 19,
    "token_count": 6
  },
  {
    "chunk_id": "582bf0ca-0c3f-48c6-bcc2-e5e65bb54a68",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Physics Simulation\n\nUnity's physics engine can simulate various physical properties:\n\n```csharp\nusing UnityEngine;\n\npublic class AdvancedPhysicsSetup : MonoBehaviour\n{\n    [Header(\"Physics Materials\")]\n    public PhysicMaterial defaultMaterial;\n    public PhysicMaterial lowFrictionMaterial;\n    public PhysicMaterial highFrictionMaterial;\n    \n    [Header(\"Joint Configuration\")]\n    public ConfigurableJoint jointComponent;\n    public float jointSpring = 10000f;\n    public float jointDamping = 1000f;\n    \n    void Start()\n    {\n        ConfigurePhysics();\n    }\n    \n    void ConfigurePhysics()\n    {\n        // Set up joints with realistic properties\n        if (jointComponent != null)\n        {\n            // Positional drive for precise control\n            jointComponent.solverPositionDrive = jointSpring;\n            jointComponent.solverPositionDrive = jointDamping;\n            \n            // Set joint limits based on real robot specifications\n            SoftJointLimit lowLimit = new SoftJointLimit();\n            lowLimit.limit = -45f * Mathf.Deg2Rad; // Convert to radians\n            jointComponent.lowAngularXLimit = lowLimit;\n            \n            SoftJointLimit highLimit = new SoftJointLimit();\n            highLimit.limit = 45f * Mathf.Deg2Rad;\n            jointComponent.highAngularXLimit = highLimit;\n        }\n    }\n}\n```",
    "chapter": 3,
    "section": "Physics Simulation",
    "page": 20,
    "token_count": 276
  },
  {
    "chunk_id": "ad64a1f0-371a-41be-afbe-527f6ec986d9",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Custom Sensor Simulation\n\nUnity allows for custom sensor implementations:\n\n```csharp\nusing UnityEngine;\nusing System.Collections.Generic;",
    "chapter": 3,
    "section": "Custom Sensor Simulation",
    "page": 21,
    "token_count": 24
  },
  {
    "chunk_id": "1ac228da-df2b-47c0-ae99-c0b7a1e34ee1",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Custom Sensor Simulation\n\npublic class CustomLidarSimulation : MonoBehaviour\n{\n    [Header(\"LIDAR Configuration\")]\n    public int rayCount = 360;\n    public float scanRange = 20.0f;\n    public float fieldOfView = 360.0f;\n    public LayerMask detectionMask;\n    \n    [Header(\"Noise Parameters\")]\n    public float noiseStdDev = 0.01f;\n    public float biasError = 0.0f;\n    \n    private List<float> ranges;\n    private List<Vector3> rayDirections;\n    \n    void Start()\n    {\n        InitializeLidar();\n    }\n    \n    void InitializeLidar()\n    {\n        ranges = new List<float>(rayCount);\n        rayDirections = new List<Vector3>(rayCount);\n        \n        // Pre-calculate ray directions\n        for (int i = 0; i < rayCount; i++)\n        {\n            float angle = (fieldOfView / rayCount) * i * Mathf.Deg2Rad;\n            Vector3 direction = new Vector3(\n                Mathf.Cos(angle),\n                0,\n                Mathf.Sin(angle)\n            ).normalized;\n            \n            rayDirections.Add(direction);\n        }\n    }\n    \n    void Update()\n    {\n        SimulateLidarScan();\n    }\n    \n    void SimulateLidarScan()\n    {\n        ranges.Clear();\n        \n        for (int i = 0; i < rayCount; i++)\n        {\n            Vector3 direction = transform.TransformDirection(rayDirections[i]);\n            RaycastHit hit;\n            \n            if (Physics.Raycast(transform.position, direction, out hit, scanRange, detectionMask))\n            {\n                float distance = hit.distance;\n                // Add noise to the measurement\n                distance += Random.Range(-noiseStdDev, noiseStdDev) + biasError;\n                ranges.Add(distance);\n            }\n            else\n            {\n                ranges.Add(scanRange); // Max range if nothing detected\n            }\n        }\n        \n        // Publish ranges via ROS or use internally\n        ProcessLidarData();\n    }\n    \n    void ProcessLidarData()\n    {\n        // Process the simulated LIDAR data\n        // This could involve publishing to ROS topics\n    }\n    \n    // Visualization for debugging\n    void OnDrawGizmosSelected()\n    {\n        if (rayDirections != null && ranges != null && rayDirections.Count == ranges.Count)\n        {\n            for (int i = 0; i < rayDirections.Count; i++)\n            {\n                Vector3 direction = transform.TransformDirection(rayDirections[i]);\n                Gizmos.color = ranges[i] < scanRange ? Color.red : Color.green;\n                Gizmos.DrawRay(transform.position, direction * ranges[i]);\n            }\n        }\n    }\n}\n```",
    "chapter": 3,
    "section": "Custom Sensor Simulation",
    "page": 22,
    "token_count": 556
  },
  {
    "chunk_id": "004b5fa7-1a1f-4bc2-8e09-565ffa427a49",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Integration with External Systems",
    "chapter": 3,
    "section": "Integration with External Systems",
    "page": 23,
    "token_count": 5
  },
  {
    "chunk_id": "0cf8c7bb-e3e7-4287-aded-d41ca8b4a2d2",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### ROS 2 Bridge Configuration\n\nConfiguring the ROS 2 bridge for Unity requires careful attention to network settings:\n\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class RosBridgeConfig : MonoBehaviour\n{\n    [Header(\"Network Configuration\")]\n    public string rosIpAddress = \"127.0.0.1\";\n    public int rosPort = 10000;\n    \n    [Header(\"Connection Settings\")]\n    public bool autoConnect = true;\n    public float connectionTimeout = 10.0f;\n    \n    private ROSConnection rosConnection;\n    \n    void Start()\n    {\n        if (autoConnect)\n        {\n            ConnectToRosBridge();\n        }\n    }\n    \n    public void ConnectToRosBridge()\n    {\n        // Initialize ROS connection with custom settings\n        rosConnection = ROSConnection.instance;\n        \n        // Set IP and port\n        rosConnection.Initialize(rosIpAddress, rosPort);\n        \n        Debug.Log($\"Connecting to ROS bridge at {rosIpAddress}:{rosPort}\");\n    }\n    \n    public void Disconnect()\n    {\n        if (rosConnection != null)\n        {\n            rosConnection.Close();\n        }\n    }\n}\n```",
    "chapter": 3,
    "section": "ROS 2 Bridge Configuration",
    "page": 24,
    "token_count": 238
  },
  {
    "chunk_id": "d3f11493-652d-4d70-8621-da8406cc6eef",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Unity-ROS 2 Message Types\n\nUnity can work with custom ROS 2 message types:\n\n```csharp\nusing System;\nusing Unity.Robotics.ROSTCPConnector.MessageGeneration;\n\n// Example of a custom message for Unity-specific robot control\n[Serializable]\npublic class UnityRobotControl : Message\n{\n    public const string k_RosMessageName = \"unity_robot_msgs/RobotControl\";\n    public override string RosMessageName => k_RosMessageName;\n\npublic string robot_name;\n    public float[] joint_positions;\n    public float[] joint_velocities;\n    public float[] joint_efforts;\n    public RosMessageTypes.Std.Header header;\n\npublic UnityRobotControl()\n    {\n        header = new RosMessageTypes.Std.Header();\n        robot_name = \"\";\n        joint_positions = new float[0];\n        joint_velocities = new float[0];\n        joint_efforts = new float[0];\n    }",
    "chapter": 3,
    "section": "Unity-ROS 2 Message Types",
    "page": 25,
    "token_count": 186
  },
  {
    "chunk_id": "05b6e38d-9c02-45e5-bd2c-9efb12030ed3",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Unity-ROS 2 Message Types\n\npublic UnityRobotControl(RosMessageTypes.Std.Header header, string robot_name, float[] joint_positions, float[] joint_velocities, float[] joint_efforts) : base()\n    {\n        this.header = header;\n        this.robot_name = robot_name;\n        this.joint_positions = joint_positions;\n        this.joint_velocities = joint_velocities;\n        this.joint_efforts = joint_efforts;\n    }\n    \n    public static UnityRobotControl Deserialize(byte[] data)\n    {\n        var msg = new UnityRobotControl();\n        var offset = 0;\n        msg.header = RosMessageTypes.Std.Header.Deserialize(data, ref offset);\n        msg.robot_name = Serialization.DeserializeString(data, ref offset);\n        msg.joint_positions = Serialization.DeserializeFloatArray(data, ref offset);\n        msg.joint_velocities = Serialization.DeserializeFloatArray(data, ref offset);\n        msg.joint_efforts = Serialization.DeserializeFloatArray(data, ref offset);\n        return msg;\n    }\n}\n```",
    "chapter": 3,
    "section": "Unity-ROS 2 Message Types",
    "page": 26,
    "token_count": 207
  },
  {
    "chunk_id": "17b2fa9e-c797-4733-81e5-aafec6fe4ddc",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Best Practices",
    "chapter": 3,
    "section": "Best Practices",
    "page": 27,
    "token_count": 3
  },
  {
    "chunk_id": "59760a7b-cefb-4d22-adc5-da588ab7810b",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Performance Optimization\n\n1. **LOD Systems**: Use Level of Detail to reduce rendering complexity for distant objects\n2. **Occlusion Culling**: Hide objects not visible to the camera\n3. **Baking Lighting**: Pre-compute static lighting for better performance\n4. **Texture Compression**: Use appropriate texture formats for your target platform",
    "chapter": 3,
    "section": "Performance Optimization",
    "page": 28,
    "token_count": 70
  },
  {
    "chunk_id": "e4c5f6bd-7a69-4e48-b1bb-fac92d491d61",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Quality Considerations\n\n1. **Visual Fidelity**: Match the visual quality to your use case requirements\n2. **Physical Accuracy**: Balance visual quality with realistic physics\n3. **Sensor Matching**: Ensure synthetic sensor data matches real sensor characteristics\n4. **Environmental Conditions**: Include realistic environmental variations",
    "chapter": 3,
    "section": "Quality Considerations",
    "page": 29,
    "token_count": 60
  },
  {
    "chunk_id": "0f289782-9227-401b-be78-a5ba617b53f7",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Validation Approaches\n\n1. **Photorealistic Validation**: Compare synthetic images with real camera images\n2. **Sensor Accuracy**: Validate simulated sensor readings against real sensors\n3. **Behavior Comparison**: Ensure robot behaviors are consistent between sim and reality",
    "chapter": 3,
    "section": "Validation Approaches",
    "page": 30,
    "token_count": 50
  },
  {
    "chunk_id": "bbd7c0ea-67f9-416c-9541-0f8443796a7d",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Diagrams",
    "chapter": 3,
    "section": "Diagrams",
    "page": 31,
    "token_count": 3
  },
  {
    "chunk_id": "1540deba-a55e-489c-a4c9-dffd6956eeb5",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Unity Robotics Architecture",
    "chapter": 3,
    "section": "Unity Robotics Architecture",
    "page": 32,
    "token_count": 4
  },
  {
    "chunk_id": "40b61429-0850-4bff-832e-f67deb987d36",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Unity Robotics Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    UNITY ROBOTICS ARCHITECTURE                  │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │  UNITY      │    │  ROS#       │    │  ROS 2      │         │\n│  │  ENGINE     │    │  BRIDGE     │    │  NETWORK    │         │\n│  │             │    │             │    │             │         │\n│  │ • Rendering │◄──►│ • Message   │◄──►│ • sensor_msgs│         │\n│  │ • Physics   │    │   Bridge    │    │ • geometry │         │\n│  │ • Audio     │    │ • TCP/IP    │    │ • nav_msgs  │         │\n│  │ • Input     │    │ • Protocols │    │             │         │\n│  └─────────────┘    └─────────────┘    └─────────────┘         │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                UNITY PERCEPTION PACKAGE                 │   │\n│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐      │   │\n│  │  │ Camera      │ │ Semantic    │ │ Synthetic   │      │   │\n│  │  │ Simulation  │ │ Segmentation│ │ Data Gen    │      │   │\n│  │  └─────────────┘ └─────────────┘ └─────────────┘      │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 3,
    "section": "Unity Robotics Architecture",
    "page": 33,
    "token_count": 433
  },
  {
    "chunk_id": "ef0506af-b0e8-43c2-bb3a-5ee0c9e8a19a",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Simulation Workflow in Unity\n\n```\nDesign Environment ──▶ Create Assets ──▶ Configure Sensors ──▶ Test in Unity\n      │                   │                  │                   │\n      ▼                   ▼                  ▼                   ▼\nCreate Materials ←── Import Models ←── Add Sensors ←─────── Validate\n& Lighting         & Textures         & Scripts            Performance\n```",
    "chapter": 3,
    "section": "Simulation Workflow in Unity",
    "page": 34,
    "token_count": 84
  },
  {
    "chunk_id": "79dea834-8e24-4d58-892b-7f6547ef81e9",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Set up Unity for robotics applications with ROS 2 integration\n2. Create photorealistic environments suitable for AI training\n3. Implement custom sensor simulation in Unity\n4. Configure Unity Perception for synthetic data generation\n5. Establish communication between Unity and ROS 2 networks",
    "chapter": 3,
    "section": "Learning Outcomes",
    "page": 35,
    "token_count": 72
  },
  {
    "chunk_id": "4a9a1d24-03c8-4af1-b3db-2a5ae809271f",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Advanced Topics",
    "chapter": 3,
    "section": "Advanced Topics",
    "page": 36,
    "token_count": 3
  },
  {
    "chunk_id": "ea05a7dd-5965-4062-ba60-b21b8a21d638",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### NVIDIA Isaac Integration\n\nUnity can integrate with NVIDIA Isaac for advanced AI training:\n- Isaac Unity plugin for enhanced robotics simulation\n- TensorRT integration for optimized AI inference\n- PhysX physics engine for realistic simulation",
    "chapter": 3,
    "section": "NVIDIA Isaac Integration",
    "page": 37,
    "token_count": 42
  },
  {
    "chunk_id": "db3ff629-6aca-4080-a6f8-33dee4deb1e9",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Virtual Reality Integration\n\nUnity supports VR for immersive simulation experiences:\n- Testing in virtual reality environments\n- Human-in-the-loop simulation\n- Collaborative design and testing",
    "chapter": 3,
    "section": "Virtual Reality Integration",
    "page": 38,
    "token_count": 33
  },
  {
    "chunk_id": "67fe0b13-5700-4f34-9ef7-83d85eccb065",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "### Cloud-Based Simulation\n\nUnity can be deployed on cloud platforms for scalable simulation:\n- AWS RoboMaker integration\n- Google Cloud simulation clusters\n- Docker-based Unity deployments",
    "chapter": 3,
    "section": "Cloud-Based Simulation",
    "page": 39,
    "token_count": 34
  },
  {
    "chunk_id": "7a3d64d6-b9f5-49d2-9d62-d35a2e266463",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## Further Reading\n\n- \"Unity in Robotics: A Comprehensive Guide\" (IEEE Robotics & Automation Magazine, 2024) [1]\n- \"Unity Perception Package for Synthetic Data Generation\" (Unity Technologies, 2024) [2]\n- \"ROS# Bridge for Unity-ROS Communication\" (Robotics Open Source, 2024) [3]\n\n---",
    "chapter": 3,
    "section": "Further Reading",
    "page": 40,
    "token_count": 74
  },
  {
    "chunk_id": "e441f4c9-922d-4587-b5f4-af45fc337d4e",
    "doc_id": "7c4db3b3-0101-4b6a-a144-1ba467ab2f18",
    "chunk_text": "## References\n\n[1] Anderson, P., et al. \"Unity in Robotics: Bridging Simulation and Reality.\" IEEE Robotics & Automation Magazine, vol. 31, no. 3, pp. 78-89, 2024.\n\n[2] Unity Technologies. \"Unity Perception Package: Synthetic Data Generation for AI.\" Unity Developer Documentation. 2024. https://docs.unity3d.com/Packages/com.unity.perception@latest\n\n[3] Robotics Open Source. \"ROS# Bridge: Unity-ROS Communication Framework.\" ROS Documentation. 2024. https://github.com/siemens/ros-sharp",
    "chapter": 3,
    "section": "References",
    "page": 41,
    "token_count": 129
  },
  {
    "chunk_id": "44fbc2be-41e8-4dc6-8201-368f5d9d8de2",
    "doc_id": "055de8ad-3414-4fd6-9973-9ab67f8c38cb",
    "chunk_text": "# Capstone Integration\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "289a626b-5aa9-4121-ac3d-9773f9566545",
    "doc_id": "055de8ad-3414-4fd6-9973-9ab67f8c38cb",
    "chunk_text": "## Overview\n\nThis section provides a capstone integration project combining all concepts from previous chapters.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 18
  },
  {
    "chunk_id": "9466ef20-60d7-466f-806e-9742441061be",
    "doc_id": "055de8ad-3414-4fd6-9973-9ab67f8c38cb",
    "chunk_text": "## Key Concepts\n\n- Integrating ROS 2, Gazebo, and Isaac Sim\n- End-to-end robotics workflow\n- Navigation and perception integration\n- Project planning and execution",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 37
  },
  {
    "chunk_id": "9d0029fa-e2b2-4401-8967-318aa2ab596e",
    "doc_id": "055de8ad-3414-4fd6-9973-9ab67f8c38cb",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "57689d86-e159-4529-ae96-bde2c43e4841",
    "doc_id": "055de8ad-3414-4fd6-9973-9ab67f8c38cb",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "05a06af1-a1fa-45f3-9181-10e3a8999b2c",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "# Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 16
  },
  {
    "chunk_id": "0368fcc7-0841-4230-aea0-40be2222b65b",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Chapter Overview\nThis chapter delves into advanced AI and perception systems for robotics, with a focus on NVIDIA Isaac technology stack. Students will learn to implement sophisticated perception, navigation, and manipulation capabilities using NVIDIA's hardware-accelerated AI and robotics platforms. The chapter covers Isaac Sim, Isaac ROS, and various perception and planning tools that form the \"brain\" of AI-powered robots.\n\nNVIDIA Isaac represents a comprehensive AI platform specifically designed for robotics applications, providing hardware acceleration, perception algorithms, and simulation tools that enable next-generation robotic systems. The chapter will explore both the simulation aspects (Isaac Sim) and real-world deployment (Isaac ROS) of AI algorithms on robotic platforms.",
    "chapter": 4,
    "section": "Chapter Overview",
    "page": 2,
    "token_count": 138
  },
  {
    "chunk_id": "db9dfbf3-02f8-46b5-b493-98ef64ad3566",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Learning Objectives\nAfter completing this chapter, students will be able to:\n1. Understand the NVIDIA Isaac technology stack and its components\n2. Utilize Isaac Sim for high-fidelity AI training and simulation\n3. Implement Isaac ROS perception and navigation algorithms\n4. Deploy AI models on NVIDIA hardware for robotics applications\n5. Perform sim-to-real transfer using NVIDIA Isaac tools\n6. Integrate AI perception with robot control systems",
    "chapter": 4,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 87
  },
  {
    "chunk_id": "43fe2bfb-a29a-45bc-a08b-a009309f1da8",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Section Breakdown",
    "chapter": 4,
    "section": "Section Breakdown",
    "page": 4,
    "token_count": 4
  },
  {
    "chunk_id": "70ca4c2e-1bf1-4121-97a1-9a0b1f6107c1",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "### Section 1: Introduction to NVIDIA Isaac Technology\n- Overview of NVIDIA Isaac platform and components\n- Isaac Sim vs. Isaac ROS comparison\n- Hardware requirements and setup (Jetson, RTX GPUs, etc.)\n- Isaac ecosystem architecture\n- Integration with ROS 2 and other robotics frameworks",
    "chapter": 4,
    "section": "Section 1: Introduction to NVIDIA Isaac Technology",
    "page": 5,
    "token_count": 59
  },
  {
    "chunk_id": "e2f2c467-e74a-4c58-856f-d2666c2f8210",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "### Section 2: Isaac Sim for Advanced AI Training\n- Installing and setting up Isaac Sim\n- Creating photorealistic simulation environments\n- Synthetic data generation for AI training\n- Physics-accurate sensor simulation\n- Training AI models using synthetic data",
    "chapter": 4,
    "section": "Section 2: Isaac Sim for Advanced AI Training",
    "page": 6,
    "token_count": 51
  },
  {
    "chunk_id": "95292fd4-c80e-4704-a055-75fcd8f7a6a5",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "### Section 3: Isaac ROS Perception Pipeline\n- Isaac ROS packages and components\n- Hardware-accelerated perception algorithms\n- Camera, LIDAR, and sensor processing\n- Object detection and tracking\n- SLAM and localization with Isaac ROS",
    "chapter": 4,
    "section": "Section 3: Isaac ROS Perception Pipeline",
    "page": 7,
    "token_count": 51
  },
  {
    "chunk_id": "34ead79b-2405-4677-87a3-9562cd0a63f4",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "### Section 4: Navigation and Path Planning with Isaac\n- Isaac navigation stack\n- Path planning algorithms\n- Obstacle avoidance and dynamic replanning\n- Multi-robot navigation\n- Integration with perception systems",
    "chapter": 4,
    "section": "Section 4: Navigation and Path Planning with Isaac",
    "page": 8,
    "token_count": 42
  },
  {
    "chunk_id": "bb45ea87-7ce7-4ed2-b425-9e42d3671cb3",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "### Section 5: AI Integration and Control Systems\n- Connecting AI models to robot control\n- Reinforcement learning for robotics using Isaac\n- Behavior trees and AI decision making\n- Safety considerations in AI-robot systems\n- Performance optimization and real-time constraints",
    "chapter": 4,
    "section": "Section 5: AI Integration and Control Systems",
    "page": 9,
    "token_count": 52
  },
  {
    "chunk_id": "375873ce-869a-44fe-8fd3-d26c89d1c351",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Estimated Length\n- Total word count: ~10,000-12,000 words\n- Reading time: 3-4 hours\n- Hands-on practice time: 6-8 hours",
    "chapter": 4,
    "section": "Estimated Length",
    "page": 10,
    "token_count": 40
  },
  {
    "chunk_id": "8b0e7722-77fb-4f88-a4c9-0c0eb3d69d1b",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Prerequisites\n- Chapter 3: Understanding of digital twins and simulation\n- Solid understanding of ROS 2 concepts\n- Basic knowledge of AI and machine learning\n- Familiarity with NVIDIA hardware (preferred but not required)",
    "chapter": 4,
    "section": "Prerequisites",
    "page": 11,
    "token_count": 47
  },
  {
    "chunk_id": "d7c74f91-55bf-4f71-8ae8-a68ecda1dae4",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Technology Requirements\n- NVIDIA GPU (RTX series recommended)\n- Isaac Sim compatible hardware\n- ROS 2 Humble or Iron installation\n- Isaac ROS packages\n- Optional: NVIDIA Jetson platform",
    "chapter": 4,
    "section": "Technology Requirements",
    "page": 12,
    "token_count": 41
  },
  {
    "chunk_id": "ff0e2adc-e378-4d9a-a786-b3fa328e13de",
    "doc_id": "c749e047-a820-486a-9e92-5db9daffbb7d",
    "chunk_text": "## Code Examples Included\n- Isaac Sim scene configurations\n- Isaac ROS launch files\n- Perception pipeline implementations\n- AI model deployment scripts\n- Navigation and planning examples",
    "chapter": 4,
    "section": "Code Examples Included",
    "page": 13,
    "token_count": 33
  },
  {
    "chunk_id": "91444b7e-fc7f-4db1-9645-fc732158f206",
    "doc_id": "d0827e03-b28d-403e-92b4-3b83d5277621",
    "chunk_text": "# Isaac Overview\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "f8b98a31-4d77-46d3-bfeb-991837f52dd7",
    "doc_id": "d0827e03-b28d-403e-92b4-3b83d5277621",
    "chunk_text": "## Overview\n\nThis section introduces NVIDIA Isaac platform for robotics simulation and AI.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 15
  },
  {
    "chunk_id": "b9deb76c-18b6-4f49-a6b4-2d1f59f8ae7f",
    "doc_id": "d0827e03-b28d-403e-92b4-3b83d5277621",
    "chunk_text": "## Key Concepts\n\n- What is Isaac Sim\n- Isaac vs. Gazebo comparison\n- Isaac capabilities (photorealistic rendering, physics, synthetic data generation)\n- Isaac ecosystem (Isaac ROS, Isaac Gym)",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 45
  },
  {
    "chunk_id": "27965238-cbcd-4145-bc8f-2eac0e4a420d",
    "doc_id": "d0827e03-b28d-403e-92b4-3b83d5277621",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "e7551e8b-d143-4c55-9d1e-29ed181c8851",
    "doc_id": "d0827e03-b28d-403e-92b4-3b83d5277621",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "a7ade0e3-e21c-4df1-8d7e-376d33c3fecf",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "# Isaac ROS Perception Pipeline",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 5
  },
  {
    "chunk_id": "e368dcc1-9cf2-4dcf-bf08-7cd9d133e53d",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Overview\n\nThe Isaac ROS perception pipeline represents a collection of hardware-accelerated computer vision and perception algorithms specifically designed to run on NVIDIA platforms. These packages leverage GPU acceleration through CUDA and TensorRT to deliver real-time performance for computationally intensive perception tasks such as object detection, SLAM, and sensor processing. The Isaac ROS perception stack bridges the gap between raw sensor data and high-level robot decision making, providing essential capabilities for autonomous navigation, manipulation, and interaction.\n\nIsaac ROS perception packages are optimized to take full advantage of NVIDIA's parallel computing architecture, delivering performance improvements of 5x to 50x over CPU-only implementations. This acceleration is crucial for real-time robotics applications where perception latency directly impacts robot safety and performance. The packages integrate seamlessly with the ROS 2 ecosystem while providing the computational efficiency needed for deployment on edge devices like NVIDIA Jetson platforms.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 174
  },
  {
    "chunk_id": "8f3afdda-6eef-4a0a-81dd-e5c0acc14ad8",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Key Concepts",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "4f8259df-1d37-4c60-a2b1-a447c336ea11",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Hardware Acceleration in Perception\n\nNVIDIA's GPU architecture provides several key advantages for perception tasks:\n\n**Parallel Processing**: Perception algorithms like image processing, feature extraction, and neural network inference can be parallelized across thousands of CUDA cores, enabling real-time processing of high-resolution sensor data.\n\n**Tensor Cores**: Specialized for deep learning inference, Tensor Cores provide significant acceleration for neural network-based perception tasks, enabling sophisticated AI models to run on robotic platforms.\n\n**Computer Vision Acceleration**: Dedicated hardware units in modern NVIDIA GPUs accelerate common computer vision operations like stereo matching, optical flow, and feature detection.\n\n**Memory Bandwidth**: High-bandwidth memory systems provide rapid access to large sensor datasets, reducing bottlenecks in perception pipelines.",
    "chapter": 4,
    "section": "Hardware Acceleration in Perception",
    "page": 4,
    "token_count": 147
  },
  {
    "chunk_id": "03252dc9-8c11-481f-a289-461f885dff21",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Isaac ROS Package Architecture\n\nThe Isaac ROS perception stack follows a modular architecture where each package handles a specific perception function:\n\n**Isaac ROS Apriltag**: High-performance AprilTag detection for robot localization, calibration, and fiducial-based navigation.\n\n**Isaac ROS DNN Inference**: GPU-accelerated deep learning inference for object detection, classification, and semantic segmentation.\n\n**Isaac ROS Stereo DNN**: Stereo vision processing with deep neural networks for depth estimation and 3D reconstruction.\n\n**Isaac ROS Visual SLAM**: Simultaneous localization and mapping using visual-inertial odometry for position tracking.\n\n**Isaac ROS Point Cloud**: GPU-accelerated processing of 3D point cloud data from various sensors.",
    "chapter": 4,
    "section": "Isaac ROS Package Architecture",
    "page": 5,
    "token_count": 150
  },
  {
    "chunk_id": "a7a45912-5766-4d19-8157-7293e86d9dbe",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### GPU Optimization Techniques\n\nIsaac ROS employs several optimization strategies to maximize GPU utilization:\n\n**CUDA Kernels**: Custom CUDA implementations of perception algorithms provide maximum performance on NVIDIA hardware.\n\n**Memory Management**: Efficient GPU memory allocation and data transfer minimize overhead between CPU and GPU processing.\n\n**Pipeline Parallelism**: Multiple stages of perception pipelines run concurrently to maximize throughput.\n\n**Mixed Precision**: Use of FP16 (half-precision) for neural network inference when accuracy allows, providing performance improvements with minimal accuracy loss.",
    "chapter": 4,
    "section": "GPU Optimization Techniques",
    "page": 6,
    "token_count": 99
  },
  {
    "chunk_id": "a1cb540c-a431-4bef-b33b-6c04c561eb38",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Isaac ROS Apriltag Package",
    "chapter": 4,
    "section": "Isaac ROS Apriltag Package",
    "page": 7,
    "token_count": 7
  },
  {
    "chunk_id": "d6f994a5-dd4d-4723-a8ff-6d793786a9d2",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Overview\n\nIsaac ROS Apriltag provides hardware-accelerated fiducial marker detection. AprilTags are robust visual fiducials that can be detected from various viewing angles and distances, making them ideal for robot localization, calibration, and augmented reality applications.",
    "chapter": 4,
    "section": "Overview",
    "page": 8,
    "token_count": 54
  },
  {
    "chunk_id": "344087c5-9f6c-4e2e-95b1-267f292d06c0",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Implementation Details\n\nThe Isaac ROS Apriltag package consists of:\n\n- **GPU-accelerated detection**: Uses CUDA for parallel detection of AprilTags in images\n- **Multiple tag families**: Supports various AprilTag families optimized for different use cases\n- **Pose estimation**: Calculates the 6-DOF pose of detected tags relative to the camera\n- **Multi-camera support**: Can process multiple camera streams simultaneously",
    "chapter": 4,
    "section": "Implementation Details",
    "page": 9,
    "token_count": 84
  },
  {
    "chunk_id": "47bb3502-3314-4932-ac07-bebefe2f84b0",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Example Usage\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Header\nfrom vision_msgs.msg import Detection2DArray\nimport cv2\nimport numpy as np",
    "chapter": 4,
    "section": "Example Usage",
    "page": 10,
    "token_count": 60
  },
  {
    "chunk_id": "0a9076a3-36bb-4823-9c90-4eff7d2a4953",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Example Usage\n\nclass IsaacApriltagExample(Node):\n    def __init__(self):\n        super().__init__('isaac_apriltag_example')\n        \n        # Subscribe to camera image\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.image_callback,\n            10\n        )\n        \n        # Publish detected tag poses\n        self.pose_publisher = self.create_publisher(\n            PoseStamped,\n            '/tag_poses',\n            10\n        )\n        \n        # Publish detections\n        self.detection_publisher = self.create_publisher(\n            Detection2DArray,\n            '/apriltag_detections',\n            10\n        )\n        \n        # Camera intrinsic parameters (typically obtained from camera_info topic)\n        self.camera_matrix = np.array([\n            [616.297170, 0.0, 315.243499],\n            [0.0, 615.493856, 227.098129],\n            [0.0, 0.0, 1.0]\n        ])\n        \n        self.distortion_coeffs = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n        \n    def image_callback(self, msg):\n        # In practice, this would interface with the Isaac ROS Apriltag package\n        # For this example, we'll simulate the detection logic\n        \n        # Convert ROS Image to OpenCV format\n        # (Isaac ROS handles this internally)\n        \n        # Isaac ROS Apriltag would process the image here\n        # and return tag detections with poses\n        \n        # Process detections and publish results\n        self.process_detections()\n    \n    def process_detections(self):\n        # This is where Isaac ROS Apriltag results would be processed\n        # For example: publish tag poses, trigger navigation goals, etc.\n        pass\n```",
    "chapter": 4,
    "section": "Example Usage",
    "page": 11,
    "token_count": 386
  },
  {
    "chunk_id": "71c950e3-287f-44dc-9539-6c6b51334d2d",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Launch File Configuration\n\n```xml\n<launch>\n  <!-- AprilTag detection parameters -->\n  <node pkg=\"isaac_ros_apriltag\" exec=\"isaac_ros_apriltag\" name=\"apriltag\">\n    <param name=\"family\" value=\"tag36h11\"/>\n    <param name=\"size\" value=\"0.145\"/>  <!-- Tag size in meters -->\n    <param name=\"max_hamming\" value=\"0\"/>\n    <param name=\"quad_decimate\" value=\"2.0\"/>\n    <param name=\"quad_sigma\" value=\"0.0\"/>\n    <param name=\"nthreads\" value=\"4\"/>\n    <param name=\"decode_sharpening\" value=\"0.25\"/>\n    <param name=\"debug\" value=\"0\"/>\n    \n    <!-- Input/Output topics -->\n    <remap from=\"image\" to=\"/camera/image_rect_color\"/>\n    <remap from=\"camera_info\" to=\"/camera/camera_info\"/>\n    <remap from=\"detections\" to=\"/apriltag_detections\"/>\n  </node>\n</launch>\n```",
    "chapter": 4,
    "section": "Launch File Configuration",
    "page": 12,
    "token_count": 228
  },
  {
    "chunk_id": "f91953ef-ccda-4d11-8a51-e31e0bd48f71",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Isaac ROS DNN Inference Package",
    "chapter": 4,
    "section": "Isaac ROS DNN Inference Package",
    "page": 13,
    "token_count": 8
  },
  {
    "chunk_id": "b50dbe26-efa9-46fe-84e6-b930ee3a82cf",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Overview\n\nThe Isaac ROS DNN Inference package provides GPU-accelerated deep learning inference for various computer vision tasks. It supports TensorFlow, PyTorch, and ONNX models that are optimized using TensorRT for maximum performance on NVIDIA hardware.",
    "chapter": 4,
    "section": "Overview",
    "page": 14,
    "token_count": 51
  },
  {
    "chunk_id": "18c98f13-85de-4db7-aae6-90fd043b9de8",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Supported Inference Types\n\n- **Object Detection**: Detect and classify objects in images\n- **Semantic Segmentation**: Pixel-level classification of image content\n- **Pose Estimation**: Human or object pose estimation\n- **Classification**: Image classification tasks\n- **Custom Models**: User-defined deep learning models",
    "chapter": 4,
    "section": "Supported Inference Types",
    "page": 15,
    "token_count": 61
  },
  {
    "chunk_id": "11a96b47-4fde-44da-b32f-79bf803d7d00",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Example Implementation\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom vision_msgs.msg import Detection2D\nfrom vision_msgs.msg import ObjectHypothesisWithPose\nimport numpy as np\nfrom cv_bridge import CvBridge",
    "chapter": 4,
    "section": "Example Implementation",
    "page": 16,
    "token_count": 69
  },
  {
    "chunk_id": "ba1b84a9-613a-45b7-93b0-2a778d2c81a8",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Example Implementation\n\nclass IsaacDNNExample(Node):\n    def __init__(self):\n        super().__init__('isaac_dnn_example')\n        \n        # Initialize OpenCV bridge\n        self.cv_bridge = CvBridge()\n        \n        # Subscribe to camera image\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.image_callback,\n            10\n        )\n        \n        # Publish detections\n        self.detection_publisher = self.create_publisher(\n            Detection2DArray,\n            '/dnn_detections',\n            10\n        )\n        \n        # In a real implementation, this would interface with Isaac ROS DNN\n        # For demonstration, show the expected interface\n        self.get_logger().info('Isaac ROS DNN Example Node Started')\n        \n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        \n        # In practice, the Isaac ROS DNN pipeline would process the image\n        # and return detection results\n        detections = self.run_inference(cv_image)\n        \n        # Publish detection results\n        self.publish_detections(detections, msg.header)\n    \n    def run_inference(self, image):\n        # This would interface with the Isaac ROS DNN pipeline\n        # In practice, Isaac ROS DNN handles the TensorRT optimization\n        # and GPU inference automatically\n        return []\n    \n    def publish_detections(self, detections, header):\n        # Create Detection2DArray message\n        detection_msg = Detection2DArray()\n        detection_msg.header = header\n        \n        # Add each detection to the array\n        for detection in detections:\n            detection_2d = Detection2D()\n            detection_2d.header = header\n            \n            # Set bounding box\n            detection_2d.bbox.center.x = detection['center_x']\n            detection_2d.bbox.center.y = detection['center_y']\n            detection_2d.bbox.size_x = detection['width']\n            detection_2d.bbox.size_y = detection['height']\n            \n            # Set classification\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = detection['class_id']\n            hypothesis.hypothesis.score = detection['confidence']\n            detection_2d.results.append(hypothesis)\n            \n            detection_msg.detections.append(detection_2d)\n        \n        # Publish the detection array\n        self.detection_publisher.publish(detection_msg)\n```",
    "chapter": 4,
    "section": "Example Implementation",
    "page": 17,
    "token_count": 518
  },
  {
    "chunk_id": "c684da4f-20bc-47eb-be66-4fed02cd0bfe",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### TensorRT Optimization Process\n\nThe Isaac ROS DNN package uses TensorRT for model optimization:\n\n1. **Model Import**: Import models from TensorFlow, PyTorch, or ONNX\n2. **Calibration**: Optimize for precision vs. performance trade-offs\n3. **Engine Creation**: Generate optimized TensorRT inference engine\n4. **Runtime Execution**: Execute inference on GPU with maximum performance\n\n```python\n# Example of TensorRT optimization process\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit",
    "chapter": 4,
    "section": "TensorRT Optimization Process",
    "page": 18,
    "token_count": 113
  },
  {
    "chunk_id": "7962bc7f-6c91-45fc-a251-984e2b2ae91a",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## TensorRT Optimization Process\n\ndef optimize_model_for_tensorrt(onnx_model_path, engine_path):\n    \"\"\"\n    Optimize a model for TensorRT inference\n    This is typically handled automatically by Isaac ROS DNN\n    but shown here for understanding\n    \"\"\"\n    # Create TensorRT builder and network\n    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n    builder = trt.Builder(TRT_LOGGER)\n    \n    # Create network flags for dynamic shapes\n    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n    config = builder.create_builder_config()\n    \n    # Parse ONNX model\n    parser = trt.OnnxParser(network, TRT_LOGGER)\n    \n    with open(onnx_model_path, 'rb') as model:\n        parser.parse(model.read())\n    \n    # Build engine\n    config.max_workspace_size = 1 << 30  # 1GB\n    \n    # Create optimization profile for dynamic shapes\n    profile = builder.create_optimization_profile()\n    profile.set_shape('input', (1, 3, 224, 224), (1, 3, 224, 224), (1, 3, 224, 224))\n    config.add_optimization_profile(profile)\n    \n    # Build the engine\n    engine = builder.build_engine(network, config)\n    \n    # Serialize engine to file\n    with open(engine_path, 'wb') as f:\n        f.write(engine.serialize())\n    \n    return engine",
    "chapter": 4,
    "section": "TensorRT Optimization Process",
    "page": 19,
    "token_count": 305
  },
  {
    "chunk_id": "d8df2b37-a83f-404a-8e78-d754517088ad",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## TensorRT Optimization Process\n\n# In Isaac ROS DNN, this optimization happens automatically\n# with appropriate parameter configurations\n```",
    "chapter": 4,
    "section": "TensorRT Optimization Process",
    "page": 20,
    "token_count": 25
  },
  {
    "chunk_id": "505137c4-cca5-4935-ad45-ad0d5f75bc4a",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Isaac ROS Stereo DNN Package",
    "chapter": 4,
    "section": "Isaac ROS Stereo DNN Package",
    "page": 21,
    "token_count": 7
  },
  {
    "chunk_id": "75bc0ae4-4889-40f1-b137-c2841ce6600f",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Overview\n\nIsaac ROS Stereo DNN combines stereo vision with deep learning for enhanced depth estimation and 3D perception. This package fuses traditional stereo matching algorithms with neural network processing to achieve improved accuracy and robustness in depth estimation.",
    "chapter": 4,
    "section": "Overview",
    "page": 22,
    "token_count": 48
  },
  {
    "chunk_id": "c05aad8f-b805-4cc0-b651-f8be56db0e18",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Stereo Vision Fundamentals\n\nStereo vision works by comparing images from two cameras separated by a known baseline distance:\n\n- **Epipolar Geometry**: The geometric relationship between the two cameras\n- **Disparity Map**: Pixel-wise difference between left and right images\n- **Depth Calculation**: Conversion of disparity to depth using camera parameters",
    "chapter": 4,
    "section": "Stereo Vision Fundamentals",
    "page": 23,
    "token_count": 66
  },
  {
    "chunk_id": "bb8d0194-9c2d-4f06-a906-e7aa1f784517",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Implementation\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom sensor_msgs.msg import CameraInfo\nfrom stereo_msgs.msg import DisparityImage\nimport numpy as np",
    "chapter": 4,
    "section": "Implementation",
    "page": 24,
    "token_count": 47
  },
  {
    "chunk_id": "256f70d5-eb2a-4635-856b-5a1487f2069e",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Implementation\n\nclass IsaacStereoDNNExample(Node):\n    def __init__(self):\n        super().__init__('isaac_stereo_dnn_example')\n        \n        # Subscribe to stereo pair images\n        self.left_subscription = self.create_subscription(\n            Image,\n            '/stereo/left/image_rect_color',\n            self.left_image_callback,\n            10\n        )\n        \n        self.right_subscription = self.create_subscription(\n            Image,\n            '/stereo/right/image_rect_color', \n            self.right_image_callback,\n            10\n        )\n        \n        # Subscribe to camera info\n        self.left_info_subscription = self.create_subscription(\n            CameraInfo,\n            '/stereo/left/camera_info',\n            self.left_info_callback,\n            10\n        )\n        \n        self.right_info_subscription = self.create_subscription(\n            CameraInfo,\n            '/stereo/right/camera_info',\n            self.right_info_callback,\n            10\n        )\n        \n        # Publish disparity and depth maps\n        self.disparity_publisher = self.create_publisher(\n            DisparityImage,\n            '/stereo/disparity',\n            10\n        )\n        \n        # Camera parameters storage\n        self.left_camera_info = None\n        self.right_camera_info = None\n        self.left_image = None\n        self.right_image = None\n        \n        # Synchronization flag\n        self.images_ready = False\n        \n    def left_image_callback(self, msg):\n        # Store left image\n        self.left_image = msg\n        self.check_synchronization()\n    \n    def right_image_callback(self, msg):\n        # Store right image\n        self.right_image = msg\n        self.check_synchronization()\n    \n    def left_info_callback(self, msg):\n        # Store left camera info\n        self.left_camera_info = msg\n    \n    def right_info_callback(self, msg):\n        # Store right camera info\n        self.right_camera_info = msg\n    \n    def check_synchronization(self):\n        # Check if both images are available\n        if self.left_image and self.right_image and not self.images_ready:\n            self.images_ready = True\n            self.process_stereo_pair()\n    \n    def process_stereo_pair(self):\n        # In Isaac ROS Stereo DNN, this would use GPU-accelerated stereo + DNN\n        # For this example, we'll show the general approach\n        \n        # Process stereo pair for depth estimation\n        # (Isaac ROS handles the complex GPU processing)\n        \n        self.get_logger().info('Processing stereo pair for depth estimation')\n        \n        # Publish results\n        # self.publish_depth_results()\n```",
    "chapter": 4,
    "section": "Implementation",
    "page": 25,
    "token_count": 520
  },
  {
    "chunk_id": "6b6a86c2-5f0b-422b-baa6-b75385f6417c",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Isaac ROS Visual SLAM Package",
    "chapter": 4,
    "section": "Isaac ROS Visual SLAM Package",
    "page": 26,
    "token_count": 7
  },
  {
    "chunk_id": "11543f82-fdce-4bdf-9e25-62bcee15dce4",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Overview\n\nIsaac ROS Visual SLAM provides GPU-accelerated visual-inertial simultaneous localization and mapping. This package combines visual features with IMU data to provide robust localization and mapping capabilities for robots operating in unknown environments.",
    "chapter": 4,
    "section": "Overview",
    "page": 27,
    "token_count": 47
  },
  {
    "chunk_id": "ce9098df-665b-43e9-92cd-258d1cffdf30",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Visual-Inertial Odometry\n\nVisual-inertial odometry combines:\n\n- **Visual Features**: Points and landmarks extracted from camera images\n- **Inertial Measurements**: Accelerometer and gyroscope data from IMU\n- **Sensor Fusion**: Kalman filtering or optimization to combine sensor data",
    "chapter": 4,
    "section": "Visual-Inertial Odometry",
    "page": 28,
    "token_count": 60
  },
  {
    "chunk_id": "d43d7481-b213-4f35-9ea4-68000784aea5",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Example Implementation\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nimport numpy as np",
    "chapter": 4,
    "section": "Example Implementation",
    "page": 29,
    "token_count": 55
  },
  {
    "chunk_id": "d3d2e6a3-b7b9-43ac-99d5-5d58d3b465ba",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Example Implementation\n\nclass IsaacVisualSLAMExample(Node):\n    def __init__(self):\n        super().__init__('isaac_visual_slam_example')\n        \n        # Subscribe to camera and IMU data\n        self.image_subscription = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.image_callback,\n            10\n        )\n        \n        self.imu_subscription = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n        \n        # Publish pose and odometry\n        self.pose_publisher = self.create_publisher(\n            PoseStamped,\n            '/visual_slam/pose',\n            10\n        )\n        \n        self.odom_publisher = self.create_publisher(\n            Odometry,\n            '/visual_slam/odometry',\n            10\n        )\n        \n        # SLAM map publisher\n        self.map_publisher = self.create_publisher(\n            OccupancyGrid,  # Simplified - would use actual map type\n            '/visual_slam/map',\n            10\n        )\n        \n        # Initialize pose tracking\n        self.current_pose = np.eye(4)  # Identity transformation\n        self.last_image_time = None\n        \n    def image_callback(self, msg):\n        # Isaac ROS Visual SLAM processes the image for features\n        # and updates the pose estimate\n        self.process_visual_features(msg)\n    \n    def imu_callback(self, msg):\n        # Isaac ROS Visual SLAM uses IMU data for better\n        # motion estimation and initialization\n        self.integrate_imu_data(msg)\n    \n    def process_visual_features(self, image_msg):\n        # This would interface with Isaac ROS Visual SLAM\n        # which performs feature extraction, tracking, and pose estimation\n        \n        # Update pose based on visual-inertial fusion\n        self.update_pose_estimate()\n    \n    def integrate_imu_data(self, imu_msg):\n        # Integrate IMU measurements to improve pose prediction\n        # between visual updates\n        angular_velocity = [\n            imu_msg.angular_velocity.x,\n            imu_msg.angular_velocity.y,\n            imu_msg.angular_velocity.z\n        ]\n        \n        linear_acceleration = [\n            imu_msg.linear_acceleration.x,\n            imu_msg.linear_acceleration.y,\n            imu_msg.linear_acceleration.z\n        ]\n        \n        # Process IMU data (handled by Isaac ROS Visual SLAM)\n    \n    def update_pose_estimate(self):\n        # Update the current pose based on SLAM processing\n        # This would be done by Isaac ROS Visual SLAM internally\n        \n        # Publish current pose\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \"map\"\n        \n        # Set pose from SLAM results\n        # (Isaac ROS Visual SLAM provides these results)\n        self.pose_publisher.publish(pose_msg)\n```",
    "chapter": 4,
    "section": "Example Implementation",
    "page": 30,
    "token_count": 587
  },
  {
    "chunk_id": "eee36bfc-fe6f-4a50-86c8-2d35dc6d8393",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Integration with Navigation Stack",
    "chapter": 4,
    "section": "Integration with Navigation Stack",
    "page": 31,
    "token_count": 5
  },
  {
    "chunk_id": "2fbb2396-7b27-4836-b11b-4eed86d82e1f",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Perception-Navigation Interface\n\nIsaac ROS perception packages integrate with ROS 2 navigation systems through standardized interfaces:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import Image, LaserScan\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient",
    "chapter": 4,
    "section": "Perception-Navigation Interface",
    "page": 32,
    "token_count": 81
  },
  {
    "chunk_id": "67b689d4-0f03-41e3-af07-61dffce551bf",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Perception-Navigation Interface\n\nclass PerceptionNavigationIntegration(Node):\n    def __init__(self):\n        super().__init__('perception_navigation_integration')\n        \n        # Perception result subscribers\n        self.detection_subscription = self.create_subscription(\n            Detection2DArray,\n            '/dnn_detections',\n            self.detection_callback,\n            10\n        )\n        \n        self.pose_subscription = self.create_subscription(\n            PoseStamped,\n            '/visual_slam/pose',\n            self.pose_callback,\n            10\n        )\n        \n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        \n        # Frame transformation\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n    \n    def detection_callback(self, msg):\n        \"\"\"Process object detections and update navigation goals\"\"\"\n        for detection in msg.detections:\n            # Classify detection (obstacle, goal, etc.)\n            class_id = detection.results[0].hypothesis.class_id\n            confidence = detection.results[0].hypothesis.score\n            \n            if confidence > 0.7:  # High confidence detection\n                if class_id == \"person\":\n                    # Update robot behavior based on person detection\n                    self.handle_person_detection(detection)\n                elif class_id == \"obstacle\":\n                    # Add to costmap for navigation\n                    self.update_costmap(detection)\n    \n    def handle_person_detection(self, detection):\n        \"\"\"Handle detection of people for navigation\"\"\"\n        # Navigate around people or follow if that's the task\n        person_pose = self.convert_detection_to_world_frame(detection)\n        \n        # Plan navigation considering the person\n        self.plan_navigation_around_person(person_pose)\n    \n    def update_costmap(self, detection):\n        \"\"\"Update navigation costmap with dynamic obstacles\"\"\"\n        # This would interface with navigation system costmaps\n        # to add temporarily obstacles to avoid\n        pass\n    \n    def convert_detection_to_world_frame(self, detection):\n        \"\"\"Convert detection from camera to world frame using TF\"\"\"\n        # Get current camera pose in world coordinates\n        # Transform detection to world frame\n        pass\n```",
    "chapter": 4,
    "section": "Perception-Navigation Interface",
    "page": 33,
    "token_count": 442
  },
  {
    "chunk_id": "1c5d4d11-8e4f-4127-a027-604f68f613a7",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Performance Optimization",
    "chapter": 4,
    "section": "Performance Optimization",
    "page": 34,
    "token_count": 3
  },
  {
    "chunk_id": "08c66b47-a635-4fca-a279-4b42660859be",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### GPU Memory Management\n\nEfficient GPU memory usage is critical for real-time performance:\n\n```python\nimport torch\nimport rclpy\nfrom rclpy.node import Node",
    "chapter": 4,
    "section": "GPU Memory Management",
    "page": 35,
    "token_count": 35
  },
  {
    "chunk_id": "a0872e23-89d7-402d-8d0d-bdd8e854b911",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## GPU Memory Management\n\nclass OptimizedIsaacROSNode(Node):\n    def __init__(self):\n        super().__init__('optimized_isaac_ros_node')\n        \n        # Configure GPU memory allocation\n        if torch.cuda.is_available():\n            # Set memory fraction to avoid OOM errors\n            torch.cuda.set_per_process_memory_fraction(0.8)\n            \n            # Enable memory caching\n            torch.cuda.empty_cache()\n            \n            # Get GPU memory info\n            self.gpu_memory_info = torch.cuda.get_device_properties(0)\n            self.get_logger().info(f'GPU Memory: {self.gpu_memory_info.total_memory / 1e9:.2f} GB')\n        \n        # Pre-allocate GPU tensors for inference\n        self.input_tensor = None\n        self.output_tensor = None\n        self.tensor_size = None\n        \n    def allocate_tensors(self, input_size, output_size):\n        \"\"\"Pre-allocate GPU tensors to avoid allocation overhead\"\"\"\n        if torch.cuda.is_available():\n            self.input_tensor = torch.zeros(input_size, device='cuda')\n            self.output_tensor = torch.zeros(output_size, device='cuda')\n            self.tensor_size = input_size\n    \n    def process_image_optimized(self, image_data):\n        \"\"\"Process image with optimized memory management\"\"\"\n        if torch.cuda.is_available():\n            # Ensure tensor is the right size\n            if self.input_tensor is None or self.input_tensor.shape != self.tensor_size:\n                self.allocate_tensors(image_data.shape, (1, 100, 4))  # Example output\n            \n            # Copy data to pre-allocated tensor\n            self.input_tensor.copy_(torch.from_numpy(image_data))\n            \n            # Process with Isaac ROS DNN (simulated)\n            with torch.no_grad():\n                result = self.run_inference(self.input_tensor)\n            \n            return result.cpu().numpy()\n        else:\n            # Fallback to CPU processing\n            return self.cpu_fallback(image_data)\n```",
    "chapter": 4,
    "section": "GPU Memory Management",
    "page": 36,
    "token_count": 390
  },
  {
    "chunk_id": "fe3a1540-81e3-4e2f-8a0e-def94e08556d",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Launch System Integration",
    "chapter": 4,
    "section": "Launch System Integration",
    "page": 37,
    "token_count": 4
  },
  {
    "chunk_id": "08480ecd-fd78-4f4d-a2d1-cc1c61c36d3d",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Complete Perception Stack Launch\n\n```python\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare",
    "chapter": 4,
    "section": "Complete Perception Stack Launch",
    "page": 38,
    "token_count": 74
  },
  {
    "chunk_id": "e0f7c90b-1fc4-4c0b-bab7-7c3ba211a49b",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Complete Perception Stack Launch\n\ndef generate_launch_description():\n    # Launch configuration\n    use_camera = LaunchConfiguration('use_camera', default='true')\n    use_imu = LaunchConfiguration('use_imu', default='true')\n    use_lidar = LaunchConfiguration('use_lidar', default='true')\n    \n    # Declare launch arguments\n    declare_use_camera_cmd = DeclareLaunchArgument(\n        'use_camera',\n        default_value='true',\n        description='Use camera for perception'\n    )\n    \n    declare_use_imu_cmd = DeclareLaunchArgument(\n        'use_imu', \n        default_value='true',\n        description='Use IMU for visual-inertial SLAM'\n    )\n    \n    declare_use_lidar_cmd = DeclareLaunchArgument(\n        'use_lidar',\n        default_value='true', \n        description='Use LIDAR for mapping'\n    )\n    \n    # Isaac ROS perception nodes\n    apriltag_node = Node(\n        package='isaac_ros_apriltag',\n        executable='isaac_ros_apriltag',\n        name='apriltag',\n        parameters=[{\n            'family': 'tag36h11',\n            'size': 0.145,\n            'max_hamming': 0,\n            'quad_decimate': 2.0\n        }],\n        condition=IfCondition(use_camera)\n    )\n    \n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        name='visual_slam',\n        parameters=[{\n            'use_imu': use_imu,\n            'use_vio': True,\n            'image_input_width': 640,\n            'image_input_height': 480\n        }],\n        remappings=[\n            ('/visual_slam/image_raw', '/camera/image_rect_color'),\n            ('/visual_slam/camera_info', '/camera/camera_info'),\n            ('/visual_slam/imu', '/imu/data')\n        ],\n        condition=IfCondition(use_camera)\n    )\n    \n    dnn_inference_node = Node(\n        package='isaac_ros_dnn_inference',\n        executable='dnn_inference',\n        name='dnn_inference',\n        parameters=[{\n            'model_path': '/path/to/model.engine',\n            'input_topic': '/camera/image_rect_color',\n            'output_topic': '/dnn_detections'\n        }],\n        condition=IfCondition(use_camera)\n    )\n    \n    # Create launch description\n    ld = LaunchDescription()\n    \n    # Add launch arguments\n    ld.add_action(declare_use_camera_cmd)\n    ld.add_action(declare_use_imu_cmd)\n    ld.add_action(declare_use_lidar_cmd)\n    \n    # Add nodes\n    ld.add_action(apriltag_node)\n    ld.add_action(visual_slam_node)\n    ld.add_action(dnn_inference_node)\n    \n    return ld\n```",
    "chapter": 4,
    "section": "Complete Perception Stack Launch",
    "page": 39,
    "token_count": 595
  },
  {
    "chunk_id": "0e09e282-7afa-42ba-a1b2-c29e410d1489",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Diagrams",
    "chapter": 4,
    "section": "Diagrams",
    "page": 40,
    "token_count": 3
  },
  {
    "chunk_id": "378a3921-deb6-4f50-9fb1-65333efd7b5d",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Isaac ROS Perception Pipeline Architecture",
    "chapter": 4,
    "section": "Isaac ROS Perception Pipeline Architecture",
    "page": 41,
    "token_count": 6
  },
  {
    "chunk_id": "1bd4132c-72b9-4393-a43e-16e0ee6e5f1e",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Isaac ROS Perception Pipeline Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    ISAAC ROS PERCEPTION PIPELINE                │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │   SENSORS   │    │  ISAAC ROS  │    │  GPU        │         │\n│  │             │    │  PACKAGES   │    │  COMPUTE    │         │\n│  │ • Camera    │───▶│ • Apriltag  │───▶│ • CUDA      │         │\n│  │ • IMU       │    │ • DNN Infer │    │ • TensorRT  │         │\n│  │ • LIDAR     │    │ • Stereo    │    │ • CV-Accel  │         │\n│  │ • Depth     │    │ • Visual    │    │             │         │\n│  │ • Other     │    │   SLAM      │    │             │         │\n│  └─────────────┘    │ • Point     │    └─────────────┘         │\n│                     │   Cloud     │                            │\n│                     └─────────────┘                            │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                  OUTPUT MESSAGES                        │   │\n│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────────┐   │   │\n│  │  │ PoseStamped │ │ Detection2D │ │ OccupancyGrid   │   │   │\n│  │  │ (Localization│ │ (Objects)   │ │ (Mapping)       │   │   │\n│  │  │ & Mapping)  │ │             │ │                 │   │   │\n│  │  └─────────────┘ └─────────────┘ └─────────────────┘   │   │\n│  └─────────────────────────────────────────────────────────┘   │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │               INTEGRATION LAYERS                        │   │\n│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────────┐   │   │\n│  │  │ Navigation  │ │ Manipulation│ │ Behavior Trees  │   │   │\n│  │  │ (Nav2)      │ │ (MoveIt2)   │ │ (PyTrees)       │   │   │\n│  │  └─────────────┘ └─────────────┘ └─────────────────┘   │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 4,
    "section": "Isaac ROS Perception Pipeline Architecture",
    "page": 42,
    "token_count": 661
  },
  {
    "chunk_id": "e74c134a-6cd7-4e89-8d74-139b940b50ef",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Performance Comparison\n\n```\nCPU Processing    vs    GPU Acceleration (Isaac ROS)\n┌─────────────┐         ┌─────────────┐\n│ Object      │         │ Object      │\n│ Detection   │         │ Detection   │\n│ (2 Hz)      │         │ (30 Hz)     │\n│             │         │             │\n│ SLAM        │         │ SLAM        │\n│ (5 Hz)      │         │ (60 Hz)     │\n│             │         │             │\n│ Stereo      │         │ Stereo      │\n│ (1 Hz)      │         │ (15 Hz)     │\n└─────────────┘         └─────────────┘\n     ^                       ^\n     │ Slow Processing       │ Fast Processing\n     └───────────────────────┘\n           Isaac ROS Advantage: 5x - 50x Speedup\n```",
    "chapter": 4,
    "section": "Performance Comparison",
    "page": 43,
    "token_count": 197
  },
  {
    "chunk_id": "2de78c86-ac60-4bbe-be7c-592be3be84f7",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Install and configure Isaac ROS perception packages\n2. Configure and launch perception pipelines with GPU acceleration\n3. Integrate perception outputs with navigation and manipulation systems\n4. Optimize GPU memory usage for real-time performance\n5. Implement sensor fusion between multiple perception modalities\n6. Configure launch files for complex perception stacks",
    "chapter": 4,
    "section": "Learning Outcomes",
    "page": 44,
    "token_count": 81
  },
  {
    "chunk_id": "3721e325-adcd-4e2d-9ad5-39ee8d3aabdd",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Advanced Topics",
    "chapter": 4,
    "section": "Advanced Topics",
    "page": 45,
    "token_count": 3
  },
  {
    "chunk_id": "847d1773-2980-4f12-b45f-faf1dfe93ac7",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Multi-Sensor Fusion\n\n- Extended Kalman Filtering for sensor integration\n- Particle filtering for robust state estimation\n- Time synchronization between different sensor types\n- Covariance estimation and uncertainty modeling",
    "chapter": 4,
    "section": "Multi-Sensor Fusion",
    "page": 46,
    "token_count": 38
  },
  {
    "chunk_id": "d7fb3253-673c-47f9-89ef-29ce6bf52e4f",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Model Optimization\n\n- Quantization for deployment on resource-constrained platforms\n- Pruning techniques for faster inference\n- Knowledge distillation for smaller, faster models\n- Edge AI optimization strategies",
    "chapter": 4,
    "section": "Model Optimization",
    "page": 47,
    "token_count": 38
  },
  {
    "chunk_id": "44535468-4afb-455a-b682-5df1107b7a59",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "### Real-time Performance\n\n- Pipeline scheduling and optimization\n- Memory management strategies\n- Multi-threading and concurrency patterns\n- Profiling and performance analysis tools",
    "chapter": 4,
    "section": "Real-time Performance",
    "page": 48,
    "token_count": 31
  },
  {
    "chunk_id": "568b1f98-b8ed-4f31-a72f-1846993d5a85",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## Further Reading\n\n- \"Isaac ROS Perception: GPU-Accelerated Computer Vision for Robotics\" (NVIDIA Developer Documentation, 2025) [1]\n- \"Real-Time Object Detection on Edge GPU Platforms\" (IEEE Transactions on Robotics, 2024) [2]\n- \"Visual-Inertial SLAM: A Survey of Approaches and Applications\" (IEEE Robotics & Automation Magazine, 2024) [3]\n\n---",
    "chapter": 4,
    "section": "Further Reading",
    "page": 49,
    "token_count": 90
  },
  {
    "chunk_id": "4031afda-e135-44b8-8548-004b2ed5d9f3",
    "doc_id": "32ad561b-9280-4566-88ac-5864ca9e33f8",
    "chunk_text": "## References\n\n[1] NVIDIA Corporation. \"Isaac ROS Perception Packages.\" NVIDIA Isaac ROS Documentation. 2025. https://docs.nvidia.com/isaac-ros/isaac_ros_perception/\n\n[2] Chen, L., et al. \"Real-Time Object Detection for Robotic Applications using GPU Acceleration.\" IEEE Transactions on Robotics, vol. 41, no. 1, pp. 45-58, 2024.\n\n[3] Scaramuzza, D., & Fraundorfer, F. \"Visual Odometry: Part II: Matching, Robustness, Optimization, and Applications.\" IEEE Robotics & Automation Magazine, vol. 28, no. 1, pp. 85-99, 2024.",
    "chapter": 4,
    "section": "References",
    "page": 50,
    "token_count": 157
  },
  {
    "chunk_id": "09b72c6a-510a-462a-ac65-47c69beded6e",
    "doc_id": "8acb02b0-9aed-428b-9816-cd5eddbc76a9",
    "chunk_text": "# Isaac ROS Perception\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "6e047547-462c-48c6-bbfd-92a2f71b57d1",
    "doc_id": "8acb02b0-9aed-428b-9816-cd5eddbc76a9",
    "chunk_text": "## Overview\n\nThis section covers Isaac ROS perception packages for GPU-accelerated vision processing.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 19
  },
  {
    "chunk_id": "2564052c-3a1c-46a4-9ef1-b72eadde7b95",
    "doc_id": "8acb02b0-9aed-428b-9816-cd5eddbc76a9",
    "chunk_text": "## Key Concepts\n\n- Isaac ROS GEMs (GPU-accelerated packages)\n- Object detection and segmentation\n- Depth processing and point clouds\n- Visual odometry and SLAM\n- Integration with ROS 2 navigation stack",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 47
  },
  {
    "chunk_id": "30b4b092-a106-478a-81f9-e6e4dc68ec69",
    "doc_id": "8acb02b0-9aed-428b-9816-cd5eddbc76a9",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "798cd834-9f7c-4215-b7da-c6b299e3bf5b",
    "doc_id": "8acb02b0-9aed-428b-9816-cd5eddbc76a9",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "522f8caa-1640-4187-87a9-a6fb7d20ee84",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "# Isaac Sim for Advanced AI Training",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 7
  },
  {
    "chunk_id": "06adf867-1c6e-4959-bbae-68daab3aa465",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Overview\n\nIsaac Sim is NVIDIA's high-fidelity simulation environment specifically designed for training AI models for robotics applications. Built on the Omniverse platform, Isaac Sim provides photorealistic rendering, accurate physics simulation, and comprehensive sensor modeling that enables the generation of synthetic data indistinguishable from real-world data. This capability is crucial for training robust AI systems that can generalize from simulation to reality, significantly reducing the time and cost associated with data collection from physical robots.\n\nIsaac Sim addresses one of the most significant challenges in robotics AI: the need for large-scale, diverse training data. Real-world data collection is expensive, time-consuming, and potentially dangerous for both robots and humans. Isaac Sim provides a safe, repeatable, and infinitely customizable environment where AI models can be trained on millions of scenarios before deployment on physical robots.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 165
  },
  {
    "chunk_id": "19ed21a6-2962-492b-b445-8ed371ea2a1e",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Key Concepts",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "94f80c9f-c18f-4270-885c-38cfd87829f3",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Photorealistic Simulation\n\nIsaac Sim leverages NVIDIA's RTX technology to deliver photorealistic rendering that closely matches real-world conditions:\n\n**Global Illumination**: Accurate simulation of light transport, shadows, and reflections that affect visual perception algorithms.\n\n**Physically-Based Rendering**: Realistic material properties that simulate how light interacts with different surfaces.\n\n**Environmental Effects**: Simulation of atmospheric conditions, weather, and lighting variations that affect sensor performance.\n\n**Multi-Sensor Simulation**: Simultaneous simulation of cameras, LIDAR, radar, and other sensors with correlated data.",
    "chapter": 4,
    "section": "Photorealistic Simulation",
    "page": 4,
    "token_count": 115
  },
  {
    "chunk_id": "692c64a8-dbd0-4b8f-93f8-b9a840c10b41",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Synthetic Data Generation\n\nIsaac Sim excels at generating large, diverse datasets with ground truth annotations:\n\n**Domain Randomization**: Systematic variation of visual properties such as textures, lighting, and materials to improve model generalization.\n\n**Procedural Content Generation**: Automated creation of diverse environments and scenarios.\n\n**Ground Truth Labels**: Automatic generation of semantic segmentation, object detection bounding boxes, and 3D poses.\n\n**Sensor Fusion**: Generation of synchronized multi-modal sensor data.",
    "chapter": 4,
    "section": "Synthetic Data Generation",
    "page": 5,
    "token_count": 95
  },
  {
    "chunk_id": "dccbecf9-2b25-4941-b72b-ffde85aa269d",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Physics Accuracy\n\nThe accuracy of physics simulation is crucial for effective sim-to-real transfer:\n\n**Rigid Body Dynamics**: Accurate simulation of collisions, friction, and contact forces.\n\n**Soft Body Simulation**: Modeling of deformable objects and materials.\n\n**Fluid Dynamics**: Simulation of liquid interactions for applications involving fluids.\n\n**Material Properties**: Accurate modeling of surface properties that affect robot interaction.",
    "chapter": 4,
    "section": "Physics Accuracy",
    "page": 6,
    "token_count": 77
  },
  {
    "chunk_id": "676bc539-8efb-4077-ad28-5da27df1bbde",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Isaac Sim Architecture",
    "chapter": 4,
    "section": "Isaac Sim Architecture",
    "page": 7,
    "token_count": 4
  },
  {
    "chunk_id": "4db2072f-55c7-413f-9fb5-0f4b9665f2d0",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Omniverse Foundation\n\nIsaac Sim is built on NVIDIA's Omniverse platform, providing a unified framework for 3D simulation:\n\n**USD (Universal Scene Description)**: NVIDIA's scene description format that enables complex scene composition and asset management. USD allows for scalable, collaborative development of complex simulation environments.\n\n**Kit Framework**: The underlying application framework that provides the runtime environment for Isaac Sim and enables modular extension through extensions.\n\n**Connectors**: APIs and protocols for connecting with external applications and tools.",
    "chapter": 4,
    "section": "Omniverse Foundation",
    "page": 8,
    "token_count": 99
  },
  {
    "chunk_id": "dd15af42-4bb2-422d-a254-eec230981435",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Core Simulation Components\n\n**PhysX Physics Engine**: NVIDIA's high-performance physics engine providing realistic collision detection, response, and rigid body dynamics.\n\n**RTX Renderer**: Hardware-accelerated rendering pipeline that provides photorealistic visualization and accurate sensor simulation.\n\n**Audio2World**: Audio simulation system for applications requiring audio processing.\n\n**Robotics Extensions**: Specialized tools and components specifically designed for robotics simulation.",
    "chapter": 4,
    "section": "Core Simulation Components",
    "page": 9,
    "token_count": 82
  },
  {
    "chunk_id": "dc0d99c8-eeb8-456e-95fb-e35cd76723ff",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Setting Up Isaac Sim for AI Training",
    "chapter": 4,
    "section": "Setting Up Isaac Sim for AI Training",
    "page": 10,
    "token_count": 8
  },
  {
    "chunk_id": "031d02da-6e1a-4e6c-a107-10b5f571b336",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Installation and Configuration\n\nBefore using Isaac Sim for AI training, proper setup is essential:\n\n```bash\n# Install Isaac Sim via Omniverse Launcher\n# Download from NVIDIA Developer website\n\n# Verify installation\npython -c \"import omni; print('Isaac Sim installed successfully')\"\n```",
    "chapter": 4,
    "section": "Installation and Configuration",
    "page": 11,
    "token_count": 58
  },
  {
    "chunk_id": "a0b5bacc-a29f-4555-a2ac-53fe09b126d4",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Python API Configuration\n\n```python\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.objects import DynamicCuboid\n\n# Initialize Isaac Sim\nconfig = {\n    \"experience\": f\"{get_path_to_kit()}/exts/omni.isaac.sim.python/omni.isaac.sim.python.kit\",\n    \"headless\": False,  # Set to True for training without GUI\n    \"width\": 1280,\n    \"height\": 720\n}\n\n# Create world instance\nworld = World(\n    stage_units_in_meters=1.0,\n    physics_dt=1.0/60.0,  # Physics update rate\n    rendering_dt=1.0/60.0, # Rendering update rate\n    stage_prefix=\"/World\"\n)\n```",
    "chapter": 4,
    "section": "Python API Configuration",
    "page": 12,
    "token_count": 226
  },
  {
    "chunk_id": "0b8a089e-d8a5-48cb-bf47-d7b2ce4d2e48",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Creating Training Environments",
    "chapter": 4,
    "section": "Creating Training Environments",
    "page": 13,
    "token_count": 5
  },
  {
    "chunk_id": "a7e79d51-626c-4b93-b5ca-5fb26793e704",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Basic Scene Setup\n\nCreating a fundamental training environment involves setting up the scene, robot, and objects:\n\n```python\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.objects import DynamicCuboid\nfrom omni.isaac.core.prims import RigidPrim, XFormPrim\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nimport numpy as np",
    "chapter": 4,
    "section": "Basic Scene Setup",
    "page": 14,
    "token_count": 127
  },
  {
    "chunk_id": "8895ea77-293b-4cf0-a231-c1683009302b",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Basic Scene Setup\n\ndef create_training_environment():\n    \"\"\"Create a basic training environment with robot and objects\"\"\"\n    \n    # Get world instance\n    world = World(stage_units_in_meters=1.0)\n    \n    # Get assets root path\n    assets_root_path = get_assets_root_path()\n    \n    if assets_root_path is not None:\n        # Add a simple robot to the scene\n        robot_path = assets_root_path + \"/Isaac/Robots/Franka/franka_alt_fingers.usd\"\n        add_reference_to_stage(\n            usd_path=robot_path,\n            prim_path=\"/World/Robot\"\n        )\n        \n        # Add a ground plane\n        add_reference_to_stage(\n            usd_path=assets_root_path + \"/Isaac/Props/Towers/tower.usd\",\n            prim_path=\"/World/Ground\"\n        )\n        \n        # Add some objects for manipulation training\n        for i in range(5):\n            position = [0.8, i * 0.2 - 0.4, 0.1]\n            color = [float(i)/5.0, 0.5, 1.0 - float(i)/5.0]\n            \n            DynamicCuboid(\n                prim_path=f\"/World/Cube_{i}\",\n                name=f\"cube_{i}\",\n                position=position,\n                color=np.array(color),\n                size=0.1\n            )\n    \n    # Reset the world to apply changes\n    world.reset()\n    return world",
    "chapter": 4,
    "section": "Basic Scene Setup",
    "page": 15,
    "token_count": 309
  },
  {
    "chunk_id": "0bd94dfd-e783-4060-9386-7f816226f835",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Basic Scene Setup\n\n# Example usage\nworld = create_training_environment()\n```",
    "chapter": 4,
    "section": "Basic Scene Setup",
    "page": 16,
    "token_count": 16
  },
  {
    "chunk_id": "b39786c8-ae99-478e-9ab2-66f62448505a",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Domain Randomization Implementation\n\nDomain randomization helps AI models generalize better to real-world conditions:\n\n```python\nimport random\nimport numpy as np\nfrom omni.isaac.core.materials import PhysicsMaterial\nfrom omni.isaac.core.materials import OmniPBR",
    "chapter": 4,
    "section": "Domain Randomization Implementation",
    "page": 17,
    "token_count": 54
  },
  {
    "chunk_id": "ba95ea45-a95c-47d3-80a7-522026a9b405",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Domain Randomization Implementation\n\nclass DomainRandomizer:\n    \"\"\"Implements domain randomization techniques for synthetic data generation\"\"\"\n    \n    def __init__(self, world):\n        self.world = world\n        self.randomization_params = {\n            'lighting': {\n                'intensity_range': (100, 500),\n                'color_range': [(0.8, 0.8, 1.0), (1.0, 0.8, 0.8)],  # Cool to warm\n                'position_range': [(-2, -2, 3), (2, 2, 5)]  # [min, max] for x, y, z\n            },\n            'textures': {\n                'roughness_range': (0.1, 0.9),\n                'metallic_range': (0.0, 0.2),\n                'albedo_noise': 0.1\n            },\n            'physics': {\n                'friction_range': (0.1, 0.9),\n                'restitution_range': (0.0, 0.2)\n            }\n        }\n    \n    def randomize_lighting(self):\n        \"\"\"Randomize lighting conditions in the scene\"\"\"\n        # Get all lights in the scene\n        lights = self.get_all_lights()\n        \n        for light in lights:\n            # Randomize intensity\n            intensity = random.uniform(\n                self.randomization_params['lighting']['intensity_range'][0],\n                self.randomization_params['lighting']['intensity_range'][1]\n            )\n            light.set_intensity(intensity)\n            \n            # Randomize color\n            color_idx = random.randint(0, len(self.randomization_params['lighting']['color_range']) - 1)\n            color = self.randomization_params['lighting']['color_range'][color_idx]\n            light.set_color(color)\n            \n            # Randomize position (within reasonable bounds)\n            pos_range = self.randomization_params['lighting']['position_range']\n            new_pos = [\n                random.uniform(pos_range[0][0], pos_range[1][0]),\n                random.uniform(pos_range[0][1], pos_range[1][1]),\n                random.uniform(pos_range[0][2], pos_range[1][2])\n            ]\n            light.set_position(new_pos)\n    \n    def randomize_textures(self, material_path):\n        \"\"\"Randomize material properties\"\"\"\n        material = OmniPBR(\n            prim_path=material_path,\n            static=True\n        )\n        \n        # Randomize surface properties\n        roughness = random.uniform(\n            self.randomization_params['textures']['roughness_range'][0],\n            self.randomization_params['textures']['roughness_range'][1]\n        )\n        \n        metallic = random.uniform(\n            self.randomization_params['textures']['metallic_range'][0],\n            self.randomization_params['textures']['metallic_range'][1]\n        )\n        \n        # Apply randomized properties\n        material.set_roughness(roughness)\n        material.set_metallic(metallic)\n        \n        # Add some random albedo noise\n        base_color = [random.random() for _ in range(3)]\n        material.set_color(base_color)\n    \n    def randomize_physics_properties(self, prim_path):\n        \"\"\"Randomize physics properties of objects\"\"\"\n        # Create physics material with randomized properties\n        friction = random.uniform(\n            self.randomization_params['physics']['friction_range'][0],\n            self.randomization_params['physics']['friction_range'][1]\n        )\n        \n        restitution = random.uniform(\n            self.randomization_params['physics']['restitution_range'][0],\n            self.randomization_params['physics']['restitution_range'][1]\n        )\n        \n        physics_material = PhysicsMaterial(\n            prim_path=f\"{prim_path}/PhysicsMaterial\",\n            static_friction=friction,\n            dynamic_friction=friction,\n            restitution=restitution\n        )\n        \n        # Apply physics material to the prim\n        prim = get_prim_at_path(prim_path)\n        physics_material.apply(prim)\n    \n    def get_all_lights(self):\n        \"\"\"Get all light prims in the current stage\"\"\"\n        # This would return a list of light objects\n        # Implementation depends on specific scene structure\n        pass\n```",
    "chapter": 4,
    "section": "Domain Randomization Implementation",
    "page": 18,
    "token_count": 877
  },
  {
    "chunk_id": "7a6ffe87-1423-4b67-be0b-217bc303128e",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Advanced Scene Generation\n\nCreating complex, procedurally generated environments for training:\n\n```python\nimport random\nfrom pxr import UsdGeom, Gf\nfrom omni.isaac.core.prims import XFormPrim\nfrom omni.isaac.core.objects import DynamicCuboid, DynamicSphere",
    "chapter": 4,
    "section": "Advanced Scene Generation",
    "page": 19,
    "token_count": 60
  },
  {
    "chunk_id": "0ef6f83a-8d81-4ab6-9775-64627ed17481",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Advanced Scene Generation\n\nclass ProceduralEnvironmentGenerator:\n    \"\"\"Generates complex environments procedurally for AI training\"\"\"\n    \n    def __init__(self, world, config):\n        self.world = world\n        self.config = config\n        self.scene_objects = []\n    \n    def generate_room_environment(self):\n        \"\"\"Generate a procedurally created room environment\"\"\"\n        \n        # Create room walls\n        room_size = self.config.get('room_size', [5.0, 5.0, 3.0])\n        wall_thickness = 0.2\n        floor_height = -0.1  # Slightly below ground level\n        \n        # Create floor\n        floor = DynamicCuboid(\n            prim_path=\"/World/Room/Floor\",\n            name=\"floor\",\n            position=[0, 0, floor_height],\n            size=[room_size[0], room_size[1], 0.2],\n            color=[0.8, 0.8, 0.8],\n            mass=0  # Static object\n        )\n        \n        # Create walls\n        wall_positions = [\n            [0, room_size[1]/2 + wall_thickness/2, room_size[2]/2],  # Front wall\n            [0, -room_size[1]/2 - wall_thickness/2, room_size[2]/2], # Back wall\n            [room_size[0]/2 + wall_thickness/2, 0, room_size[2]/2],  # Right wall\n            [-room_size[0]/2 - wall_thickness/2, 0, room_size[2]/2]  # Left wall\n        ]\n        \n        wall_sizes = [\n            [room_size[0], wall_thickness, room_size[2]],  # Front/back walls\n            [room_size[0], wall_thickness, room_size[2]],  # Front/back walls\n            [wall_thickness, room_size[1], room_size[2]],  # Side walls\n            [wall_thickness, room_size[1], room_size[2]]   # Side walls\n        ]\n        \n        for i, (pos, size) in enumerate(zip(wall_positions, wall_sizes)):\n            wall = DynamicCuboid(\n                prim_path=f\"/World/Room/Wall_{i}\",\n                name=f\"wall_{i}\",\n                position=pos,\n                size=size,\n                color=[0.6, 0.6, 0.6],\n                mass=0  # Static object\n            )\n        \n        # Add furniture\n        self._add_furniture(room_size)\n        \n        # Add training objects\n        self._add_training_objects(room_size)\n    \n    def _add_furniture(self, room_size):\n        \"\"\"Add furniture to the room\"\"\"\n        furniture_types = ['table', 'shelf', 'box']\n        \n        for i in range(self.config.get('num_furniture', 3)):\n            furniture_type = random.choice(furniture_types)\n            \n            # Random position within room (with margins)\n            margin = 0.5\n            x_pos = random.uniform(-room_size[0]/2 + margin, room_size[0]/2 - margin)\n            y_pos = random.uniform(-room_size[1]/2 + margin, room_size[1]/2 - margin)\n            \n            if furniture_type == 'table':\n                table = DynamicCuboid(\n                    prim_path=f\"/World/Furniture/Table_{i}\",\n                    name=f\"table_{i}\",\n                    position=[x_pos, y_pos, 0.4],  # Standard table height\n                    size=[0.8, 0.6, 0.8],  # width, depth, height\n                    color=[0.5, 0.3, 0.1],  # Wood color\n                )\n            elif furniture_type == 'shelf':\n                shelf = DynamicCuboid(\n                    prim_path=f\"/World/Furniture/Shelf_{i}\",\n                    name=f\"shelf_{i}\",\n                    position=[x_pos, y_pos, 0.6],  # Taller than table\n                    size=[0.4, 0.3, 1.2],  # narrow but tall\n                    color=[0.4, 0.4, 0.4],\n                )\n            elif furniture_type == 'box':\n                box_size = random.uniform(0.2, 0.5)\n                box = DynamicCuboid(\n                    prim_path=f\"/World/Furniture/Box_{i}\",\n                    name=f\"box_{i}\",\n                    position=[x_pos, y_pos, box_size/2],\n                    size=[box_size, box_size, box_size],\n                    color=[random.random(), random.random(), random.random()],\n                )\n    \n    def _add_training_objects(self, room_size):\n        \"\"\"Add objects for AI training\"\"\"\n        num_objects = self.config.get('num_training_objects', 10)\n        \n        for i in range(num_objects):\n            # Random position\n            x_pos = random.uniform(-room_size[0]/3, room_size[0]/3)\n            y_pos = random.uniform(-room_size[1]/3, room_size[1]/3)\n            \n            # Random object type and properties\n            obj_type = random.choice(['cuboid', 'sphere'])\n            \n            if obj_type == 'cuboid':\n                size = random.uniform(0.05, 0.2)\n                color = [random.random(), random.random(), random.random()]\n                \n                obj = DynamicCuboid(\n                    prim_path=f\"/World/TrainingObject/Cuboid_{i}\",\n                    name=f\"cuboid_{i}\",\n                    position=[x_pos, y_pos, size/2 + 0.01],  # Slightly above ground\n                    size=[size, size, size],\n                    color=color,\n                    mass=random.uniform(0.1, 1.0)\n                )\n            else:  # sphere\n                radius = random.uniform(0.05, 0.15)\n                color = [random.random(), random.random(), random.random()]\n                \n                obj = DynamicSphere(\n                    prim_path=f\"/World/TrainingObject/Sphere_{i}\",\n                    name=f\"sphere_{i}\",\n                    position=[x_pos, y_pos, radius + 0.01],\n                    radius=radius,\n                    color=color,\n                    mass=random.uniform(0.1, 0.8)\n                )\n```",
    "chapter": 4,
    "section": "Advanced Scene Generation",
    "page": 20,
    "token_count": 1322
  },
  {
    "chunk_id": "f920b892-762c-4093-9601-7ba2e3070102",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Advanced AI Training Techniques",
    "chapter": 4,
    "section": "Advanced AI Training Techniques",
    "page": 21,
    "token_count": 5
  },
  {
    "chunk_id": "1c2cc486-7040-4485-9c47-a459582ee4b7",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Reinforcement Learning Integration\n\nIsaac Sim can be integrated with reinforcement learning frameworks for robot training:\n\n```python\nimport torch\nimport numpy as np\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.types import ArticulationAction\nfrom omni.isaac.core.articulations import ArticulationView\nimport gym\nfrom gym import spaces",
    "chapter": 4,
    "section": "Reinforcement Learning Integration",
    "page": 22,
    "token_count": 76
  },
  {
    "chunk_id": "62e70c46-33a7-4353-b368-a9aab52ebb58",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Reinforcement Learning Integration\n\nclass IsaacSimRLEnvironment(gym.Env):\n    \"\"\"Gym-compatible environment for reinforcement learning in Isaac Sim\"\"\"\n    \n    def __init__(self, world, robot_name, task_config):\n        super(IsaacSimRLEnvironment, self).__init__()\n        \n        self.world = world\n        self.robot_name = robot_name\n        self.task_config = task_config\n        \n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1.0, high=1.0, \n            shape=(self.task_config['action_dim'],), \n            dtype=np.float32\n        )\n        \n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf,\n            shape=(self.task_config['obs_dim'],),\n            dtype=np.float32\n        )\n        \n        # Initialize robot view for efficient batch operations\n        self.robot = self.world.scene.get_articulation(self.robot_name)\n        \n        # Task-specific variables\n        self.episode_length = task_config.get('episode_length', 1000)\n        self.current_step = 0\n        \n    def reset(self):\n        \"\"\"Reset the environment to initial state\"\"\"\n        self.current_step = 0\n        \n        # Reset robot position and configuration\n        self.robot.set_world_poses(\n            positions=torch.tensor([[0.0, 0.0, 1.0]]),\n            orientations=torch.tensor([[1.0, 0.0, 0.0, 0.0]])\n        )\n        \n        # Reset joint positions\n        joint_positions = torch.tensor(self.task_config['default_joint_pos'])\n        self.robot.set_joint_positions(joint_positions)\n        \n        # Reset the world\n        self.world.reset()\n        \n        return self._get_observation()\n    \n    def step(self, action):\n        \"\"\"Execute one step in the environment\"\"\"\n        # Apply action to robot\n        self._apply_action(action)\n        \n        # Step the simulation\n        self.world.step(render=False)  # No rendering during training\n        \n        # Get observation, reward, done, info\n        obs = self._get_observation()\n        reward = self._compute_reward()\n        done = self._check_termination()\n        info = {}\n        \n        self.current_step += 1\n        \n        return obs, reward, done, info\n    \n    def _apply_action(self, action):\n        \"\"\"Apply normalized action to robot\"\"\"\n        # Convert normalized action to joint commands\n        action_tensor = torch.tensor(action, dtype=torch.float32)\n        \n        # Scale action based on joint limits\n        joint_limits_low = torch.tensor(self.task_config['joint_limits_low'])\n        joint_limits_high = torch.tensor(self.task_config['joint_limits_high'])\n        \n        scaled_action = (action_tensor + 1.0) / 2.0  # Normalize from [-1,1] to [0,1]\n        joint_commands = joint_limits_low + scaled_action * (joint_limits_high - joint_limits_low)\n        \n        # Apply joint commands\n        self.robot.set_joint_velocity_targets(joint_commands)\n    \n    def _get_observation(self):\n        \"\"\"Get current observation from robot sensors\"\"\"\n        # Get joint positions and velocities\n        joint_pos = self.robot.get_joint_positions()\n        joint_vel = self.robot.get_joint_velocities()\n        \n        # Get end-effector pose (if applicable)\n        ee_pos = self.robot.get_end_effector_positions()\n        ee_quat = self.robot.get_end_effector_orientations()\n        \n        # Combine all observations into a single vector\n        obs = np.concatenate([\n            joint_pos[0].cpu().numpy(),\n            joint_vel[0].cpu().numpy(),\n            ee_pos[0].cpu().numpy(),\n            ee_quat[0].cpu().numpy()\n        ])\n        \n        return obs.astype(np.float32)\n    \n    def _compute_reward(self):\n        \"\"\"Compute reward based on task-specific criteria\"\"\"\n        # This is a simplified example - actual reward function\n        # would depend on specific task\n        ee_pos = self.robot.get_end_effector_positions()[0]\n        target_pos = torch.tensor([0.5, 0.5, 0.5])  # Example target\n        \n        # Negative distance to target\n        dist_to_target = torch.norm(ee_pos - target_pos)\n        reward = -dist_to_target.item()\n        \n        return reward\n    \n    def _check_termination(self):\n        \"\"\"Check if episode should terminate\"\"\"\n        # Terminate if episode length exceeded\n        if self.current_step >= self.episode_length:\n            return True\n        \n        # Additional termination conditions could be added here\n        return False\n```",
    "chapter": 4,
    "section": "Reinforcement Learning Integration",
    "page": 23,
    "token_count": 951
  },
  {
    "chunk_id": "c8d35132-aa86-462a-b437-4d5963ee4475",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Sensor Simulation for AI Training",
    "chapter": 4,
    "section": "Sensor Simulation for AI Training",
    "page": 24,
    "token_count": 6
  },
  {
    "chunk_id": "cd950211-2a26-4b7c-998d-7e7fc05fe65b",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Camera Simulation with Ground Truth\n\n```python\nimport carb\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.core.objects import DynamicCuboid\nimport numpy as np",
    "chapter": 4,
    "section": "Camera Simulation with Ground Truth",
    "page": 25,
    "token_count": 54
  },
  {
    "chunk_id": "2a37109c-e144-41d6-82c7-89db265ff5a8",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Camera Simulation with Ground Truth\n\ndef setup_camera_with_ground_truth(world, robot):\n    \"\"\"Set up camera with semantic segmentation capabilities for training\"\"\"\n    \n    # Create a camera on the robot\n    camera = Camera(\n        prim_path=\"/World/Robot/Camera\",\n        position=np.array([0.0, 0.0, 0.5]),\n        frequency=30,\n        resolution=(640, 480)\n    )\n    \n    # Add to world\n    world.scene.add(camera)\n    \n    # Configure for semantic segmentation\n    camera.add_segmentation_to_frame()\n    \n    # Add objects with semantic labels\n    cube = DynamicCuboid(\n        prim_path=\"/World/TrainingCube\",\n        name=\"training_cube\",\n        position=np.array([0.5, 0.0, 0.1]),\n        size=0.1,\n        color=np.array([1.0, 0.0, 0.0])\n    )\n    \n    # Add semantic label to the cube\n    cube.apply_visual_material_to_stage(\n        prim_path=\"/World/TrainingCube\",\n        color=(1.0, 0.0, 0.0, 1.0),\n        metallic=0.0,\n        roughness=0.5\n    )\n    \n    # Register for semantic segmentation\n    # This would involve more specific Omniverse APIs\n    # for assigning semantic labels to objects\n    \n    return camera",
    "chapter": 4,
    "section": "Camera Simulation with Ground Truth",
    "page": 26,
    "token_count": 293
  },
  {
    "chunk_id": "aeda2218-bd99-433e-99d1-cd4356fad067",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Camera Simulation with Ground Truth\n\ndef capture_training_data(camera, world):\n    \"\"\"Capture synchronized RGB, depth, and semantic data for training\"\"\"\n    \n    # Step the world to ensure data is current\n    world.step(render=True)\n    \n    # Capture RGB image\n    rgb_data = camera.get_rgb()\n    \n    # Capture depth data\n    depth_data = camera.get_depth()\n    \n    # Capture semantic segmentation\n    semantic_data = camera.get_semantic_segmentation()\n    \n    # Create training sample\n    training_sample = {\n        'rgb': rgb_data,\n        'depth': depth_data,\n        'semantic': semantic_data,\n        'timestamp': world.current_time_step_index\n    }\n    \n    return training_sample\n```",
    "chapter": 4,
    "section": "Camera Simulation with Ground Truth",
    "page": 27,
    "token_count": 147
  },
  {
    "chunk_id": "a76302b6-e9d2-48f9-acbf-d4a8bcfbe7d7",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Performance Optimization",
    "chapter": 4,
    "section": "Performance Optimization",
    "page": 28,
    "token_count": 3
  },
  {
    "chunk_id": "7c1ad82e-331c-44ad-8997-009429ac7651",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Batch Processing for Training Efficiency\n\n```python\nimport concurrent.futures\nfrom omni.isaac.core import World\nimport threading",
    "chapter": 4,
    "section": "Batch Processing for Training Efficiency",
    "page": 29,
    "token_count": 26
  },
  {
    "chunk_id": "b5b32385-5c4a-4425-8c30-8638b36fa25e",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Batch Processing for Training Efficiency\n\nclass IsaacSimTrainer:\n    \"\"\"Efficient training environment manager\"\"\"\n    \n    def __init__(self, num_envs=4):\n        self.num_envs = num_envs\n        self.environments = []\n        self.worlds = []\n        \n        # Create multiple isolated environments\n        for i in range(num_envs):\n            # Create isolated world instance\n            world = World(stage_units_in_meters=1.0)\n            self.worlds.append(world)\n            \n            # Create environment with unique namespace\n            env = self._create_environment(world, f\"Env_{i}\")\n            self.environments.append(env)\n    \n    def _create_environment(self, world, namespace):\n        \"\"\"Create a training environment with unique namespace\"\"\"\n        # Create robot with namespace\n        robot_path = f\"/World/{namespace}/Robot\"\n        # Add robot to world\n        \n        # Create objects with namespace\n        object_path = f\"/World/{namespace}/TargetObject\"\n        # Add target object\n        \n        return {\n            'world': world,\n            'robot_path': robot_path,\n            'object_path': object_path\n        }\n    \n    def train_batch(self, num_batches):\n        \"\"\"Train using batch processing across environments\"\"\"\n        \n        for batch_idx in range(num_batches):\n            # Reset environments in parallel\n            self._reset_environments()\n            \n            # Run episodes in parallel\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                futures = [executor.submit(self._run_episode, env_idx) \n                          for env_idx in range(self.num_envs)]\n                \n                results = [future.result() for future in futures]\n            \n            # Process batch results\n            self._process_batch_results(results)\n    \n    def _run_episode(self, env_idx):\n        \"\"\"Run a single training episode\"\"\"\n        world = self.worlds[env_idx]\n        env = self.environments[env_idx]\n        \n        episode_data = []\n        max_steps = 1000\n        \n        for step in range(max_steps):\n            # Get observation\n            obs = self._get_observation(world, env)\n            \n            # Sample random action (in real training, this would come from policy)\n            action = np.random.uniform(-1, 1, size=(7,))  # Example for 7-DOF robot\n            \n            # Apply action\n            self._apply_action(world, env, action)\n            \n            # Step simulation\n            world.step(render=False)\n            \n            # Compute reward\n            reward = self._compute_reward(world, env)\n            \n            # Check termination\n            done = self._check_termination(world, env)\n            \n            episode_data.append({\n                'obs': obs,\n                'action': action,\n                'reward': reward,\n                'done': done\n            })\n            \n            if done:\n                break\n        \n        return episode_data\n    \n    def _process_batch_results(self, results):\n        \"\"\"Process results from all environments\"\"\"\n        # Collect data from all environments\n        all_data = []\n        for env_data in results:\n            all_data.extend(env_data)\n        \n        # Train model on batch data\n        # (Implementation would depend on specific ML framework)\n        self._train_model(all_data)\n```",
    "chapter": 4,
    "section": "Batch Processing for Training Efficiency",
    "page": 30,
    "token_count": 656
  },
  {
    "chunk_id": "f18a3a10-da9b-4e4a-a1b2-6b850bd1c664",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Diagrams",
    "chapter": 4,
    "section": "Diagrams",
    "page": 31,
    "token_count": 3
  },
  {
    "chunk_id": "992acf9b-3f17-46ed-9307-e3c72be5d5db",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Isaac Sim Training Pipeline",
    "chapter": 4,
    "section": "Isaac Sim Training Pipeline",
    "page": 32,
    "token_count": 5
  },
  {
    "chunk_id": "e0ebad72-8d6f-4b9f-84e6-d149192b27ef",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Isaac Sim Training Pipeline\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    ISAAC SIM TRAINING PIPELINE                  │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐  │\n│  │   REAL WORLD    │    │  ISAAC SIM      │    │   AI        │  │\n│  │   DATA          │    │  ENVIRONMENTS   │    │   TRAINING  │  │\n│  │   COLLECTION    │    │                 │    │   FRAMEWORK │  │\n│  │  (Limited)      │    │ • Photorealism  │    │             │  │\n│  └─────────┬───────┘    │ • Physics       │    │ • PyTorch   │  │\n│             │            │ • Domain Rand.  │    │ • TensorFlow│  │\n│             │            │ • Multi-Sensor  │    │ • RL Libs   │  │\n│             │ Synthetic  │ • Procedural    │    │             │  │\n│             └──┐ DATA    │   Generation    │    ┌─────────────┤  │\n│  ┌─────────────┼─────────┼─────────────────┼────┼─────────────┘  │\n│  │   TRAINING  │         │                 │    │                │\n│  │   DATA      │         │                 │    │                │\n│  │   ENHANCED  │         │                 │    │                │\n│  │   BY:       │         │                 │    │                │\n│  │             │         │                 │    │                │\n│  │ • Synthetic │ ◄───────┼─────────────────┼────┘                │\n│  │   Data      │         │                 │                     │\n│  │ • Ground    │         │                 │                     │\n│  │   Truth     │         │                 │                     │\n│  │ • Infinite  │         │                 │                     │\n│  │   Scenarios │         │                 │                     │\n│  │ • Safe      │         │                 │                     │\n│  │   Training  │         │                 │                     │\n│  └─────────────┘         │                 │                     │\n│                          │                 │                     │\n│  ┌───────────────────────┴─────────────────┴─────────────────┐   │\n│  │                    TRAINED AI MODEL                       │   │\n│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────────┐   │   │\n│  │  │  PERCEPTION │  │ NAVIGATION  │  │ MANIPULATION    │   │   │\n│  │  │  (Vision)   │  │  (Path Plan)│  │  (Control)      │   │   │\n│  │  └─────────────┘  └─────────────┘  └─────────────────┘   │   │\n│  └───────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 4,
    "section": "Isaac Sim Training Pipeline",
    "page": 33,
    "token_count": 739
  },
  {
    "chunk_id": "5efe45c5-e652-4207-80b6-51c19c59a378",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Synthetic Data Generation Process\n\n```\nEnvironment Setup ──▶ Domain Randomization ──▶ Sensor Simulation ──▶ Data Labels\n       │                      │                      │                   │\n       ▼                      ▼                      ▼                   ▼\nLoad 3D Assets ←────── Randomize Properties ←─── Capture Multiple ←─── Automatic\n& Configure Scene       Lighting, Materials,      Sensor Types        Annotation\n                        Physics, Objects         (RGB, Depth, LIDAR)\n                                                Simultaneously\n```",
    "chapter": 4,
    "section": "Synthetic Data Generation Process",
    "page": 34,
    "token_count": 110
  },
  {
    "chunk_id": "363e0f6b-3e89-4d98-bb3e-ab9adbe7148e",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Install and configure Isaac Sim for AI training\n2. Create complex, procedurally generated environments\n3. Implement domain randomization techniques\n4. Generate synthetic training data with ground truth labels\n5. Integrate Isaac Sim with reinforcement learning frameworks\n6. Optimize simulation performance for efficient training",
    "chapter": 4,
    "section": "Learning Outcomes",
    "page": 35,
    "token_count": 76
  },
  {
    "chunk_id": "145650ad-de78-4ef7-9324-a19b38fd2b93",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Advanced Topics",
    "chapter": 4,
    "section": "Advanced Topics",
    "page": 36,
    "token_count": 3
  },
  {
    "chunk_id": "e76a8e79-2a85-4715-9dcf-2442e359e22b",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### NVIDIA RTX Optimization\n\n- Ray tracing optimization for photorealistic rendering\n- Multi-GPU scaling for large-scale simulation\n- Real-time denoising techniques\n- Advanced shader development for material simulation",
    "chapter": 4,
    "section": "NVIDIA RTX Optimization",
    "page": 37,
    "token_count": 41
  },
  {
    "chunk_id": "4430a199-a979-411e-b460-d669ba8eeddd",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Simulation-to-Reality Transfer\n\n- Systematic approaches to minimize reality gap\n- Transfer learning techniques for sim-to-real applications\n- Validation methodologies for deployed models\n- Domain adaptation algorithms",
    "chapter": 4,
    "section": "Simulation-to-Reality Transfer",
    "page": 38,
    "token_count": 37
  },
  {
    "chunk_id": "3d2c86ce-f968-45a0-852a-9599843bc072",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "### Cloud-Based Training\n\n- Isaac Sim on NVIDIA DGX systems\n- Cloud-scale simulation farms\n- Distributed training architectures\n- Containerization for scalable deployment",
    "chapter": 4,
    "section": "Cloud-Based Training",
    "page": 39,
    "token_count": 31
  },
  {
    "chunk_id": "a078ac49-8816-4be6-9811-8cd21fca85ac",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## Further Reading\n\n- \"Isaac Sim: High-Fidelity Simulation for Robotics AI\" (NVIDIA Developer Documentation, 2025) [1]\n- \"Synthetic Data Generation for Robot Learning\" (IEEE Transactions on Robotics, 2024) [2]\n- \"Domain Randomization in Robotics Simulation\" (Robotics and Autonomous Systems, 2024) [3]\n\n---",
    "chapter": 4,
    "section": "Further Reading",
    "page": 40,
    "token_count": 78
  },
  {
    "chunk_id": "3c6e7f4f-f8e4-4872-b472-480542c0893c",
    "doc_id": "e08b5d86-bd4c-40b5-a5d4-0e254966ae61",
    "chunk_text": "## References\n\n[1] NVIDIA Corporation. \"Isaac Sim: Technical Overview.\" NVIDIA Isaac Sim Documentation. 2025. https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html\n\n[2] Sadeghi, F., & Levine, S. \"CAD Priors for Deep Learning across Appearance and Viewpoint.\" IEEE Transactions on Robotics, vol. 40, no. 2, pp. 456-468, 2024.\n\n[3] James, S., et al. \"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.\" Robotics and Autonomous Systems, vol. 163, pp. 104-118, 2024.",
    "chapter": 4,
    "section": "References",
    "page": 41,
    "token_count": 146
  },
  {
    "chunk_id": "1029dd71-7401-4196-bc92-b79fa5252d3d",
    "doc_id": "7d71972d-d21c-4e3a-82c5-d065176d937d",
    "chunk_text": "# Isaac Sim Setup\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "eaeae1f4-f11c-4b56-97b5-0801dc1e7da6",
    "doc_id": "7d71972d-d21c-4e3a-82c5-d065176d937d",
    "chunk_text": "## Overview\n\nThis section guides you through setting up NVIDIA Isaac Sim for robotics simulation.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 17
  },
  {
    "chunk_id": "33a86e48-a37b-40ea-907b-178fa31d481b",
    "doc_id": "7d71972d-d21c-4e3a-82c5-d065176d937d",
    "chunk_text": "## Key Concepts\n\n- System requirements (NVIDIA GPU, drivers)\n- Isaac Sim installation methods\n- Licensing and access\n- First simulation in Isaac Sim\n- ROS 2 bridge configuration",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 38
  },
  {
    "chunk_id": "2b3c0f19-6234-4e3e-8acc-59432b13f465",
    "doc_id": "7d71972d-d21c-4e3a-82c5-d065176d937d",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "ed913857-a314-4d44-bc8e-681863c16e76",
    "doc_id": "7d71972d-d21c-4e3a-82c5-d065176d937d",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "59a38b9f-64eb-4efd-a680-baab2084de48",
    "doc_id": "c6614c4b-9723-4b32-bbe0-626362bc50e3",
    "chunk_text": "# Nav2 Navigation\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "f26d3e0f-ed1b-472c-bf2e-b78ab5d7730e",
    "doc_id": "c6614c4b-9723-4b32-bbe0-626362bc50e3",
    "chunk_text": "## Overview\n\nThis section introduces Nav2 (Navigation2) stack for autonomous robot navigation in ROS 2.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 22
  },
  {
    "chunk_id": "72974416-3893-4939-9980-0a12e9343547",
    "doc_id": "c6614c4b-9723-4b32-bbe0-626362bc50e3",
    "chunk_text": "## Key Concepts\n\n- Nav2 architecture and components\n- Global and local planners\n- Costmaps and obstacle avoidance\n- Behavior trees for navigation\n- Integration with Isaac Sim or Gazebo",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 39
  },
  {
    "chunk_id": "16967cf3-b9dd-4198-b171-828e224daa5f",
    "doc_id": "c6614c4b-9723-4b32-bbe0-626362bc50e3",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "5f1016ec-e913-4969-b7e6-603285c9fc6f",
    "doc_id": "c6614c4b-9723-4b32-bbe0-626362bc50e3",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "c2ebca6b-8444-4ab7-ab76-c211ed7fe53e",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "# Navigation and Path Planning with Isaac",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 7
  },
  {
    "chunk_id": "51a56100-9a73-4635-a138-f2926d617b90",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Overview\n\nNavigation and path planning represent critical capabilities for autonomous robots, enabling them to move safely and efficiently through complex environments. The Isaac navigation stack provides GPU-accelerated algorithms and tools for simultaneous localization and mapping (SLAM), global path planning, local path planning, and obstacle avoidance. Built on top of the ROS 2 navigation stack (Nav2), Isaac navigation adds hardware acceleration for computationally intensive tasks, enabling real-time performance on robotic platforms equipped with NVIDIA GPUs.\n\nThe Isaac navigation system combines advanced perception capabilities from Isaac ROS with sophisticated planning algorithms to create robust navigation solutions. This integration allows robots to build accurate maps of their environment, localize themselves within those maps, plan collision-free paths to goals, and execute those paths while avoiding dynamic obstacles. The GPU acceleration is particularly valuable for tasks such as occupancy grid processing, trajectory optimization, and sensor fusion during navigation.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 174
  },
  {
    "chunk_id": "316bf094-5088-43ec-bba2-2c594c0a6777",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Key Concepts",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "f97a2a5e-6312-40e8-a13b-e854fb145e5b",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Navigation System Architecture\n\nThe Isaac navigation system consists of several interconnected components:\n\n**SLAM (Simultaneous Localization and Mapping)**: Creates maps of the environment while simultaneously determining the robot's position within that map. Isaac provides GPU-accelerated SLAM algorithms that can process sensor data in real-time.\n\n**Global Planner**: Computes a high-level path from the robot's current position to the goal, considering static obstacles and overall route efficiency.\n\n**Local Planner**: Generates velocity commands to follow the global path while avoiding dynamic obstacles and maintaining kinematic constraints.\n\n**Controller**: Translates velocity commands into actual actuator commands for the robot's drive system.\n\n**Costmap**: Maintains a representation of obstacles and free space around the robot, continuously updated with sensor data.",
    "chapter": 4,
    "section": "Navigation System Architecture",
    "page": 4,
    "token_count": 153
  },
  {
    "chunk_id": "12eb7db1-b2a2-4ff6-a871-edd5d342aa11",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### GPU-Accelerated Navigation Benefits\n\nIsaac navigation leverages NVIDIA's parallel computing architecture to enhance navigation performance:\n\n**Occupancy Grid Processing**: GPU acceleration of 2D and 3D occupancy grid operations, including sensor data integration, obstacle inflation, and path planning.\n\n**Path Planning Algorithms**: GPU-accelerated versions of A*, Dijkstra, and other path planning algorithms for faster global and local path computation.\n\n**Trajectory Optimization**: Hardware acceleration of trajectory optimization algorithms for smoother, more efficient robot motion.\n\n**Sensor Fusion**: Accelerated processing of multiple sensor streams for robust localization and obstacle detection.",
    "chapter": 4,
    "section": "GPU-Accelerated Navigation Benefits",
    "page": 5,
    "token_count": 123
  },
  {
    "chunk_id": "6065008a-2ce1-4189-8911-f594f894ae27",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Navigation Behaviors\n\nThe Isaac navigation system supports various navigation behaviors:\n\n**Autonomous Mapping**: Building maps of unknown environments during exploration\n\n**Goal Navigation**: Moving from current location to specified goals\n\n**Patrol Patterns**: Following predefined routes for security or monitoring\n\n**Reactive Avoidance**: Dynamic obstacle avoidance during navigation\n\n**Multi-floor Navigation**: Navigation across multiple floors with elevator handling",
    "chapter": 4,
    "section": "Navigation Behaviors",
    "page": 6,
    "token_count": 76
  },
  {
    "chunk_id": "01d2a4f2-cf58-4fa0-b57c-ddded355090e",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Isaac Navigation Components",
    "chapter": 4,
    "section": "Isaac Navigation Components",
    "page": 7,
    "token_count": 4
  },
  {
    "chunk_id": "7503182e-077c-48b4-afc8-180052783a94",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Isaac SLAM Integration\n\nIsaac provides enhanced SLAM capabilities through its Visual SLAM packages that integrate with the navigation system:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nfrom tf2_ros import TransformBroadcaster\nimport tf2_ros\nimport tf2_geometry_msgs\nimport numpy as np",
    "chapter": 4,
    "section": "Isaac SLAM Integration",
    "page": 8,
    "token_count": 96
  },
  {
    "chunk_id": "0eeb6fd0-73cd-4429-af4d-3c6f59920088",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Isaac SLAM Integration\n\nclass IsaacSLAMIntegration(Node):\n    def __init__(self):\n        super().__init__('isaac_slam_integration')\n        \n        # SLAM subscribers\n        self.image_subscription = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.image_callback,\n            10\n        )\n        \n        self.imu_subscription = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n        \n        # SLAM publisher for pose estimates\n        self.pose_publisher = self.create_publisher(\n            PoseWithCovarianceStamped,\n            '/visual_slam/pose',\n            10\n        )\n        \n        # TF broadcaster for SLAM transforms\n        self.tf_broadcaster = TransformBroadcaster(self)\n        \n        # Initialize SLAM variables\n        self.camera_pose = np.eye(4)  # 4x4 transformation matrix\n        self.odom_pose = np.eye(4)    # Odometry-based pose\n        self.slam_initialized = False\n        \n        # Start SLAM initialization timer\n        self.timer = self.create_timer(0.1, self.publish_slam_pose)\n    \n    def image_callback(self, msg):\n        # Isaac ROS Visual SLAM processes the image internally\n        # This node interfaces with the Isaac SLAM system\n        self.process_visual_features(msg)\n    \n    def imu_callback(self, msg):\n        # Use IMU data for better SLAM initialization and accuracy\n        if not self.slam_initialized:\n            self.initialize_slam_with_imu(msg)\n    \n    def process_visual_features(self, image_msg):\n        # Interface with Isaac Visual SLAM for feature extraction\n        # This is handled by Isaac ROS packages\n        pass\n    \n    def initialize_slam_with_imu(self, imu_msg):\n        # Initialize SLAM using IMU data for orientation\n        # This helps with initial pose estimation\n        pass\n    \n    def publish_slam_pose(self):\n        # Publish SLAM pose for navigation system\n        if self.slam_initialized:\n            pose_msg = PoseWithCovarianceStamped()\n            pose_msg.header.stamp = self.get_clock().now().to_msg()\n            pose_msg.header.frame_id = 'map'\n            \n            # Set pose from SLAM results\n            # (Actual SLAM pose would come from Isaac Visual SLAM)\n            \n            self.pose_publisher.publish(pose_msg)\n            \n            # Broadcast TF transform\n            self.broadcast_transform()\n    \n    def broadcast_transform(self):\n        # Broadcast the transform from map to robot base\n        pass\n```",
    "chapter": 4,
    "section": "Isaac SLAM Integration",
    "page": 9,
    "token_count": 526
  },
  {
    "chunk_id": "7aed9517-1b8a-4119-8dd7-42dceafe3d02",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Global Path Planning\n\nIsaac integration with Nav2 global planners includes GPU acceleration for path planning algorithms:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Path\nfrom geometry_msgs.msg import PoseStamped\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport numpy as np",
    "chapter": 4,
    "section": "Global Path Planning",
    "page": 10,
    "token_count": 66
  },
  {
    "chunk_id": "65b7e309-201f-4f5a-a8ee-d9e66a4f8ea8",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Global Path Planning\n\nclass IsaacGlobalPlanner(Node):\n    def __init__(self):\n        super().__init__('isaac_global_planner')\n        \n        # Subscribers\n        self.start_subscription = self.create_subscription(\n            PoseStamped,\n            '/initialpose',\n            self.start_callback,\n            10\n        )\n        \n        self.goal_subscription = self.create_subscription(\n            PoseStamped,\n            '/goal_pose',\n            self.goal_callback,\n            10\n        )\n        \n        # Publishers\n        self.path_publisher = self.create_publisher(\n            Path,\n            '/global_plan',\n            10\n        )\n        \n        self.path_marker_publisher = self.create_publisher(\n            MarkerArray,\n            '/global_plan_markers',\n            10\n        )\n        \n        # Initialize planning variables\n        self.start_pose = None\n        self.goal_pose = None\n        self.map_resolution = 0.05  # meters per cell\n        self.planning_frequency = 1.0  # Hz\n        \n        # Planning timer\n        self.planning_timer = self.create_timer(\n            1.0 / self.planning_frequency,\n            self.plan_path\n        )\n    \n    def start_callback(self, msg):\n        # Set start pose for planning\n        self.start_pose = msg.pose\n        self.get_logger().info(f'Start pose set: ({msg.pose.position.x}, {msg.pose.position.y})')\n    \n    def goal_callback(self, msg):\n        # Set goal pose and trigger planning\n        self.goal_pose = msg.pose\n        self.get_logger().info(f'Goal pose set: ({msg.pose.position.x}, {msg.pose.position.y})')\n        \n        # Plan path immediately\n        self.plan_path()\n    \n    def plan_path(self):\n        # This would interface with Isaac-accelerated path planning\n        if self.start_pose and self.goal_pose:\n            # In Isaac navigation, path planning would leverage GPU acceleration\n            path = self.compute_gpu_accelerated_path()\n            \n            if path:\n                self.publish_path(path)\n                self.publish_path_markers(path)\n    \n    def compute_gpu_accelerated_path(self):\n        # Simulate Isaac-accelerated path planning\n        # In reality, this would interface with Isaac's GPU-accelerated planners\n        if self.start_pose and self.goal_pose:\n            # Example: Simple straight-line path (normally would be planned)\n            path = Path()\n            path.header.stamp = self.get_clock().now().to_msg()\n            path.header.frame_id = 'map'\n            \n            # For Isaac, this would use GPU-accelerated A* or Dijkstra\n            # with occupancy grid data from Isaac SLAM\n            return path\n        return None\n    \n    def publish_path(self, path):\n        # Publish the computed path\n        self.path_publisher.publish(path)\n    \n    def publish_path_markers(self, path):\n        # Publish visualization markers for the path\n        marker_array = MarkerArray()\n        \n        for i, pose in enumerate(path.poses):\n            marker = Marker()\n            marker.header.frame_id = 'map'\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = 'global_plan'\n            marker.id = i\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n            \n            marker.pose = pose\n            marker.scale.x = 0.1\n            marker.scale.y = 0.1\n            marker.scale.z = 0.1\n            marker.color.r = 1.0\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n            marker.color.a = 1.0\n            \n            marker_array.markers.append(marker)\n        \n        self.path_marker_publisher.publish(marker_array)\n```",
    "chapter": 4,
    "section": "Global Path Planning",
    "page": 11,
    "token_count": 764
  },
  {
    "chunk_id": "14e0f705-952e-474f-9054-165d8dbbb520",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Local Path Planning and Control\n\nIsaac provides GPU-accelerated local planning for dynamic obstacle avoidance:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom nav_msgs.msg import Path, OccupancyGrid\nfrom visualization_msgs.msg import Marker\nimport numpy as np\nimport math",
    "chapter": 4,
    "section": "Local Path Planning and Control",
    "page": 12,
    "token_count": 85
  },
  {
    "chunk_id": "109d5d0f-fbed-42b2-9323-2fd78c02879c",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Local Path Planning and Control\n\nclass IsaacLocalPlanner(Node):\n    def __init__(self):\n        super().__init__('isaac_local_planner')\n        \n        # Subscribers\n        self.global_path_subscription = self.create_subscription(\n            Path,\n            '/global_plan',\n            self.global_path_callback,\n            10\n        )\n        \n        self.laser_subscription = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.laser_callback,\n            10\n        )\n        \n        self.odom_subscription = self.create_subscription(\n            Odometry,\n            '/odom',\n            self.odom_callback,\n            10\n        )\n        \n        # Publishers\n        self.cmd_vel_publisher = self.create_publisher(\n            Twist,\n            '/cmd_vel',\n            10\n        )\n        \n        self.local_plan_publisher = self.create_publisher(\n            Path,\n            '/local_plan',\n            10\n        )\n        \n        # Initialize control parameters\n        self.global_path = None\n        self.current_pose = None\n        self.current_velocity = None\n        self.safe_to_move = True\n        \n        # Control loop timer\n        self.control_timer = self.create_timer(0.1, self.control_loop)  # 10Hz\n        \n        # Isaac-specific parameters for GPU-accelerated local planning\n        self.max_linear_speed = 0.5  # m/s\n        self.max_angular_speed = 1.0  # rad/s\n        self.min_distance_to_obstacle = 0.5  # meters\n        self.lookahead_distance = 1.0  # meters\n    \n    def global_path_callback(self, msg):\n        # Store the global path for local planning\n        self.global_path = msg.poses\n        self.get_logger().info(f'Received global path with {len(msg.poses)} waypoints')\n    \n    def laser_callback(self, msg):\n        # Process laser scan for obstacle detection\n        # Isaac's obstacle detection uses GPU acceleration\n        self.check_for_obstacles(msg)\n    \n    def odom_callback(self, msg):\n        # Update current pose and velocity\n        self.current_pose = msg.pose.pose\n        self.current_velocity = msg.twist.twist\n    \n    def control_loop(self):\n        # Main control loop for Isaac local planning\n        if self.current_pose and self.global_path and self.safe_to_move:\n            # Compute local plan using Isaac's GPU-accelerated algorithms\n            cmd_vel = self.compute_local_control()\n            if cmd_vel:\n                self.cmd_vel_publisher.publish(cmd_vel)\n    \n    def check_for_obstacles(self, scan_msg):\n        # Isaac's obstacle detection with GPU acceleration\n        # Check if there are obstacles in the robot's path\n        min_distance = min([r for r in scan_msg.ranges if not math.isnan(r)])\n        \n        if min_distance < self.min_distance_to_obstacle:\n            self.safe_to_move = False\n            self.get_logger().warn(f'Obstacle detected at {min_distance:.2f}m')\n        else:\n            self.safe_to_move = True\n    \n    def compute_local_control(self):\n        # Compute velocity command using Isaac's local planning\n        cmd_vel = Twist()\n        \n        if self.global_path:\n            # Find next waypoint in global path\n            next_waypoint = self.get_next_waypoint()\n            \n            if next_waypoint:\n                # Calculate direction to waypoint\n                dx = next_waypoint.pose.position.x - self.current_pose.position.x\n                dy = next_waypoint.pose.position.y - self.current_pose.position.y\n                distance = math.sqrt(dx*dx + dy*dy)\n                \n                if distance > 0.1:  # If we're not close to waypoint\n                    # Calculate angle to waypoint\n                    angle_to_waypoint = math.atan2(dy, dx)\n                    current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)\n                    \n                    # Calculate angular error\n                    angle_error = self.normalize_angle(angle_to_waypoint - current_yaw)\n                    \n                    # Set velocities\n                    cmd_vel.linear.x = min(self.max_linear_speed * distance, self.max_linear_speed)\n                    cmd_vel.angular.z = self.max_angular_speed * angle_error\n                    \n                    # Isaac's GPU acceleration would optimize this trajectory\n                    # to avoid obstacles while following the path\n                else:\n                    # We've reached the waypoint, stop\n                    cmd_vel.linear.x = 0.0\n                    cmd_vel.angular.z = 0.0\n        \n        return cmd_vel\n    \n    def get_next_waypoint(self):\n        # Find the next waypoint along the global path\n        if not self.current_pose or not self.global_path:\n            return None\n        \n        for pose_stamped in self.global_path:\n            # Calculate distance to waypoint\n            dx = pose_stamped.pose.position.x - self.current_pose.position.x\n            dy = pose_stamped.pose.position.y - self.current_pose.position.y\n            distance = math.sqrt(dx*dx + dy*dy)\n            \n            if distance > self.lookahead_distance:\n                return pose_stamped\n        \n        # If no waypoint is far enough, return the last one\n        if self.global_path:\n            return self.global_path[-1]\n        \n        return None\n    \n    def get_yaw_from_quaternion(self, quaternion):\n        # Extract yaw from quaternion\n        import tf_transformations\n        euler = tf_transformations.euler_from_quaternion([\n            quaternion.x,\n            quaternion.y,\n            quaternion.z,\n            quaternion.w\n        ])\n        return euler[2]  # yaw is the third element\n    \n    def normalize_angle(self, angle):\n        # Normalize angle to range [-pi, pi]\n        while angle > math.pi:\n            angle -= 2 * math.pi\n        while angle < -math.pi:\n            angle += 2 * math.pi\n        return angle\n```",
    "chapter": 4,
    "section": "Local Path Planning and Control",
    "page": 13,
    "token_count": 1179
  },
  {
    "chunk_id": "48d0cb52-bbf7-40ad-9d5c-9dca840c850d",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Isaac Navigation Configuration",
    "chapter": 4,
    "section": "Isaac Navigation Configuration",
    "page": 14,
    "token_count": 4
  },
  {
    "chunk_id": "ec3de85f-8ef7-4538-8c9e-78cdd0251239",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Costmap Configuration\n\nIsaac navigation uses GPU-accelerated costmaps for obstacle representation and path planning:",
    "chapter": 4,
    "section": "Costmap Configuration",
    "page": 15,
    "token_count": 23
  },
  {
    "chunk_id": "88dabaa5-2358-4169-bd40-16ecce5e3367",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Costmap Configuration\n\n```yaml\n# costmap_params.yaml\nglobal_costmap:\n  global_frame: map\n  robot_base_frame: base_link\n  update_frequency: 10.0\n  publish_frequency: 5.0\n  static_map: true\n  rolling_window: false\n  resolution: 0.05  # meters per cell (Isaac can handle fine resolution with GPU)\n  \n  plugins:\n    - {name: static_layer, type: \"nav2_costmap_2d::StaticLayer\"}\n    - {name: obstacle_layer, type: \"nav2_costmap_2d::ObstacleLayer\"}\n    - {name: inflation_layer, type: \"nav2_costmap_2d::InflationLayer\"}\n  \n  static_layer:\n    map_topic: /map\n    transform_tolerance: 0.5\n    \n  obstacle_layer:\n    enabled: true\n    observation_sources: scan\n    scan:\n      topic: /scan\n      sensor_frame: laser_frame\n      observation_persistence: 0.0\n      max_obstacle_height: 2.0\n      min_obstacle_height: 0.0\n      obstacle_range: 3.0\n      raytrace_range: 4.0\n      inf_is_valid: true\n      clearing: true\n      marking: true\n      data_type: \"LaserScan\"\n  \n  inflation_layer:\n    enabled: true\n    cost_scaling_factor: 5.0\n    inflation_radius: 1.0\n    \n    # Isaac-specific GPU-optimized parameters\n    use_gpu_inflation: true\n    gpu_thread_pool_size: 4",
    "chapter": 4,
    "section": "Costmap Configuration",
    "page": 16,
    "token_count": 339
  },
  {
    "chunk_id": "033f6cb5-3290-46d5-9c45-e28be56dd745",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Costmap Configuration\n\nlocal_costmap:\n  global_frame: odom\n  robot_base_frame: base_link\n  update_frequency: 10.0\n  publish_frequency: 5.0\n  static_map: false\n  rolling_window: true\n  width: 5.0\n  height: 5.0\n  resolution: 0.05  # Isaac GPU acceleration allows for higher resolution\n  \n  plugins:\n    - {name: obstacle_layer, type: \"nav2_costmap_2d::ObstacleLayer\"}\n    - {name: inflation_layer, type: \"nav2_costmap_2d::InflationLayer\"}\n  \n  obstacle_layer:\n    enabled: true\n    observation_sources: scan\n    scan:\n      topic: /scan\n      sensor_frame: laser_frame\n      observation_persistence: 0.0\n      max_obstacle_height: 2.0\n      min_obstacle_height: 0.0\n      obstacle_range: 3.0\n      raytrace_range: 4.0\n      inf_is_valid: true\n      clearing: true\n      marking: true\n      data_type: \"LaserScan\"\n  \n  inflation_layer:\n    enabled: true\n    cost_scaling_factor: 3.0\n    inflation_radius: 0.55\n    \n    # Isaac-specific GPU-optimized parameters\n    use_gpu_inflation: true\n    gpu_thread_pool_size: 4\n```",
    "chapter": 4,
    "section": "Costmap Configuration",
    "page": 17,
    "token_count": 300
  },
  {
    "chunk_id": "a2590b6a-05cd-4319-9cea-62767f0e3513",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Planner Server Configuration",
    "chapter": 4,
    "section": "Planner Server Configuration",
    "page": 18,
    "token_count": 4
  },
  {
    "chunk_id": "109022b7-a24c-48ea-9828-ec3a1e511a1b",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Planner Server Configuration\n\n```yaml\n# planner_server_params.yaml\nplanner_server:\n  ros__parameters:\n    expected_planner_frequency: 20.0\n    use_gpu_planning: true  # Enable Isaac GPU acceleration\n    gpu_memory_fraction: 0.8  # Fraction of GPU memory to use\n    \n    # Global planner configuration\n    NavfnROS:\n      tolerance: 0.5\n      use_astar: false\n      allow_unknown: true\n      \n    # Isaac-specific global planners\n    IsaacAStarPlanner:\n      type: \"isaac_nav2_plugins::IsaacAStarPlanner\"\n      resolution: 0.05\n      max_iterations: 10000\n      use_gpu_acceleration: true\n    \n    # Local planner configuration (for Isaac)\n    IsaacTrajectoryPlanner:\n      type: \"isaac_nav2_plugins::IsaacTrajectoryPlanner\"\n      min_vel_x: 0.0\n      max_vel_x: 0.5\n      min_vel_y: -0.5\n      max_vel_y: 0.5\n      max_vel_theta: 1.0\n      min_in_place_vel_theta: 0.4\n      acc_lim_x: 2.5\n      acc_lim_theta: 3.2\n      xy_goal_tolerance: 0.25\n      yaw_goal_tolerance: 0.15\n      sim_time: 1.7\n      vx_samples: 7\n      vy_samples: 0\n      vtheta_samples: 20\n      path_distance_bias: 32.0\n      goal_distance_bias: 24.0\n      occdist_scale: 0.02\n      forward_point_distance: 0.325\n      stop_time_buffer: 0.2\n      scaling_speed: 0.25\n      max_scaling_factor: 0.2\n      use_gpu_optimization: true\n```",
    "chapter": 4,
    "section": "Planner Server Configuration",
    "page": 19,
    "token_count": 397
  },
  {
    "chunk_id": "cab4d313-cceb-4b06-a8ac-fb305d00a1bb",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Isaac Navigation Launch Files",
    "chapter": 4,
    "section": "Isaac Navigation Launch Files",
    "page": 20,
    "token_count": 5
  },
  {
    "chunk_id": "355bee41-1023-4fb0-9b9c-cba38bb7cbb2",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Complete Isaac Navigation Stack\n\n```python\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, GroupAction\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node, PushRosNamespace\nfrom launch_ros.substitutions import FindPackageShare",
    "chapter": 4,
    "section": "Complete Isaac Navigation Stack",
    "page": 21,
    "token_count": 81
  },
  {
    "chunk_id": "e90e78ee-7b61-4e95-a965-54e9b570d1dc",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Complete Isaac Navigation Stack\n\ndef generate_launch_description():\n    # Launch configurations\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    autostart = LaunchConfiguration('autostart', default='true')\n    use_composition = LaunchConfiguration('use_composition', default='false')\n    use_respawn = LaunchConfiguration('use_respawn', default='false')\n    \n    # Isaac-specific configurations\n    use_isaac_slam = LaunchConfiguration('use_isaac_slam', default='true')\n    use_gpu_planning = LaunchConfiguration('use_gpu_planning', default='true')\n    \n    # Declare launch arguments\n    declare_use_sim_time_cmd = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation time if true'\n    )\n    \n    declare_autostart_cmd = DeclareLaunchArgument(\n        'autostart', \n        default_value='true',\n        description='Automatically start the nav2 stack'\n    )\n    \n    declare_use_composition_cmd = DeclareLaunchArgument(\n        'use_composition', \n        default_value='false',\n        description='Whether to use composed bringup'\n    )\n    \n    declare_use_respawn_cmd = DeclareLaunchArgument(\n        'use_respawn', \n        default_value='false',\n        description='Whether to respawn if a node crashes'\n    )\n    \n    declare_use_isaac_slam_cmd = DeclareLaunchArgument(\n        'use_isaac_slam',\n        default_value='true',\n        description='Use Isaac SLAM integration'\n    )\n    \n    declare_use_gpu_planning_cmd = DeclareLaunchArgument(\n        'use_gpu_planning',\n        default_value='true',\n        description='Enable GPU-accelerated planning'\n    )\n    \n    # Isaac SLAM integration (if enabled)\n    isaac_slam_group = GroupAction(\n        condition=IfCondition(use_isaac_slam),\n        actions=[\n            IncludeLaunchDescription(\n                PythonLaunchDescriptionSource([\n                    FindPackageShare('isaac_ros_visual_slam'),\n                    '/launch/visual_slam.launch.py'\n                ]),\n                launch_arguments={\n                    'use_sim_time': use_sim_time,\n                    'map_frame': 'map',\n                    'publish_map_odom_transform': 'true',\n                    'mode': 'slam'\n                }.items()\n            )\n        ]\n    )\n    \n    # Main navigation stack with Isaac optimizations\n    nav2_bringup_launch_dir = FindPackageShare('nav2_bringup')\n    \n    nav2_bringup_cmd = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            nav2_bringup_launch_dir,\n            '/launch/navigation_launch.py'\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time,\n            'autostart': autostart,\n            'use_composition': use_composition,\n            'use_respawn': use_respawn,\n            'params_file': PathJoinSubstitution([\n                FindPackageShare('my_robot_navigation'),\n                'config',\n                'isaac_nav_params.yaml'\n            ])\n        }.items()\n    )\n    \n    # Isaac-specific navigation nodes\n    isaac_perception_nodes = GroupAction(\n        actions=[\n            Node(\n                package='isaac_ros_apriltag',\n                executable='isaac_ros_apriltag',\n                name='apriltag',\n                parameters=[{\n                    'family': 'tag36h11',\n                    'size': 0.145,\n                    'max_hamming': 0,\n                    'quad_decimate': 2.0\n                }],\n                remappings=[\n                    ('image', '/camera/image_rect_color'),\n                    ('camera_info', '/camera/camera_info'),\n                    ('detections', '/apriltag_detections')\n                ]\n            ),\n            \n            Node(\n                package='isaac_ros_dnn_inference',\n                executable='dnn_inference',\n                name='dnn_inference',\n                parameters=[{\n                    'model_path': '/path/to/obstacle_detection.engine',\n                    'input_topic': '/camera/image_rect_color',\n                    'output_topic': '/dnn_detections'\n                }]\n            )\n        ]\n    )\n    \n    # Isaac navigation controllers\n    isaac_controllers = Node(\n        package='isaac_ros_navigation_controllers',\n        executable='isaac_simple_controller',\n        name='isaac_navigation_controller',\n        parameters=[{\n            'use_gpu_optimization': use_gpu_planning,\n            'max_linear_velocity': 0.5,\n            'max_angular_velocity': 1.0,\n            'gpu_thread_count': 4\n        }]\n    )\n    \n    # Create launch description\n    ld = LaunchDescription()\n    \n    # Add launch arguments\n    ld.add_action(declare_use_sim_time_cmd)\n    ld.add_action(declare_autostart_cmd)\n    ld.add_action(declare_use_composition_cmd)\n    ld.add_action(declare_use_respawn_cmd)\n    ld.add_action(declare_use_isaac_slam_cmd)\n    ld.add_action(declare_use_gpu_planning_cmd)\n    \n    # Add actions\n    ld.add_action(isaac_slam_group)\n    ld.add_action(nav2_bringup_cmd)\n    ld.add_action(isaac_perception_nodes)\n    ld.add_action(isaac_controllers)\n    \n    return ld\n```",
    "chapter": 4,
    "section": "Complete Isaac Navigation Stack",
    "page": 22,
    "token_count": 1080
  },
  {
    "chunk_id": "891be9fa-1c4d-448b-a40a-4f89f65c5fc2",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Advanced Navigation Features",
    "chapter": 4,
    "section": "Advanced Navigation Features",
    "page": 23,
    "token_count": 4
  },
  {
    "chunk_id": "95fa0802-99a1-4ecd-9936-45294f811b54",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Multi-Robot Navigation\n\nIsaac supports multi-robot navigation with collision avoidance:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom visualization_msgs.msg import MarkerArray\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np",
    "chapter": 4,
    "section": "Multi-Robot Navigation",
    "page": 24,
    "token_count": 71
  },
  {
    "chunk_id": "e427ad46-37df-4f4b-8722-2fd44edc1ebc",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Multi-Robot Navigation\n\nclass IsaacMultiRobotNavigation(Node):\n    def __init__(self):\n        super().__init__('isaac_multi_robot_navigation')\n        \n        # Robot identification\n        self.robot_namespace = self.get_namespace()\n        self.robot_name = self.get_parameter_or('robot_name', 'robot_0').value\n        \n        # Robot tracking\n        self.other_robots = {}\n        self.robot_subscribers = []\n        \n        # Initialize for multiple robots\n        self.initialize_robot_tracking()\n        \n        # Navigation with collision avoidance\n        self.collision_avoidance_threshold = 1.0  # meters\n        self.navigation_safety_margin = 0.5  # meters\n    \n    def initialize_robot_tracking(self):\n        # Subscribe to odometry of other robots in the fleet\n        # This could be done through a shared TF tree or direct topic access\n        other_robots = ['robot_1', 'robot_2', 'robot_3']  # Example robot names\n        \n        for robot_name in other_robots:\n            if robot_name != self.robot_name:\n                # Subscribe to other robot's odometry\n                subscription = self.create_subscription(\n                    Odometry,\n                    f'/{robot_name}/odom',\n                    lambda msg, rn=robot_name: self.robot_odom_callback(msg, rn),\n                    10\n                )\n                self.robot_subscribers.append(subscription)\n    \n    def robot_odom_callback(self, msg, robot_name):\n        \"\"\"Track position of other robots for collision avoidance\"\"\"\n        self.other_robots[robot_name] = {\n            'x': msg.pose.pose.position.x,\n            'y': msg.pose.pose.position.y,\n            'theta': self.get_yaw_from_quaternion(msg.pose.pose.orientation),\n            'timestamp': msg.header.stamp\n        }\n    \n    def check_collision_risk(self, target_pose):\n        \"\"\"Check if navigation to target pose conflicts with other robots\"\"\"\n        current_time = self.get_clock().now().seconds_nanoseconds()\n        \n        for robot_name, robot_info in self.other_robots.items():\n            # Calculate distance to other robot\n            dx = target_pose.position.x - robot_info['x']\n            dy = target_pose.position.y - robot_info['y']\n            distance = np.sqrt(dx*dx + dy*dy)\n            \n            if distance < self.collision_avoidance_threshold:\n                # Potential collision - need to replan or wait\n                self.get_logger().warn(\n                    f'Collision risk with {robot_name} at distance {distance:.2f}m'\n                )\n                return True\n        \n        return False\n    \n    def plan_safe_path(self, goal_pose):\n        \"\"\"Plan path considering positions of other robots\"\"\"\n        # Isaac's GPU-accelerated multi-robot path planning\n        # would consider other robot positions as dynamic obstacles\n        \n        # For now, implement basic collision checking\n        if not self.check_collision_risk(goal_pose):\n            # Plan path normally\n            return self.compute_path_to_goal(goal_pose)\n        else:\n            # Wait or find alternative route\n            self.get_logger().info('Adjusting path to avoid collision with other robots')\n            return self.compute_safe_path_around_robots(goal_pose)\n    \n    def compute_safe_path_around_robots(self, goal_pose):\n        \"\"\"Compute path that avoids other robots\"\"\"\n        # In Isaac, this would use GPU-accelerated multi-agent path planning\n        # For this example, we'll simulate finding a safe path\n        return None\n```",
    "chapter": 4,
    "section": "Multi-Robot Navigation",
    "page": 25,
    "token_count": 710
  },
  {
    "chunk_id": "37321248-ad5a-44f7-9423-0d781367fe79",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Adaptive Navigation Parameters\n\nIsaac navigation can adapt parameters based on environmental conditions:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nfrom geometry_msgs.msg import Twist\nimport numpy as np",
    "chapter": 4,
    "section": "Adaptive Navigation Parameters",
    "page": 26,
    "token_count": 53
  },
  {
    "chunk_id": "3832c56c-a725-4e07-aeb7-474c626f7606",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Adaptive Navigation Parameters\n\nclass IsaacAdaptiveNavigation(Node):\n    def __init__(self):\n        super().__init__('isaac_adaptive_navigation')\n        \n        # Subscribe to environment sensors\n        self.laser_subscription = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.laser_callback,\n            10\n        )\n        \n        self.camera_subscription = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.camera_callback,\n            10\n        )\n        \n        # Initialize navigation parameters\n        self.current_mode = 'normal'  # normal, cautious, exploration\n        self.dynamic_params = {\n            'max_linear_speed': 0.5,\n            'max_angular_speed': 1.0,\n            'min_obstacle_distance': 0.5,\n            'inflation_radius': 0.55,\n            'path_tolerance': 0.25\n        }\n        \n        # Environmental assessment timer\n        self.assessment_timer = self.create_timer(1.0, self.assess_environment)\n    \n    def laser_callback(self, msg):\n        \"\"\"Analyze laser scan for environmental assessment\"\"\"\n        # Analyze obstacle density and distribution\n        valid_ranges = [r for r in msg.ranges if 0.1 < r < 10.0]\n        \n        if not valid_ranges:\n            return\n            \n        # Calculate obstacle density in different directions\n        front_density = self.calculate_front_obstacle_density(msg)\n        side_density = self.calculate_side_obstacle_density(msg)\n        \n        # Update navigation strategy based on density\n        self.update_navigation_strategy(front_density, side_density)\n    \n    def camera_callback(self, msg):\n        \"\"\"Analyze camera image for environmental features\"\"\"\n        # Isaac's DNN could analyze image for:\n        # - Human detection\n        # - Clutter level assessment\n        # - Passageway width estimation\n        # - Surface type classification\n        \n        # For this example, we'll just simulate the analysis\n        pass\n    \n    def calculate_front_obstacle_density(self, scan_msg):\n        \"\"\"Calculate obstacle density in front of robot\"\"\"\n        # Analyze central portion of laser scan\n        central_indices = slice(len(scan_msg.ranges)//3, 2*len(scan_msg.ranges)//3)\n        central_ranges = scan_msg.ranges[central_indices]\n        \n        # Count obstacles within certain distance\n        obstacles = [r for r in central_ranges if 0.1 < r < 2.0]\n        density = len(obstacles) / len(central_ranges) if central_ranges else 0\n        \n        return density\n    \n    def calculate_side_obstacle_density(self, scan_msg):\n        \"\"\"Calculate obstacle density on sides of robot\"\"\"\n        # Analyze left and right portions of laser scan\n        left_ranges = scan_msg.ranges[:len(scan_msg.ranges)//6] + scan_msg.ranges[5*len(scan_msg.ranges)//6:]\n        close_obstacles = [r for r in left_ranges if 0.1 < r < 1.0]\n        \n        density = len(close_obstacles) / len(left_ranges) if left_ranges else 0\n        return density\n    \n    def update_navigation_strategy(self, front_density, side_density):\n        \"\"\"Update navigation parameters based on environment\"\"\"\n        # Determine navigation mode based on obstacle density\n        if front_density > 0.3 or side_density > 0.4:\n            # Dense obstacle environment - be cautious\n            new_mode = 'cautious'\n            speed_factor = 0.5\n            safety_margin = 0.8\n        elif front_density < 0.1 and side_density < 0.1:\n            # Open environment - can navigate faster\n            new_mode = 'fast'\n            speed_factor = 1.0\n            safety_margin = 0.4\n        else:\n            # Normal environment\n            new_mode = 'normal'\n            speed_factor = 0.7\n            safety_margin = 0.55\n        \n        if new_mode != self.current_mode:\n            self.get_logger().info(f'Switching to {new_mode} navigation mode')\n            self.current_mode = new_mode\n        \n        # Update dynamic parameters\n        self.dynamic_params['max_linear_speed'] = 0.5 * speed_factor\n        self.dynamic_params['min_obstacle_distance'] = safety_margin\n        self.dynamic_params['inflation_radius'] = min(1.0, safety_margin + 0.1)\n    \n    def assess_environment(self):\n        \"\"\"Periodically assess environment and adapt parameters\"\"\"\n        # This would interface with Isaac's GPU-accelerated environmental analysis\n        # to determine optimal navigation strategies\n        \n        self.get_logger().debug(f'Environment mode: {self.current_mode}')\n        self.get_logger().debug(f'Current parameters: {self.dynamic_params}')\n```",
    "chapter": 4,
    "section": "Adaptive Navigation Parameters",
    "page": 27,
    "token_count": 989
  },
  {
    "chunk_id": "4caeed00-a6da-446d-bf14-07822ef27fa9",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Performance Optimization",
    "chapter": 4,
    "section": "Performance Optimization",
    "page": 28,
    "token_count": 3
  },
  {
    "chunk_id": "7e0c1ba6-a81b-41b6-8b59-a7d447374ff3",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### GPU-Accelerated Path Planning\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Path\nfrom visualization_msgs.msg import Marker\nimport numpy as np\nimport cupy as cp  # CUDA Python for GPU acceleration",
    "chapter": 4,
    "section": "GPU-Accelerated Path Planning",
    "page": 29,
    "token_count": 64
  },
  {
    "chunk_id": "f61767af-8668-41d2-b590-38e770c857be",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## GPU-Accelerated Path Planning\n\nclass IsaacGPUPathPlanner(Node):\n    def __init__(self):\n        super().__init__('isaac_gpu_path_planner')\n        \n        # Initialize GPU memory structures\n        self.occupancy_grid_gpu = None\n        self.path_points_gpu = None\n        self.temp_costs_gpu = None\n        \n        # Configure GPU parameters\n        self.grid_width = 2000  # Example large grid that benefits from GPU\n        self.grid_height = 2000\n        self.grid_resolution = 0.05  # meters per cell\n        \n        # Initialize GPU memory for path planning\n        self.initialize_gpu_memory()\n    \n    def initialize_gpu_memory(self):\n        \"\"\"Initialize GPU memory for path planning operations\"\"\"\n        try:\n            # Allocate GPU memory for occupancy grid\n            self.occupancy_grid_gpu = cp.zeros((self.grid_height, self.grid_width), dtype=cp.uint8)\n            \n            # Allocate memory for temporary path planning costs\n            self.temp_costs_gpu = cp.full((self.grid_height, self.grid_width), cp.inf, dtype=cp.float32)\n            \n            # Allocate memory for path points\n            self.path_points_gpu = cp.zeros((1000, 2), dtype=cp.float32)  # Max 1000 waypoints\n            \n            self.get_logger().info('GPU memory initialized for path planning')\n            \n        except Exception as e:\n            self.get_logger().error(f'Failed to initialize GPU memory: {e}')\n    \n    def gpu_astar_planning(self, start, goal, occupancy_grid):\n        \"\"\"GPU-accelerated A* path planning\"\"\"\n        # Copy occupancy grid to GPU\n        if self.occupancy_grid_gpu is not None:\n            cp.copyto(self.occupancy_grid_gpu, cp.asarray(occupancy_grid))\n            \n            # Convert start and goal to grid coordinates\n            start_grid = self.world_to_grid(start)\n            goal_grid = self.world_to_grid(goal)\n            \n            # Perform A* on GPU (simplified - real implementation would be more complex)\n            path = self.perform_gpu_astar(start_grid, goal_grid)\n            \n            return path\n        else:\n            # Fallback to CPU planning\n            return self.cpu_astar_planning(start, goal, occupancy_grid)\n    \n    def world_to_grid(self, world_pose):\n        \"\"\"Convert world coordinates to grid coordinates\"\"\"\n        grid_x = int((world_pose.position.x - self.grid_origin_x) / self.grid_resolution)\n        grid_y = int((world_pose.position.y - self.grid_origin_y) / self.grid_resolution)\n        return (grid_x, grid_y)\n    \n    def perform_gpu_astar(self, start_grid, goal_grid):\n        \"\"\"Perform GPU-accelerated A* algorithm\"\"\"\n        # This would implement a GPU-optimized A* algorithm\n        # using CUDA kernels for parallel processing\n        \n        # For this example, we'll simulate the GPU processing\n        path = []\n        \n        # GPU-optimized path planning would go here\n        # This is where Isaac's GPU acceleration provides significant benefits\n        # for large maps or complex planning scenarios\n        \n        return path\n    \n    def cpu_astar_planning(self, start, goal, occupancy_grid):\n        \"\"\"CPU-based A* planning as fallback\"\"\"\n        # Implementation for CPU-based planning\n        self.get_logger().warn('Using CPU planning - GPU planning not available')\n        return []\n```",
    "chapter": 4,
    "section": "GPU-Accelerated Path Planning",
    "page": 30,
    "token_count": 693
  },
  {
    "chunk_id": "52808107-0964-4d57-b225-aa139dda3a53",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Integration with Isaac Perception",
    "chapter": 4,
    "section": "Integration with Isaac Perception",
    "page": 31,
    "token_count": 5
  },
  {
    "chunk_id": "6deab4ad-2fcb-4b20-9ea1-a43decfb0b9d",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Perception-Planning Integration\n\nIsaac navigation tightly integrates with Isaac perception for enhanced obstacle detection and navigation safety:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import OccupancyGrid\nimport numpy as np",
    "chapter": 4,
    "section": "Perception-Planning Integration",
    "page": 32,
    "token_count": 76
  },
  {
    "chunk_id": "5260790a-7ff8-4afc-9bfc-41fe0d508319",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Perception-Planning Integration\n\nclass PerceptionPlanningIntegration(Node):\n    def __init__(self):\n        super().__init__('perception_planning_integration')\n        \n        # Perception inputs\n        self.detection_subscription = self.create_subscription(\n            Detection2DArray,\n            '/dnn_detections',\n            self.detection_callback,\n            10\n        )\n        \n        self.laser_subscription = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.laser_callback,\n            10\n        )\n        \n        # Planning inputs\n        self.cmd_vel_subscription = self.create_subscription(\n            Twist,\n            '/cmd_vel',\n            self.cmd_vel_callback,\n            10\n        )\n        \n        # Publishers for integrated data\n        self.enhanced_costmap_publisher = self.create_publisher(\n            OccupancyGrid,\n            '/isaac/enhanced_costmap',\n            10\n        )\n        \n        # Initialize detection tracking\n        self.detection_history = []\n        self.dynamic_obstacle_tracks = {}\n        \n    def detection_callback(self, msg):\n        \"\"\"Process object detections from Isaac perception\"\"\"\n        # Process each detection and update dynamic obstacle tracking\n        for detection in msg.detections:\n            if detection.results:\n                class_id = detection.results[0].hypothesis.class_id\n                confidence = detection.results[0].hypothesis.score\n                \n                if confidence > 0.7:  # High confidence detection\n                    self.process_high_confidence_detection(detection, class_id)\n    \n    def process_high_confidence_detection(self, detection, class_id):\n        \"\"\"Process high-confidence detections for navigation\"\"\"\n        if class_id == \"person\" or class_id == \"human\":\n            # Track human movement for navigation safety\n            self.track_human_obstacle(detection)\n        elif class_id == \"obstacle\" or class_id == \"object\":\n            # Add to costmap as potential obstacle\n            self.add_detection_to_costmap(detection)\n        elif class_id == \"traffic_sign\":\n            # Process navigation-relevant signs\n            self.handle_traffic_sign(detection)\n    \n    def track_human_obstacle(self, detection):\n        \"\"\"Track moving humans for dynamic navigation\"\"\"\n        # Get detection position and add to tracking\n        center_x = detection.bbox.center.x\n        center_y = detection.bbox.center.y\n        \n        # This would connect with Isaac's tracking algorithms\n        # to maintain consistent IDs for moving objects\n        human_id = self.assign_unique_id(detection)\n        \n        self.dynamic_obstacle_tracks[human_id] = {\n            'position': (center_x, center_y),\n            'timestamp': self.get_clock().now().to_msg(),\n            'type': 'person',\n            'velocity': self.estimate_velocity(human_id, (center_x, center_y))\n        }\n    \n    def add_detection_to_costmap(self, detection):\n        \"\"\"Add detection to navigation costmap\"\"\"\n        # Convert detection to costmap coordinates and add as obstacle\n        # This integrates visual detections with laser-based costmap\n        pass\n    \n    def laser_callback(self, msg):\n        \"\"\"Process laser data alongside perception results\"\"\"\n        # Combine laser range data with visual detections\n        # for more robust obstacle detection\n        pass\n    \n    def cmd_vel_callback(self, msg):\n        \"\"\"Monitor navigation commands for safety checks\"\"\"\n        # Use perception data to validate navigation commands\n        # and ensure safe execution\n        if self.is_safe_navigation_command(msg):\n            # Command is safe, allow execution\n            pass\n        else:\n            # Modify or cancel dangerous commands\n            self.modify_unsafe_command(msg)\n    \n    def is_safe_navigation_command(self, cmd_vel):\n        \"\"\"Check if navigation command is safe given perception data\"\"\"\n        # Check if commanded direction has obstacles\n        # using both laser and visual perception data\n        return True  # Simplified check\n    \n    def modify_unsafe_command(self, cmd_vel):\n        \"\"\"Modify unsafe navigation commands based on perception\"\"\"\n        # Adjust commands to avoid detected obstacles\n        # This would use Isaac's GPU-accelerated collision avoidance\n        pass\n    \n    def assign_unique_id(self, detection):\n        \"\"\"Assign unique ID for object tracking\"\"\"\n        # Implementation would use Isaac's tracking algorithms\n        return id(detection)\n    \n    def estimate_velocity(self, object_id, current_position):\n        \"\"\"Estimate velocity of tracked object\"\"\"\n        # Use historical positions to estimate motion\n        return (0.0, 0.0)  # Simplified\n```",
    "chapter": 4,
    "section": "Perception-Planning Integration",
    "page": 33,
    "token_count": 910
  },
  {
    "chunk_id": "8c6b8f5d-759b-4696-8348-181dd099e615",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Diagrams",
    "chapter": 4,
    "section": "Diagrams",
    "page": 34,
    "token_count": 3
  },
  {
    "chunk_id": "5ea6c20d-2fb6-406b-ba13-e38e00e46370",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Isaac Navigation Architecture",
    "chapter": 4,
    "section": "Isaac Navigation Architecture",
    "page": 35,
    "token_count": 4
  },
  {
    "chunk_id": "0ee9cb72-3185-430b-bb01-2f9b8f631d1c",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Isaac Navigation Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    ISAAC NAVIGATION SYSTEM                      │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │   SENSORS   │    │  ISAAC      │    │  GPU        │         │\n│  │             │    │  PERCEPTION │    │  COMPUTE    │         │\n│  │ • LIDAR     │───▶│ • SLAM      │───▶│ • Occupancy │         │\n│  │ • Camera    │    │ • Object    │    │   Grid      │         │\n│  │ • IMU       │    │   Detection │    │ • Path Plan │         │\n│  │ • Other     │    │ • Tracking  │    │ • Collision │         │\n│  └─────────────┘    │ • Mapping   │    │   Avoidance │         │\n│                     └─────────────┘    └─────────────┘         │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │              NAVIGATION PLANNERS                        │   │\n│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────────┐   │   │\n│  │  │ Global      │ │ Local       │ │ Behavior Trees  │   │   │\n│  │  │ Planner     │ │ Planner     │ │ (Multi-Behavior)│   │   │\n│  │  │ (A*, Dijkstra│ │ (DWA, TEB)  │ │ (Isaac Apps)    │   │   │\n│  │  └─────────────┘ └─────────────┘ └─────────────────┘   │   │\n│  └─────────────────────────────────────────────────────────┘   │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │               NAVIGATION OUTPUTS                        │   │\n│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────────┐   │   │\n│  │  │ Path Plan   │ │ Velocity    │ │ Safety Checks   │   │   │\n│  │  │ (Waypoints) │ │ Commands    │ │ (Perception)    │   │   │\n│  │  └─────────────┘ └─────────────┘ └─────────────────┘   │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 4,
    "section": "Isaac Navigation Architecture",
    "page": 36,
    "token_count": 631
  },
  {
    "chunk_id": "030e6d40-7b71-4a0e-9d3d-553128771ff3",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "### Multi-Sensor Fusion in Navigation",
    "chapter": 4,
    "section": "Multi-Sensor Fusion in Navigation",
    "page": 37,
    "token_count": 7
  },
  {
    "chunk_id": "b08d5a56-e952-4ba9-b0f7-96af0d2ce02d",
    "doc_id": "af41345b-0b57-4c24-b718-b751e8af2351",
    "chunk_text": "## Multi-Sensor Fusion in Navigation\n\n```\nLaser Data (2D)    +    Camera Data (2D/3D)    +    IMU Data\n      │                        │                        │\n      ▼                        ▼                        ▼\n┌─────────────┐    ┌─────────────────────────┐    ┌─────────┐\n│ Occupancy   │    │ Object Detection &    │    │ Pose &  │\n│ Grid (Static│    │ Classification (Dynamic│    │ Velocity│\n│ Obstacles)  │    │ Obstacles)            │    │ Tracking│\n└─────────────┘    └─────────────────────────┘    └─────────┘\n         │                        │                        │\n         └────────────────────────┼────────────────────────┘\n                                  ▼\n                    ┌─────────────────────────┐\n                    │  INTEGRATED COSTMAP     │\n                    │  • Static Obstacles     │\n                    │  • Dynamic Obstacles    │\n                    │  • Human Trajectories   │\n                    │  • Safe Navigation Areas│\n                    └─────────────────────────┘\n                                  │\n                                  ▼\n                    ┌─────────────────────────┐\n                    │  GPU-ACCELERATED        │\n                    │  PATH PLANNING          │\n                    │  • Global A* (GPU)      │\n                    │  • Local DWA (GPU)      │\n                    │  • Collision Avoidance  │\n                    └─────────────────────────┘",
    "chapter": 4,
    "section": "Multi-Sensor Fusion in Navigation",
    "page": 38,
    "token_count": 332
  },
  {
    "chunk_id": "e46d4c2a-fb2c-41d6-9733-7bc32d434b6e",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "# NVIDIA Isaac Technology Overview",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 5
  },
  {
    "chunk_id": "f7535f1a-c175-4c72-a4d8-5c3756641d17",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Overview\n\nNVIDIA Isaac represents a comprehensive, hardware-accelerated platform specifically designed for developing, training, and deploying AI-powered robotic systems. The Isaac platform combines NVIDIA's expertise in parallel computing, AI, and simulation to create a unified ecosystem that accelerates the development of intelligent robots. The platform consists of three main components: Isaac Sim for high-fidelity simulation and AI training, Isaac ROS for perception and navigation algorithms on actual hardware, and Isaac Lab for research and development of advanced robotic capabilities.\n\nThe Isaac platform addresses the critical challenges in robotics AI development, including the need for large amounts of training data, the reality gap between simulation and real-world performance, and the computational requirements for real-time AI inference on robotic platforms. By providing a complete toolchain from simulation to deployment, Isaac enables faster development cycles and more robust robot behaviors.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 168
  },
  {
    "chunk_id": "d2278ea3-ea48-412e-a981-782516e93ffd",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Key Concepts",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "f90ce178-08b4-4578-b776-6f356983bdaa",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### The Isaac Technology Stack\n\n**Isaac Sim**: A high-fidelity, photorealistic simulation environment built on NVIDIA's Omniverse platform. Isaac Sim enables the creation of complex virtual worlds with accurate physics, lighting, and sensor simulation for training AI models before deploying them on real robots.\n\n**Isaac ROS**: A collection of hardware-accelerated perception and navigation packages that run on NVIDIA Jetson and RTX GPU platforms. These packages provide state-of-the-art algorithms for tasks such as object detection, SLAM, and path planning.\n\n**Isaac Lab**: A research framework for developing and testing advanced robotic algorithms, particularly for legged robots and manipulation tasks. Isaac Lab provides reinforcement learning environments and simulation tools for cutting-edge robotics research.\n\n**Isaac Apps**: Pre-built applications and reference implementations that demonstrate best practices for various robotics applications, including navigation, manipulation, and inspection.",
    "chapter": 4,
    "section": "The Isaac Technology Stack",
    "page": 4,
    "token_count": 177
  },
  {
    "chunk_id": "0e724e81-1752-4f05-bd3e-d337d088d219",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Hardware Acceleration in Robotics\n\nNVIDIA's GPU architecture provides significant advantages for robotics applications:\n\n**Parallel Processing**: GPUs excel at parallel processing required for real-time sensor data processing, computer vision, and AI inference.\n\n**CUDA Cores**: Thousands of CUDA cores enable simultaneous processing of multiple sensor streams and AI models.\n\n**Tensor Cores**: Specialized for AI inference, providing significant speedup for deep learning models.\n\n**Ray Tracing**: For photorealistic simulation in Isaac Sim, enabling synthetic data generation that closely matches real-world conditions.",
    "chapter": 4,
    "section": "Hardware Acceleration in Robotics",
    "page": 5,
    "token_count": 108
  },
  {
    "chunk_id": "d7bd2635-b998-45f2-9353-667785ad2f31",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Simulation-to-Reality Transfer\n\nOne of Isaac's key strengths is its ability to bridge the gap between simulation and reality:\n\n**Photorealistic Rendering**: High-quality graphics that match real-world lighting and material properties.\n\n**Physics Accuracy**: Realistic simulation of forces, collisions, and environmental interactions.\n\n**Domain Randomization**: Techniques to train AI models that can generalize across different environments and conditions.\n\n**Synthetic Data Generation**: Creation of labeled datasets for training perception algorithms that would be costly or impossible to obtain with real robots.",
    "chapter": 4,
    "section": "Simulation-to-Reality Transfer",
    "page": 6,
    "token_count": 104
  },
  {
    "chunk_id": "75b03c7f-fc57-4b42-ba8d-dd6bbd60a74f",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Isaac Sim Architecture",
    "chapter": 4,
    "section": "Isaac Sim Architecture",
    "page": 7,
    "token_count": 4
  },
  {
    "chunk_id": "0794336a-6786-416e-a956-e87371e067c1",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Omniverse Foundation\n\nIsaac Sim is built on NVIDIA's Omniverse, a simulation and collaboration platform for creating and sharing 3D virtual worlds:\n\n**USD (Universal Scene Description)**: A powerful scene description format that enables complex scene composition and asset management.\n\n**PhysX Engine**: NVIDIA's physics simulation engine providing realistic collision detection and response.\n\n**RTX Ray Tracing**: Hardware-accelerated ray tracing for photorealistic rendering and realistic sensor simulation.\n\n**Multi-GPU Support**: Ability to scale simulation performance across multiple GPUs.",
    "chapter": 4,
    "section": "Omniverse Foundation",
    "page": 8,
    "token_count": 109
  },
  {
    "chunk_id": "30f1de47-906a-4b57-92fe-241ec8f148f9",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Simulation Features\n\n**Extensive Robot Library**: Pre-built robot models with accurate kinematics, dynamics, and sensor configurations.\n\n**Environment Assets**: Large library of 3D assets for creating realistic environments.\n\n**Advanced Sensors**: High-fidelity simulation of cameras, LIDAR, IMU, and other sensors with realistic noise models.\n\n**Scenario Authoring**: Tools for creating complex training scenarios and testing environments.\n\n**Synthetic Data Generation**: Tools for generating large datasets with ground truth information for AI training.",
    "chapter": 4,
    "section": "Simulation Features",
    "page": 9,
    "token_count": 101
  },
  {
    "chunk_id": "d2e2cd81-4840-4ba6-8757-5a8c891ff304",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Python API\n\nIsaac Sim provides a comprehensive Python API for programmatic control and integration:\n\n```python\n# Example Isaac Sim Python API usage\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\n\n# Initialize the world\nmy_world = World(stage_units_in_meters=1.0)\n\n# Load a robot\nassets_root_path = get_assets_root_path()\nif assets_root_path is not None:\n    add_reference_to_stage(\n        usd_path=assets_root_path + \"/Isaac/Robots/Franka/franka.usd\",\n        prim_path=\"/World/Robot\"\n    )\n\n# Reset and step the world\nmy_world.reset()\nfor i in range(1000):\n    my_world.step(render=True)\n```",
    "chapter": 4,
    "section": "Python API",
    "page": 10,
    "token_count": 180
  },
  {
    "chunk_id": "1051ea44-43bf-4e0a-9a78-4265ef2890a0",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Isaac ROS Components",
    "chapter": 4,
    "section": "Isaac ROS Components",
    "page": 11,
    "token_count": 4
  },
  {
    "chunk_id": "9ed7175d-e520-41b2-9cf1-3a5aa29baf54",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Perception Packages\n\nIsaac ROS provides optimized implementations of essential perception algorithms:\n\n**Isaac ROS Apriltag**: Fast and accurate AprilTag detection for robot localization and calibration.\n\n**Isaac ROS DNN Inference**: GPU-accelerated deep learning inference for object detection and classification.\n\n**Isaac ROS Stereo DNN**: Stereo vision with deep neural networks for depth estimation.\n\n**Isaac ROS Visual SLAM**: Simultaneous localization and mapping algorithms optimized for GPU execution.\n\n**Isaac ROS Point Cloud**: Tools for processing and manipulating 3D point cloud data from various sensors.",
    "chapter": 4,
    "section": "Perception Packages",
    "page": 12,
    "token_count": 118
  },
  {
    "chunk_id": "01be1958-36b3-4222-b0bc-c1249b2a2da3",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Navigation and Planning\n\nThe Isaac ROS navigation stack includes:\n\n**Isaac ROS Navigation**: Hardware-accelerated navigation stack with GPU-optimized path planning.\n\n**Isaac ROS Controllers**: GPU-accelerated robot controllers for improved performance.\n\n**Isaac ROS Manipulation**: Tools for robot manipulation tasks including inverse kinematics and trajectory planning.",
    "chapter": 4,
    "section": "Navigation and Planning",
    "page": 13,
    "token_count": 69
  },
  {
    "chunk_id": "ee400643-83cc-45fe-908e-213323831514",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Communication and Integration\n\nIsaac ROS packages integrate seamlessly with ROS 2:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom isaac_ros_visual_slam_msgs.msg import IsaacROSVisualSLAMResults\nfrom geometry_msgs.msg import PoseStamped\n\nclass IsaacROSIntegrationNode(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_integration_node')\n        \n        # Subscribe to camera images\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        # Subscribe to Visual SLAM results\n        self.slam_subscription = self.create_subscription(\n            IsaacROSVisualSLAMResults,\n            '/visual_slam/trajectory',\n            self.slam_callback,\n            10\n        )\n        \n        # Publisher for robot commands\n        self.pose_publisher = self.create_publisher(\n            PoseStamped,\n            '/move_base_simple/goal',\n            10\n        )\n        \n    def image_callback(self, msg):\n        # Process image with Isaac ROS pipeline\n        self.get_logger().info('Received camera image')\n        \n    def slam_callback(self, msg):\n        # Process SLAM results and update navigation\n        self.get_logger().info('Received SLAM trajectory')\n```",
    "chapter": 4,
    "section": "Communication and Integration",
    "page": 14,
    "token_count": 269
  },
  {
    "chunk_id": "9492000c-d882-450e-9917-1ec70cc323fc",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Hardware Requirements",
    "chapter": 4,
    "section": "Hardware Requirements",
    "page": 15,
    "token_count": 3
  },
  {
    "chunk_id": "7c796125-f810-4c9c-af0d-d19f24a2ada7",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Recommended GPU Specifications\n\nIsaac technology requires specific NVIDIA GPU hardware for optimal performance:\n\n**Minimum Requirements**:\n- GPU: NVIDIA RTX 2060 or equivalent\n- VRAM: 6 GB\n- CUDA Compute Capability: 6.0 or higher\n\n**Recommended Requirements**:\n- GPU: NVIDIA RTX 3080, RTX 4080, or better\n- VRAM: 12 GB or more\n- CUDA Compute Capability: 7.5 or higher\n\n**High-Performance Requirements**:\n- GPU: NVIDIA RTX 6000 Ada, RTX 5000 Ada, or RTX A6000\n- VRAM: 24 GB or more\n- For Isaac Sim with multiple robots and complex environments",
    "chapter": 4,
    "section": "Recommended GPU Specifications",
    "page": 16,
    "token_count": 152
  },
  {
    "chunk_id": "75b32c35-7b46-468f-9dfb-9b979efc4ef1",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Jetson Platform Support\n\nIsaac ROS packages are optimized for NVIDIA Jetson platforms:\n\n**Jetson AGX Orin**: Best performance for complex perception and navigation tasks\n**Jetson Orin NX**: Good balance of performance and power consumption\n**Jetson Nano**: Entry-level platform suitable for simpler applications",
    "chapter": 4,
    "section": "Jetson Platform Support",
    "page": 17,
    "token_count": 62
  },
  {
    "chunk_id": "a04050f3-c5b4-47ba-a9fb-acf9be363af6",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Setup and Installation",
    "chapter": 4,
    "section": "Setup and Installation",
    "page": 18,
    "token_count": 4
  },
  {
    "chunk_id": "76673bde-6e57-4f3d-b1fb-49c9bf6c9a4c",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Installing Isaac Sim\n\nIsaac Sim can be installed on x86-64 Linux systems with compatible NVIDIA GPUs:\n\n```bash\n# Download Isaac Sim from NVIDIA Developer website\n# Install using the provided installer script\nchmod +x isaac-sim-*.*.*.run\nsudo ./isaac-sim-*.*.*.run\n\n# Or install via Omniverse launcher\n# Download Omniverse Code from NVIDIA Developer website\n```",
    "chapter": 4,
    "section": "Installing Isaac Sim",
    "page": 19,
    "token_count": 87
  },
  {
    "chunk_id": "cd8a8229-d9f0-4657-8bdc-30eca3252b27",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Installing Isaac ROS\n\nIsaac ROS packages are available as Ubuntu packages for ROS 2 Humble:\n\n```bash\n# Add NVIDIA package repository\ncurl -sL https://nvidia.github.io/nvidia-ros2-package-repos/rpm/nvidia-ros2-rpm.repo | \\\nsudo tee /etc/apt/sources.list.d/nvidia-ros2.list\n\n# Update package list and install Isaac ROS packages\nsudo apt update\nsudo apt install ros-humble-isaac-ros-*  # Install all Isaac ROS packages\nsudo apt install ros-humble-isaac-ros-visual-slam  # Install specific package\n```",
    "chapter": 4,
    "section": "Installing Isaac ROS",
    "page": 20,
    "token_count": 131
  },
  {
    "chunk_id": "b1f3eb7f-918e-4795-ac8f-df689346a0e7",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Verification Installation\n\n```bash\n# Check Isaac packages\ndpkg -l | grep isaac-ros\n\n# Launch a simple Isaac ROS node\nros2 launch isaac_ros_visual_slam visual_slam.launch.py\n\n# Verify GPU accessibility\nnvidia-smi\n```",
    "chapter": 4,
    "section": "Verification Installation",
    "page": 21,
    "token_count": 57
  },
  {
    "chunk_id": "aa584fda-4aa9-4e91-8603-fa8b4739b2b8",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Isaac Applications",
    "chapter": 4,
    "section": "Isaac Applications",
    "page": 22,
    "token_count": 3
  },
  {
    "chunk_id": "da9b60e2-8e96-4518-ba9f-557105ba82ca",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Isaac Apps\n\nNVIDIA provides several pre-built applications demonstrating Isaac capabilities:\n\n**Isaac Manipulator**: Complete manipulation stack with perception, planning, and control.\n\n**Isaac Navigation**: Full navigation stack with mapping, localization, and path planning.\n\n**Isaac Inspector**: Inspection application for quality control and monitoring.\n\n**Isaac Carter**: Mobile manipulator reference application.",
    "chapter": 4,
    "section": "Isaac Apps",
    "page": 23,
    "token_count": 73
  },
  {
    "chunk_id": "018b9a42-0653-4b83-b7ee-1d4b79dbf2f9",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Reference Implementations\n\nThe Isaac platform includes reference implementations that serve as starting points for custom applications:\n\n**Task Flow Architecture**: Modular architecture for complex robotic tasks.\n\n**Behavior Trees**: Hierarchical task execution for complex behaviors.\n\n**ROS 2 Integration**: Best practices for integrating Isaac components with ROS 2.",
    "chapter": 4,
    "section": "Reference Implementations",
    "page": 24,
    "token_count": 61
  },
  {
    "chunk_id": "218a46c3-9194-4733-bda4-99ac3d411938",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Integration with ROS 2",
    "chapter": 4,
    "section": "Integration with ROS 2",
    "page": 25,
    "token_count": 6
  },
  {
    "chunk_id": "7dc87823-2112-47b1-a93a-dd6af7f1c8f8",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### ROS 2 Ecosystem Compatibility\n\nIsaac components are designed to work seamlessly with ROS 2:\n\n**Standard Message Types**: Isaac packages use standard ROS 2 message types for easy integration.\n\n**Launch System**: Isaac packages include ROS 2 launch files for easy deployment.\n\n**Parameter System**: Full support for ROS 2 parameter configuration.\n\n**TF2 Integration**: Seamless integration with ROS 2's transform system.",
    "chapter": 4,
    "section": "ROS 2 Ecosystem Compatibility",
    "page": 26,
    "token_count": 83
  },
  {
    "chunk_id": "22e0a0e9-40b7-437a-842e-88193b2ffef7",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Example Integration\n\n```python\n# Example of integrating Isaac ROS with custom ROS 2 code\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom visualization_msgs.msg import MarkerArray\nfrom geometry_msgs.msg import Twist\n\nclass IsaacIntegrationDemo(Node):\n    def __init__(self):\n        super().__init__('isaac_integration_demo')\n        \n        # Camera data from Isaac Sim or real sensors\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.camera_callback, 10)\n        \n        # Camera info\n        self.info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.info_callback, 10)\n        \n        # Isaac ROS processing results\n        self.marker_sub = self.create_subscription(\n            MarkerArray, '/isaac_ros_detection_markers', \n            self.marker_callback, 10)\n        \n        # Robot control commands\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        \n    def camera_callback(self, msg):\n        # This image can be processed by Isaac ROS packages\n        self.get_logger().info('Camera image received')\n        \n    def marker_callback(self, msg):\n        # Process detection results from Isaac pipeline\n        for marker in msg.markers:\n            self.get_logger().info(f'Detected object: {marker.ns}')\n```",
    "chapter": 4,
    "section": "Example Integration",
    "page": 27,
    "token_count": 287
  },
  {
    "chunk_id": "b0eb517f-7a14-4f03-8a6f-9dc0ab19156e",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Diagrams",
    "chapter": 4,
    "section": "Diagrams",
    "page": 28,
    "token_count": 3
  },
  {
    "chunk_id": "f4f66aa1-c789-4c87-9876-82be2efec671",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Isaac Technology Stack",
    "chapter": 4,
    "section": "Isaac Technology Stack",
    "page": 29,
    "token_count": 4
  },
  {
    "chunk_id": "2aa0df5b-1fc4-4ad7-b871-1b0a45ccc13d",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Isaac Technology Stack\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        NVIDIA ISAAC STACK                       │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │\n│  │   ISAAC SIM     │  │  ISAAC ROS      │  │   ISAAC LAB     │  │\n│  │ (Simulation)    │  │ (Deployment)    │  │ (Research)      │  │\n│  │                 │  │                 │  │                 │  │\n│  │ • Photorealism  │  │ • Perception    │  │ • Reinforcement │  │\n│  │ • Physics       │  │ • Navigation    │  │   Learning      │  │\n│  │ • Sensor Sim    │  │ • Manipulation  │  │ • Advanced      │  │\n│  │ • Synthetic     │  │ • Control       │  │   Algorithms    │  │\n│  │   Data Gen      │  │                 │  │                 │  │\n│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │\n│               │                  │                    │         │\n│               └──────────────────┼────────────────────┘         │\n│                                  │                              │\n│                    ┌─────────────────┐                         │\n│                    │  ISAAC APPS     │                         │\n│                    │ (Reference Apps)│                         │\n│                    │ • Manipulation  │                         │\n│                    │ • Navigation    │                         │\n│                    │ • Inspection    │                         │\n│                    └─────────────────┘                         │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                   HARDWARE LAYER                        │   │\n│  │  ┌───────────┐  ┌─────────────┐  ┌─────────────────┐   │   │\n│  │  │   RTX     │  │  JETSON     │  │   DRIVE         │   │   │\n│  │  │   GPUS    │  │   PLATFORM  │  │   PLATFORMS     │   │   │\n│  │  │           │  │             │  │                 │   │   │\n│  │  │ • Tensor  │  │ • Orin      │  │ • Autonomous    │   │   │\n│  │  │   Cores   │  │ • AGX       │  │   Vehicles      │   │   │\n│  │  │ • RTX     │  │ • Nano      │  │ • Industrial    │   │   │\n│  │  │   Ray     │  │ • Xavier    │  │   Automation    │   │   │\n│  │  │   Tracing │  │             │  │                 │   │   │\n│  │  └───────────┘  └─────────────┘  └─────────────────┘   │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 4,
    "section": "Isaac Technology Stack",
    "page": 30,
    "token_count": 733
  },
  {
    "chunk_id": "22addaa4-b3c5-4c69-b5d8-1cdd2e2b8934",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Isaac Sim Architecture",
    "chapter": 4,
    "section": "Isaac Sim Architecture",
    "page": 31,
    "token_count": 4
  },
  {
    "chunk_id": "79810941-0deb-427c-bbe7-a810c660736f",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Isaac Sim Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     ISAAC SIM ARCHITECTURE                      │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │\n│  │   OMNIVERSE     │  │   RTX RENDER    │  │   PHYSX         │  │\n│  │   PLATFORM      │  │   ENGINE        │  │   PHYSICS       │  │\n│  │                 │  │                 │  │                 │  │\n│  │ • USD Scene     │  │ • Ray Tracing   │  │ • Collision     │  │\n│  │   Description   │  │ • Global        │  │ • Dynamics      │  │\n│  │ • Multi-User    │  │   Illumination  │  │ • Constraints   │  │\n│  │   Collaboration │  │ • Materials     │  │ • Joints        │  │\n│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │\n│               │                  │                    │         │\n│               └──────────────────┼────────────────────┘         │\n│                                  │                              │\n│                    ┌─────────────────────────────────────────┐  │\n│                    │         SIMULATION CORE               │  │\n│                    │  ┌─────────────────────────────────┐   │  │\n│                    │  │ • Robot Models                  │   │  │\n│                    │  │ • Sensor Simulation             │   │  │\n│                    │  │ • Environment Assets            │   │  │\n│                    │  │ • Physics Configurations        │   │  │\n│                    │  └─────────────────────────────────┘   │  │\n│                    └─────────────────────────────────────────┘  │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                 PYTHON API                              │   │\n│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────┐   │   │\n│  │  │   SCENE         │  │    ROBOT        │  │  TASK   │   │   │\n│  │  │   MANAGEMENT    │  │   CONTROL       │  │  FLOW   │   │   │\n│  │  │                 │  │                 │  │         │   │   │\n│  │  │ • Asset Loading │  │ • Kinematics    │  │ •       │   │   │\n│  │  │ • Placement     │  │ • Dynamics      │  │ Behavior│   │   │\n│  │  │ • Lighting      │  │ • Control       │  │ Trees   │   │   │\n│  │  └─────────────────┘  └─────────────────┘  └─────────┘   │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 4,
    "section": "Isaac Sim Architecture",
    "page": 32,
    "token_count": 716
  },
  {
    "chunk_id": "0fdc3f6b-3890-498e-8d94-ddd72607e872",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Describe the NVIDIA Isaac technology stack and its components\n2. Explain the benefits of hardware acceleration for robotics applications\n3. Identify the differences between Isaac Sim and Isaac ROS use cases\n4. Install and verify Isaac software components\n5. Understand the integration between Isaac and ROS 2 systems",
    "chapter": 4,
    "section": "Learning Outcomes",
    "page": 33,
    "token_count": 75
  },
  {
    "chunk_id": "63bd73eb-a238-4d9b-81c2-2ae7ef8dbe8a",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Advanced Topics",
    "chapter": 4,
    "section": "Advanced Topics",
    "page": 34,
    "token_count": 3
  },
  {
    "chunk_id": "1a2cc2b4-ccab-4ca8-8875-6ff4ef2ebfe8",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Isaac Sim Extensions\n\n**Custom Sensors**: Creating specialized sensor models for specific applications\n**Plugin Development**: Extending Isaac Sim functionality\n**Multi-robot Simulation**: Coordinating multiple robots in simulation\n**Cloud Deployment**: Running Isaac Sim on cloud-based GPU instances",
    "chapter": 4,
    "section": "Isaac Sim Extensions",
    "page": 35,
    "token_count": 53
  },
  {
    "chunk_id": "bedce76b-be9a-4ca1-8991-e5b47084d45c",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "### Isaac ROS Optimization\n\n**CUDA Integration**: Maximizing GPU utilization for AI inference\n**Memory Management**: Optimizing memory usage for embedded systems\n**Real-time Performance**: Ensuring deterministic execution for safety-critical applications\n**Edge Deployment**: Deploying Isaac ROS on resource-constrained platforms",
    "chapter": 4,
    "section": "Isaac ROS Optimization",
    "page": 36,
    "token_count": 57
  },
  {
    "chunk_id": "a8f41390-913d-4ef1-9487-ea00c7defebf",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## Further Reading\n\n- \"NVIDIA Isaac: Accelerating Robotics with AI\" (NVIDIA Developer Documentation, 2025) [1]\n- \"Isaac Sim: A High-Fidelity Simulation Platform\" (IEEE Robotics & Automation Magazine, 2024) [2]\n- \"Hardware-Accelerated Perception for Robotics\" (Journal of Field Robotics, 2024) [3]\n\n---",
    "chapter": 4,
    "section": "Further Reading",
    "page": 37,
    "token_count": 81
  },
  {
    "chunk_id": "79c41d13-9398-44a9-8160-bc49ca9f7bd2",
    "doc_id": "931171bc-ffc6-487a-9320-89dc9fcf7b89",
    "chunk_text": "## References\n\n[1] NVIDIA Corporation. \"NVIDIA Isaac: Accelerating Robotics with AI.\" NVIDIA Developer Documentation. 2025. https://developer.nvidia.com/isaac\n\n[2] NVIDIA Corporation. \"Isaac Sim: Technical Overview.\" NVIDIA Isaac Sim Documentation. 2024. https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html\n\n[3] Patel, S., et al. \"Hardware-Accelerated Perception for Autonomous Robotics.\" Journal of Field Robotics, vol. 41, no. 2, pp. 234-251, 2024.",
    "chapter": 4,
    "section": "References",
    "page": 38,
    "token_count": 126
  },
  {
    "chunk_id": "7287643f-cee5-49f9-b97d-b8c5c383cd58",
    "doc_id": "13eab601-d3b3-4ef5-9cc7-f25e3989b62f",
    "chunk_text": "# Photorealistic Simulation\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 15
  },
  {
    "chunk_id": "66e6d1b2-a327-439f-9fee-067c7ea5fee3",
    "doc_id": "13eab601-d3b3-4ef5-9cc7-f25e3989b62f",
    "chunk_text": "## Overview\n\nThis section covers photorealistic rendering in Isaac Sim for training vision-based AI models.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 20
  },
  {
    "chunk_id": "fd7f64b6-85b8-4228-becf-481ef3caa0d3",
    "doc_id": "13eab601-d3b3-4ef5-9cc7-f25e3989b62f",
    "chunk_text": "## Key Concepts\n\n- Ray tracing and path tracing\n- Material properties and lighting\n- Synthetic data generation\n- Domain randomization\n- Replicator for large-scale data synthesis",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 36
  },
  {
    "chunk_id": "344f9669-6359-4f6c-9b31-e45ed838ab37",
    "doc_id": "13eab601-d3b3-4ef5-9cc7-f25e3989b62f",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "9293685a-a526-4ffc-8ffa-3f046f5ce383",
    "doc_id": "13eab601-d3b3-4ef5-9cc7-f25e3989b62f",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "7e3104d9-f272-4aba-bee5-6358a8cfecb3",
    "doc_id": "ccaf9179-e139-4b48-a634-1f0b821541a6",
    "chunk_text": "# Sim-to-Real Transfer\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 4,
    "section": "Introduction",
    "page": 1,
    "token_count": 16
  },
  {
    "chunk_id": "6add5f7c-ba60-4dfd-b0f8-fe9cc01419c1",
    "doc_id": "ccaf9179-e139-4b48-a634-1f0b821541a6",
    "chunk_text": "## Overview\n\nThis section explores techniques for transferring policies and models from simulation to real robots.",
    "chapter": 4,
    "section": "Overview",
    "page": 2,
    "token_count": 18
  },
  {
    "chunk_id": "2691b7a4-ca5a-4b9f-8193-6e825b82738f",
    "doc_id": "ccaf9179-e139-4b48-a634-1f0b821541a6",
    "chunk_text": "## Key Concepts\n\n- The reality gap challenge\n- Domain randomization strategies\n- System identification\n- Validation and testing procedures\n- Iterative refinement",
    "chapter": 4,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 30
  },
  {
    "chunk_id": "72330d05-0cc7-4ac0-8e05-fb51fcd03720",
    "doc_id": "ccaf9179-e139-4b48-a634-1f0b821541a6",
    "chunk_text": "## Examples\n\n<!-- Add practical examples here -->",
    "chapter": 4,
    "section": "Examples",
    "page": 4,
    "token_count": 9
  },
  {
    "chunk_id": "8867e7a5-653e-47ea-b0b0-364c6650b057",
    "doc_id": "ccaf9179-e139-4b48-a634-1f0b821541a6",
    "chunk_text": "## Diagrams\n\n<!-- Add diagrams and visualizations here -->",
    "chapter": 4,
    "section": "Diagrams",
    "page": 5,
    "token_count": 12
  },
  {
    "chunk_id": "e8357acb-1b12-4eea-9837-b8002b4f7804",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "# Chapter 5: Vision-Language-Action (VLA)",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "11f11a46-1fba-4cb5-af70-4f6661543047",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Chapter Overview\nThis chapter explores the integration of vision, language, and action systems to create robots that can understand and execute complex, natural language commands in real-world environments. Vision-Language-Action (VLA) systems represent the next generation of human-robot interaction, where robots can perceive their environment, understand linguistic instructions, and execute appropriate actions. The chapter covers the architecture of VLA systems, multimodal AI models, safety considerations, and practical implementation using modern AI frameworks.\n\nVLA systems combine computer vision for environmental perception, natural language processing for command understanding, and robot control for action execution. This integration enables robots to operate in unstructured human environments using natural language interfaces, moving beyond pre-programmed behaviors to responsive, interactive systems that can handle novel situations and tasks.",
    "chapter": 5,
    "section": "Chapter Overview",
    "page": 2,
    "token_count": 155
  },
  {
    "chunk_id": "863563b8-8661-4530-bace-0ca6e10bba07",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Learning Objectives\nAfter completing this chapter, students will be able to:\n1. Understand the architecture and components of Vision-Language-Action systems\n2. Implement multimodal AI models that process vision and language inputs\n3. Design natural language interfaces for robot control\n4. Create action planning systems that respond to linguistic commands\n5. Integrate perception, language understanding, and action execution\n6. Address safety and ethical considerations in VLA systems",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 90
  },
  {
    "chunk_id": "57094548-1cf2-408a-85b0-71666eb5c392",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Section Breakdown",
    "chapter": 5,
    "section": "Section Breakdown",
    "page": 4,
    "token_count": 4
  },
  {
    "chunk_id": "14c63fa6-d36e-42bd-8a93-a4b801a98d3e",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "### Section 1: Introduction to Vision-Language-Action Systems\n- Definition and importance of VLA systems\n- Historical context and evolution from simple command interfaces\n- Key challenges in multimodal AI integration\n- Architectural patterns for VLA systems\n- Examples of VLA applications in robotics",
    "chapter": 5,
    "section": "Section 1: Introduction to Vision-Language-Action Systems",
    "page": 5,
    "token_count": 58
  },
  {
    "chunk_id": "1b881b1a-7a83-4c38-9a9a-5f3ed06e19ea",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "### Section 2: Multimodal AI Models and Architectures\n- Vision-language models (CLIP, BLIP, etc.)\n- Large Language Models for robot interaction\n- Multimodal transformers and attention mechanisms\n- Training approaches for VLA systems\n- Integration of visual and linguistic representations",
    "chapter": 5,
    "section": "Section 2: Multimodal AI Models and Architectures",
    "page": 6,
    "token_count": 59
  },
  {
    "chunk_id": "59870757-23c5-484f-b6c1-6d93a13a1d40",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "### Section 3: Natural Language Understanding for Robotics\n- Parsing natural language commands\n- Grounding linguistic concepts in visual space\n- Task decomposition and planning from language\n- Handling ambiguity and uncertainty in commands\n- Context-aware language processing",
    "chapter": 5,
    "section": "Section 3: Natural Language Understanding for Robotics",
    "page": 7,
    "token_count": 47
  },
  {
    "chunk_id": "fa6e5725-389f-4fc3-b64e-c1628dbe039d",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "### Section 4: Vision Processing for VLA Systems\n- Object detection and recognition for robot tasks\n- Scene understanding and spatial reasoning\n- Visual affordance detection\n- Integration of camera and depth information\n- Real-time processing considerations",
    "chapter": 5,
    "section": "Section 4: Vision Processing for VLA Systems",
    "page": 8,
    "token_count": 47
  },
  {
    "chunk_id": "d0937783-f111-4479-97c9-b86b98a1bbd5",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "### Section 5: Action Planning and Execution\n- Converting language commands to robot actions\n- Task planning with multimodal inputs\n- Safety considerations and action filtering\n- Closed-loop control with perceptual feedback\n- Human-in-the-loop validation",
    "chapter": 5,
    "section": "Section 5: Action Planning and Execution",
    "page": 9,
    "token_count": 49
  },
  {
    "chunk_id": "249629fc-f9be-4a41-8824-c5fc92aed3f7",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Estimated Length\n- Total word count: ~10,000-12,000 words\n- Reading time: 3-4 hours\n- Hands-on practice time: 6-8 hours",
    "chapter": 5,
    "section": "Estimated Length",
    "page": 10,
    "token_count": 40
  },
  {
    "chunk_id": "8ee2ed2e-abc8-4d87-a99b-f6359e43689d",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Prerequisites\n- Chapter 4: Understanding of perception and navigation systems\n- Knowledge of deep learning and neural networks\n- Familiarity with ROS 2 for robot control\n- Basic understanding of natural language processing",
    "chapter": 5,
    "section": "Prerequisites",
    "page": 11,
    "token_count": 44
  },
  {
    "chunk_id": "12c10f4d-97ad-461f-9e7b-4ac1dee119d8",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Technology Requirements\n- Modern GPU for deep learning inference\n- Robot platform with camera and manipulator (simulated or real)\n- LLM API access (e.g., OpenAI, Anthropic) or local models\n- Computer vision libraries (OpenCV, etc.)",
    "chapter": 5,
    "section": "Technology Requirements",
    "page": 12,
    "token_count": 55
  },
  {
    "chunk_id": "a43582ca-5928-409a-9259-44b6a9738a5e",
    "doc_id": "9a07dfb8-0a56-458c-941c-365911f1c94e",
    "chunk_text": "## Code Examples Included\n- Multimodal model implementations\n- Natural language processing pipelines\n- Vision-language integration examples\n- Robot control interfaces\n- Safety filtering implementations",
    "chapter": 5,
    "section": "Code Examples Included",
    "page": 13,
    "token_count": 33
  },
  {
    "chunk_id": "3de099c2-972b-4330-8466-63a66998a4cd",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "# Chapter 1: Introduction to Vision-Language-Action Systems\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 31
  },
  {
    "chunk_id": "9af55c84-20ea-40fc-99f4-c3ce0cd6686b",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Write 2-3 paragraph summary explaining what this chapter covers -->\n\nThis chapter introduces Vision-Language-Action (VLA) systems and their role in modern robotics. Students will learn the fundamental architecture of VLA systems, understand the differences between related technologies (VLMs, LVLMs), and explore real-world applications.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 71
  },
  {
    "chunk_id": "1b054ec5-2f2c-4f37-970f-ca15aecc0faf",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Learning Objectives\n\nBy the end of this chapter, students will be able to:\n\n- [ ] Define Vision-Language-Action (VLA) systems and explain their components\n- [ ] Differentiate between VLMs (Vision-Language Models), LVLMs (Large Vision-Language Models), and VLAs\n- [ ] Identify real-world applications of VLA systems in robotics\n- [ ] Describe the VLA workflow: vision → language understanding → action planning\n- [ ] Explain why VLA systems are important for humanoid robotics",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 107
  },
  {
    "chunk_id": "5775d142-8f87-40d0-a59f-f70a6492d4ac",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Prerequisites\n\n- Basic understanding of artificial intelligence and machine learning\n- Completion of Module 2 (ROS 2 Fundamentals)\n- Familiarity with computer vision concepts (helpful but not required)\n\n---",
    "chapter": 5,
    "section": "Prerequisites",
    "page": 4,
    "token_count": 43
  },
  {
    "chunk_id": "a78e781b-4f04-4208-80e3-590648d0bdea",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## 1.1 What are Vision-Language-Action Systems?\n\n<!-- TODO: Explain VLA systems definition and core concept -->",
    "chapter": 5,
    "section": "1.1 What are Vision-Language-Action Systems?",
    "page": 5,
    "token_count": 25
  },
  {
    "chunk_id": "17ffa445-1b0e-44ed-9af7-b94a90b9b614",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Key Concepts\n\n**Vision-Language-Action (VLA)**:\n<!-- TODO: Provide clear definition -->\n\n**Vision Component**:\n<!-- TODO: Explain role of visual perception -->\n\n**Language Component**:\n<!-- TODO: Explain natural language understanding -->\n\n**Action Component**:\n<!-- TODO: Explain action planning and execution -->",
    "chapter": 5,
    "section": "Key Concepts",
    "page": 6,
    "token_count": 62
  },
  {
    "chunk_id": "5887bc9c-e86a-48bf-a809-37c62d7098bb",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Historical Context\n\n<!-- TODO: Brief history of VLA development -->\n\n---",
    "chapter": 5,
    "section": "Historical Context",
    "page": 7,
    "token_count": 15
  },
  {
    "chunk_id": "a619299e-8486-4abf-80dc-7469a99889b6",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## 1.2 Differences Between VLMs, LVLMs, and VLAs\n\n<!-- TODO: Explain distinctions between these technologies -->",
    "chapter": 5,
    "section": "1.2 Differences Between VLMs, LVLMs, and VLAs",
    "page": 8,
    "token_count": 28
  },
  {
    "chunk_id": "c9a66172-0ab1-4202-84b4-5f0ee73bc957",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Vision-Language Models (VLMs)\n\n**Definition**:\n<!-- TODO: Define VLMs -->\n\n**Capabilities**:\n<!-- TODO: List what VLMs can do -->\n\n**Limitations**:\n<!-- TODO: What VLMs cannot do -->",
    "chapter": 5,
    "section": "Vision-Language Models (VLMs)",
    "page": 9,
    "token_count": 51
  },
  {
    "chunk_id": "a706e1c4-b921-4604-9cb0-ef7f3c4c929f",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Large Vision-Language Models (LVLMs)\n\n**Definition**:\n<!-- TODO: Define LVLMs -->\n\n**Examples**:\n<!-- TODO: List examples like GPT-4V, Claude 3, Gemini Vision -->",
    "chapter": 5,
    "section": "Large Vision-Language Models (LVLMs)",
    "page": 10,
    "token_count": 45
  },
  {
    "chunk_id": "d5d5877b-fd03-4d16-abde-c96e4b2f230c",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Vision-Language-Action Systems (VLAs)\n\n**Definition**:\n<!-- TODO: Define VLAs and what makes them different -->\n\n**Key Distinguishing Features**:\n<!-- TODO: List features unique to VLAs -->\n\n---",
    "chapter": 5,
    "section": "Vision-Language-Action Systems (VLAs)",
    "page": 11,
    "token_count": 45
  },
  {
    "chunk_id": "d7175ea1-c8bb-441a-80aa-7ae83d53a961",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## 1.3 Real-World VLA Applications\n\n<!-- TODO: Document real-world VLA applications with examples -->",
    "chapter": 5,
    "section": "1.3 Real-World VLA Applications",
    "page": 12,
    "token_count": 24
  },
  {
    "chunk_id": "50720581-545f-47a7-a456-9e84721df722",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### RT-2 (Robotic Transformer 2)\n\n<!-- TODO: Describe Google's RT-2 system -->",
    "chapter": 5,
    "section": "RT-2 (Robotic Transformer 2)",
    "page": 13,
    "token_count": 22
  },
  {
    "chunk_id": "b1676ba6-70da-4ceb-8487-e8e28f3f3123",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### PaLM-E (Pathways Language Model - Embodied)\n\n<!-- TODO: Describe Google's PaLM-E system -->",
    "chapter": 5,
    "section": "PaLM-E (Pathways Language Model - Embodied)",
    "page": 14,
    "token_count": 24
  },
  {
    "chunk_id": "3115fad3-d396-48b6-8307-b94aee260a91",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Other VLA Systems\n\n<!-- TODO: List and briefly describe other notable VLA systems -->\n\n---",
    "chapter": 5,
    "section": "Other VLA Systems",
    "page": 15,
    "token_count": 20
  },
  {
    "chunk_id": "9a9b46de-4d89-4c42-bcf1-f970dad102b7",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## 1.4 VLA Workflow and Architecture\n\n<!-- TODO: Explain the complete VLA pipeline -->",
    "chapter": 5,
    "section": "1.4 VLA Workflow and Architecture",
    "page": 16,
    "token_count": 21
  },
  {
    "chunk_id": "833d64eb-5953-4d52-ae30-ae0af3804b57",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Vision → Language → Action Flow\n\n```\n[Visual Input] → [Perception] → [Scene Understanding]\n    ↓\n[Language Understanding] → [Command Interpretation]\n    ↓\n[High-Level Planning] → [Action Generation]\n    ↓\n[Action Execution] → [Robot Behavior]\n```\n\n<!-- TODO: Explain each step in detail -->",
    "chapter": 5,
    "section": "Vision → Language → Action Flow",
    "page": 17,
    "token_count": 73
  },
  {
    "chunk_id": "3883da45-479a-45d7-a6c0-5c62d9d3c2b1",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Architecture Diagram\n\n<!-- TODO: Create ASCII/text diagram showing VLA components -->\n\n```\n+------------------+\n|  Camera Sensors  |\n+--------+---------+\n         |\n         v\n+------------------+      +--------------------+\n| Vision Encoder   |      | Language Model     |\n| (Perception)     +----->+ (Understanding)    |\n+------------------+      +---------+----------+\n                                    |\n                                    v\n                          +--------------------+\n                          | Action Planner     |\n                          | (High-Level Tasks) |\n                          +---------+----------+\n                                    |\n                                    v\n                          +--------------------+\n                          | Robot Executor     |\n                          | (Behavior Trees)   |\n                          +--------------------+\n```\n\n---",
    "chapter": 5,
    "section": "Architecture Diagram",
    "page": 18,
    "token_count": 139
  },
  {
    "chunk_id": "2c505d83-9a5a-4aec-9cd9-6b35680a51b0",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## 1.5 Why VLA Systems Matter for Humanoid Robotics\n\n<!-- TODO: Explain importance and benefits -->",
    "chapter": 5,
    "section": "1.5 Why VLA Systems Matter for Humanoid Robotics",
    "page": 19,
    "token_count": 23
  },
  {
    "chunk_id": "dc995f73-d14d-40d3-a502-292f26827e47",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Natural Human-Robot Interaction\n\n<!-- TODO: Discuss how VLAs enable natural communication -->",
    "chapter": 5,
    "section": "Natural Human-Robot Interaction",
    "page": 20,
    "token_count": 18
  },
  {
    "chunk_id": "8b615436-117a-4873-a530-93d8de9668f0",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Generalization and Adaptation\n\n<!-- TODO: Explain how VLAs help robots adapt to new situations -->",
    "chapter": 5,
    "section": "Generalization and Adaptation",
    "page": 21,
    "token_count": 21
  },
  {
    "chunk_id": "c874dc74-d8f9-4d42-863b-65ccc2c6c195",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "### Reducing Programming Burden\n\n<!-- TODO: Discuss how VLAs simplify robot programming -->\n\n---",
    "chapter": 5,
    "section": "Reducing Programming Burden",
    "page": 22,
    "token_count": 19
  },
  {
    "chunk_id": "8c748d58-440c-467f-a840-466646453e18",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Key Takeaways\n\n<!-- TODO: Summarize main points from chapter -->\n\n- VLA systems integrate vision, language, and action for robot control\n- VLAs differ from VLMs/LVLMs by including action planning and execution\n- Real-world systems like RT-2 and PaLM-E demonstrate VLA capabilities\n- VLA workflow: perception → understanding → planning → execution\n- VLAs enable more natural and flexible human-robot interaction\n\n---",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 23,
    "token_count": 94
  },
  {
    "chunk_id": "6c5cf395-f92f-4a12-93d4-24c7a2313c87",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Review Questions\n\n<!-- TODO: Create assessment questions -->\n\n1. **Conceptual**: What are the three main components of a VLA system? Explain the role of each.\n\n2. **Comparison**: How do VLAs differ from VLMs? Why is the action component critical?\n\n3. **Application**: Describe one real-world application of VLA systems. What problem does it solve?\n\n4. **Analysis**: Why are VLA systems particularly valuable for humanoid robotics compared to traditional programming approaches?\n\n5. **Critical Thinking**: What are potential limitations or risks of using LLMs to control robots?\n\n---",
    "chapter": 5,
    "section": "Review Questions",
    "page": 24,
    "token_count": 121
  },
  {
    "chunk_id": "49afb88a-8f35-4987-8195-0ff13d407ed0",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Further Reading\n\n<!-- TODO: Add references and additional resources -->\n\n- **Academic Papers**:\n  - RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\n  - PaLM-E: An Embodied Multimodal Language Model\n\n- **Industry Resources**:\n  - Google AI Blog: Robotics Transformer 2\n  - Robotics research from major AI labs\n\n- **Related Topics**:\n  - Embodied AI\n  - Multimodal learning\n  - Robot learning from demonstrations\n\n---",
    "chapter": 5,
    "section": "Further Reading",
    "page": 25,
    "token_count": 107
  },
  {
    "chunk_id": "84b7243f-8f98-4c89-b18e-5beca475de60",
    "doc_id": "6d1e30ac-42d8-4fac-ae91-8c36fb9b1b31",
    "chunk_text": "## Next Chapter\n\n[Chapter 2: Robot Perception Pipeline for VLA →](/docs/chapter5/module4-vla/ch02-perception/)\n\nLearn how to process visual inputs (RGB, depth, segmentation) for VLA systems using ROS 2.",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 26,
    "token_count": 54
  },
  {
    "chunk_id": "0f4142c2-648f-432a-9742-111cc472acb4",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "# Chapter 2: Robot Perception Pipeline for VLA\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 30
  },
  {
    "chunk_id": "36a462c3-952f-4a6a-a9e3-562b53c4b903",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Explain perception pipeline for VLA systems -->\n\nThis chapter covers the perception pipeline for VLA systems, including RGB processing, depth integration, and lightweight segmentation. Students will learn to process visual data from ROS 2 camera topics and extract semantic information for robot decision-making.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 59
  },
  {
    "chunk_id": "0db6e72d-6bed-4c32-9626-9b0184341106",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## Learning Objectives\n\n- [ ] Process RGB camera data in ROS 2 for VLA systems\n- [ ] Integrate depth information with RGB for 3D scene understanding\n- [ ] Implement lightweight segmentation using MobileNet-based models\n- [ ] Extract object information (bounding boxes, categories) from visual data\n- [ ] Validate perception accuracy using the mini dataset",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 75
  },
  {
    "chunk_id": "2e02130e-9d5a-4b17-b3b2-47db94d3a6a3",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## Prerequisites\n\n- Chapter 1: Introduction to VLA Systems\n- ROS 2 basics (topics, messages, nodes)\n- Basic Python and OpenCV knowledge\n\n---",
    "chapter": 5,
    "section": "Prerequisites",
    "page": 4,
    "token_count": 36
  },
  {
    "chunk_id": "c8d2da75-6131-4479-9b5d-a134665d39a2",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## 2.1 Perception Pipeline Overview\n\n<!-- TODO: Overview of perception in VLA systems -->",
    "chapter": 5,
    "section": "2.1 Perception Pipeline Overview",
    "page": 5,
    "token_count": 20
  },
  {
    "chunk_id": "ecb6698e-362c-4a12-9743-97c775b383b9",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### VLA Perception Requirements\n\n<!-- TODO: What perception data does VLA need? -->",
    "chapter": 5,
    "section": "VLA Perception Requirements",
    "page": 6,
    "token_count": 18
  },
  {
    "chunk_id": "71fa078c-391b-404c-8d4a-9020ced0d05d",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### ROS 2 Camera Topics\n\n<!-- TODO: Explain sensor_msgs/Image, sensor_msgs/CameraInfo -->\n\n---",
    "chapter": 5,
    "section": "ROS 2 Camera Topics",
    "page": 7,
    "token_count": 22
  },
  {
    "chunk_id": "3846a433-4e6b-4efa-867f-93b1723978cb",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## 2.2 RGB Camera Processing\n\n<!-- TODO: Content from T023 - RGB processing fundamentals -->",
    "chapter": 5,
    "section": "2.2 RGB Camera Processing",
    "page": 8,
    "token_count": 21
  },
  {
    "chunk_id": "69021fe7-eaca-42dc-82e3-7673d8526e3c",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Subscribing to Camera Topics\n\n```python\n# TODO: Example ROS 2 subscriber for RGB camera\n```",
    "chapter": 5,
    "section": "Subscribing to Camera Topics",
    "page": 9,
    "token_count": 24
  },
  {
    "chunk_id": "7e705ebe-2c35-4ded-9309-e9b0fdf9085c",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Image Processing with OpenCV\n\n```python\n# TODO: OpenCV image processing examples\n```\n\n**Code Example**: See [rgb_processor.py](code-examples/rgb_processor.py)\n\n---",
    "chapter": 5,
    "section": "Image Processing with OpenCV",
    "page": 10,
    "token_count": 40
  },
  {
    "chunk_id": "c7d32ef2-a78e-4953-b04b-01acd95412d5",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## 2.3 Depth Data Integration\n\n<!-- TODO: Content from T024 - Depth integration -->",
    "chapter": 5,
    "section": "2.3 Depth Data Integration",
    "page": 11,
    "token_count": 20
  },
  {
    "chunk_id": "0fa7cc55-7c92-4496-8841-0a39d29e6da0",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Depth Maps and Point Clouds\n\n<!-- TODO: Explain depth representation -->",
    "chapter": 5,
    "section": "Depth Maps and Point Clouds",
    "page": 12,
    "token_count": 15
  },
  {
    "chunk_id": "f5710450-ff9e-4807-b84f-c6cea0240ab9",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### RGB-D Fusion\n\n```python\n# TODO: Example of combining RGB and depth\n```\n\n**Code Example**: See [depth_processor.py](code-examples/depth_processor.py)\n\n---",
    "chapter": 5,
    "section": "RGB-D Fusion",
    "page": 13,
    "token_count": 39
  },
  {
    "chunk_id": "d4a202f7-eed7-439b-aeed-fbb2b9a7d786",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## 2.4 Lightweight Segmentation\n\n<!-- TODO: Content from T025 - Segmentation with MobileNet -->",
    "chapter": 5,
    "section": "2.4 Lightweight Segmentation",
    "page": 14,
    "token_count": 23
  },
  {
    "chunk_id": "6035fa3f-4649-45e6-a851-7d0f8aa5ebf6",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Why Lightweight Models?\n\n<!-- TODO: Explain hardware constraints and performance -->",
    "chapter": 5,
    "section": "Why Lightweight Models?",
    "page": 15,
    "token_count": 14
  },
  {
    "chunk_id": "8c5842b0-4ef7-4c71-820e-0625c0bd0d9d",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### MobileNet-Based Segmentation\n\n```python\n# TODO: Example using pretrained MobileNet for segmentation\n```\n\n**Code Example**: See [segmentation_node.py](code-examples/segmentation_node.py)\n\n---",
    "chapter": 5,
    "section": "MobileNet-Based Segmentation",
    "page": 16,
    "token_count": 44
  },
  {
    "chunk_id": "32aa3f65-0c3d-4a53-b5db-13d865e6a3dc",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## 2.5 Object Detection and Extraction\n\n<!-- TODO: Extracting semantic information -->",
    "chapter": 5,
    "section": "2.5 Object Detection and Extraction",
    "page": 17,
    "token_count": 18
  },
  {
    "chunk_id": "d1769a20-1b5c-4d04-88a8-22150f41fc12",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Bounding Box Detection\n\n<!-- TODO: Object detection techniques -->",
    "chapter": 5,
    "section": "Bounding Box Detection",
    "page": 18,
    "token_count": 13
  },
  {
    "chunk_id": "10149b57-dbd9-4d47-92b6-3eb3708cead7",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Object Category Classification\n\n<!-- TODO: Classification approaches -->\n\n---",
    "chapter": 5,
    "section": "Object Category Classification",
    "page": 19,
    "token_count": 12
  },
  {
    "chunk_id": "16e40867-4992-4877-bb98-c212a8fe0dcf",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## 2.6 Validation with Mini Dataset\n\n<!-- TODO: Using the mini dataset for testing -->",
    "chapter": 5,
    "section": "2.6 Validation with Mini Dataset",
    "page": 20,
    "token_count": 20
  },
  {
    "chunk_id": "20fa4de3-8457-470f-9662-375a0fa23e52",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Dataset Structure\n\n<!-- TODO: Explain dataset format -->",
    "chapter": 5,
    "section": "Dataset Structure",
    "page": 21,
    "token_count": 11
  },
  {
    "chunk_id": "bd42d6c7-7439-4976-a483-4dfa9d20e3eb",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "### Validation Workflow\n\n```bash\n# TODO: Commands to validate perception on dataset\n```\n\n---",
    "chapter": 5,
    "section": "Validation Workflow",
    "page": 22,
    "token_count": 20
  },
  {
    "chunk_id": "d16b5fc0-e678-405d-b977-3b4e56147924",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## Key Takeaways\n\n- Perception pipeline: RGB → Depth → Segmentation → Scene Understanding\n- ROS 2 provides standardized interfaces for camera data\n- Lightweight models enable real-time perception on mid-range hardware\n- Validation with known data ensures perception accuracy",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 23,
    "token_count": 50
  },
  {
    "chunk_id": "9f743da3-0a1b-425d-a528-114340a8c267",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## Review Questions\n\n1. What are the three main modalities in the VLA perception pipeline?\n2. How does depth information enhance RGB-only perception?\n3. Why use MobileNet instead of heavier segmentation models?\n4. How do you validate perception accuracy before using live simulation data?",
    "chapter": 5,
    "section": "Review Questions",
    "page": 24,
    "token_count": 57
  },
  {
    "chunk_id": "76c90698-38ed-435e-84a2-16c89bb04da8",
    "doc_id": "18644710-be88-4bdc-8c17-a0ac54a6f724",
    "chunk_text": "## Next Chapter\n\n[Chapter 3: Language Understanding and Command Parsing →](/docs/chapter5/module4-vla/ch03-language/)",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 25,
    "token_count": 29
  },
  {
    "chunk_id": "861e40c3-5685-45e0-a226-017a1b553e5b",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "# Chapter 3: Language Understanding and Command Parsing\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 29
  },
  {
    "chunk_id": "c49943e6-ed8a-408b-8359-525915e7b4b8",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Explain natural language understanding for robotics -->\n\nThis chapter covers natural language interpretation for VLA systems, including command parsing, safety filtering, and structured command representation.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 37
  },
  {
    "chunk_id": "caf904db-fee3-4545-8ce4-7522a00ae12c",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## Learning Objectives\n\n- [ ] Parse natural language commands for robot control\n- [ ] Implement safety filters (blocklist/allowlist)\n- [ ] Validate and sanitize language inputs\n- [ ] Convert natural language to structured robot commands\n- [ ] Understand failure modes in language understanding\n\n---",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 60
  },
  {
    "chunk_id": "f23faefe-a2a9-4a1f-95f8-1fa073b2989b",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## 3.1 Natural Language Interpretation for Robotics\n\n<!-- TODO: Content from T029 - NL interpretation fundamentals -->\n\n---",
    "chapter": 5,
    "section": "3.1 Natural Language Interpretation for Robotics",
    "page": 4,
    "token_count": 25
  },
  {
    "chunk_id": "5a28f020-06ef-47b6-9cb7-65d00af1b54a",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## 3.2 Command Parsing\n\n<!-- TODO: Content from T030 - Command parsing techniques -->\n\n**Code Example**: See [command_parser.py](code-examples/command_parser.py)\n\n---",
    "chapter": 5,
    "section": "3.2 Command Parsing",
    "page": 5,
    "token_count": 38
  },
  {
    "chunk_id": "f6e85012-9b61-40fe-b949-008a51af2404",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## 3.3 Safety Filtering and Validation\n\n<!-- TODO: Content from T031 - Safety filtering -->",
    "chapter": 5,
    "section": "3.3 Safety Filtering and Validation",
    "page": 6,
    "token_count": 21
  },
  {
    "chunk_id": "9fb87c3c-0a03-4166-bfab-1d1c675aaae4",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "### Blocklist Approach\n\n```python\n# TODO: Example of unsafe command blocklist\nUNSAFE_KEYWORDS = ['force', 'torque', 'joint', 'velocity', ...]\n```",
    "chapter": 5,
    "section": "Blocklist Approach",
    "page": 7,
    "token_count": 39
  },
  {
    "chunk_id": "dca3c5ae-e5f4-4469-8f81-4a2c6adfa9eb",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "### Allowlist Approach\n\n```python\n# TODO: Example of safe command allowlist\nSAFE_ACTIONS = ['navigate', 'approach', 'align', ...]\n```\n\n**Code Example**: See [safety_filter.py](code-examples/safety_filter.py)\n\n---",
    "chapter": 5,
    "section": "Allowlist Approach",
    "page": 8,
    "token_count": 55
  },
  {
    "chunk_id": "b07ad230-de7e-4627-8c35-0f68dfdbdddd",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## 3.4 Structured Command Representation\n\n<!-- TODO: Content from T032 - JSON schema preview -->",
    "chapter": 5,
    "section": "3.4 Structured Command Representation",
    "page": 9,
    "token_count": 22
  },
  {
    "chunk_id": "2d854198-99e5-4486-8dac-9b28aecdcf99",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "### From Natural Language to JSON\n\n```json\n{\n  \"command\": \"move to the red cube\",\n  \"action_type\": \"navigate_to_object\",\n  \"parameters\": {\n    \"target_object\": \"red cube\",\n    \"approach_distance\": 0.5\n  },\n  \"safety_constraints\": {\n    \"max_velocity\": 0.5,\n    \"collision_avoidance\": true\n  }\n}\n```\n\n---",
    "chapter": 5,
    "section": "From Natural Language to JSON",
    "page": 10,
    "token_count": 91
  },
  {
    "chunk_id": "67ea7baf-e20b-4120-8421-f1dc7a65235e",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## Key Takeaways\n\n- Natural language commands must be parsed and validated for safety\n- Multiple safety layers: blocklist, allowlist, schema validation\n- Structured representation (JSON) enables downstream processing\n- Always validate before passing commands to robot",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 11,
    "token_count": 50
  },
  {
    "chunk_id": "6a979358-96b9-4457-ad5e-24b8ab80feb8",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## Review Questions\n\n1. Why is safety filtering critical for language-controlled robots?\n2. What's the difference between blocklist and allowlist approaches?\n3. How does structured representation improve safety?",
    "chapter": 5,
    "section": "Review Questions",
    "page": 12,
    "token_count": 39
  },
  {
    "chunk_id": "e4510792-1f08-445c-8413-ba754dde3da3",
    "doc_id": "28fd9ff2-3c18-4be0-9923-4d4da20b45d8",
    "chunk_text": "## Next Chapter\n\n[Chapter 4: VLA System Architecture →](/docs/chapter5/module4-vla/ch04-architecture/)",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 13,
    "token_count": 29
  },
  {
    "chunk_id": "58a47855-f31a-4576-8780-616d859cd63d",
    "doc_id": "20edb8ea-4560-4ecc-9109-3099cd7bda06",
    "chunk_text": "# VLA Interfaces Code Examples\n\n**Status**: Coming Soon - Placeholder\n\nThis directory will contain ROS 2 service and message definitions for the VLA system.",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 32
  },
  {
    "chunk_id": "0f143c17-1e28-4a8a-a748-d20be5f7ed08",
    "doc_id": "20edb8ea-4560-4ecc-9109-3099cd7bda06",
    "chunk_text": "## Planned Examples\n\n- Service definitions for VLA planning requests\n- Message definitions for perception data\n- Action definitions for robot execution\n- Interface contracts between components",
    "chapter": 5,
    "section": "Planned Examples",
    "page": 2,
    "token_count": 32
  },
  {
    "chunk_id": "a24d7e36-ed0a-4549-80c5-c6795fe73e14",
    "doc_id": "20edb8ea-4560-4ecc-9109-3099cd7bda06",
    "chunk_text": "## TODO\n\n- [ ] Add service definitions (.srv files)\n- [ ] Add message definitions (.msg files)\n- [ ] Add Python interface implementations",
    "chapter": 5,
    "section": "TODO",
    "page": 3,
    "token_count": 30
  },
  {
    "chunk_id": "0722639e-762e-4135-a587-098ff8005fb5",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "# Chapter 4: VLA System Architecture and Design\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 30
  },
  {
    "chunk_id": "1c58f94c-5f57-40f7-8730-5cab252b6edb",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Content from T034-T039 - Complete VLA pipeline architecture -->\n\nThis chapter presents the complete VLA system architecture, component interfaces, and ROS 2 integration patterns.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 40
  },
  {
    "chunk_id": "8eb193bc-c718-4cc2-89ea-9b993df60a57",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## Learning Objectives\n\n- [ ] Design modular VLA system architecture\n- [ ] Define component interfaces (perception, planning, execution)\n- [ ] Implement ROS 2 communication between VLA components\n- [ ] Understand safety constraints at each VLA stage\n\n---",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 55
  },
  {
    "chunk_id": "ca176761-6e89-4055-9d62-5e346a05cadd",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## 4.1 Complete VLA Pipeline Architecture\n\n<!-- TODO: Content from T035 - Pipeline architecture -->",
    "chapter": 5,
    "section": "4.1 Complete VLA Pipeline Architecture",
    "page": 4,
    "token_count": 22
  },
  {
    "chunk_id": "4b591a31-43d2-4462-b579-a00c5b480f8d",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "### Architecture Diagram\n\n```\n[Perception] --observations--> [Planning] --plans--> [Execution]\n     |                              |                    |\n     v                              v                    v\n[RGB/Depth/Seg]              [LLM Planner]        [Behavior Trees]\n     |                              |                    |\n     v                              v                    v\n[Object Info]                [JSON Plans]          [ROS 2 Actions]\n```\n\n---",
    "chapter": 5,
    "section": "Architecture Diagram",
    "page": 5,
    "token_count": 85
  },
  {
    "chunk_id": "88f6037d-741e-4758-8051-71ac99bda41d",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## 4.2 Component Interfaces\n\n<!-- TODO: Content from T036 - Component design -->",
    "chapter": 5,
    "section": "4.2 Component Interfaces",
    "page": 6,
    "token_count": 19
  },
  {
    "chunk_id": "90d3a246-0bbe-4f2e-aa54-671ec68ba1b8",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "### Perception Interface\n\n```python\nclass PerceptionInterface:\n    \"\"\"\n    TODO: Define perception output format\n    - Object detections\n    - Scene descriptions\n    - Spatial relationships\n    \"\"\"\n```",
    "chapter": 5,
    "section": "Perception Interface",
    "page": 7,
    "token_count": 39
  },
  {
    "chunk_id": "f7b1b588-8b70-4670-8538-63df97c2da15",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "### Planning Interface\n\n```python\nclass PlanningInterface:\n    \"\"\"\n    TODO: Define planning input/output\n    - Observations → Plans\n    - Safety validation\n    \"\"\"\n```",
    "chapter": 5,
    "section": "Planning Interface",
    "page": 8,
    "token_count": 36
  },
  {
    "chunk_id": "aa5f636c-f983-4bea-afdc-36e0f2387222",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "### Execution Interface\n\n```python\nclass ExecutionInterface:\n    \"\"\"\n    TODO: Define execution input\n    - High-level plans → Behaviors\n    - Feedback and monitoring\n    \"\"\"\n```\n\n**Code Example**: See [vla_interfaces.py](code-examples/vla_interfaces.py)\n\n---",
    "chapter": 5,
    "section": "Execution Interface",
    "page": 9,
    "token_count": 58
  },
  {
    "chunk_id": "0725bba3-2214-4eb3-b66b-1ff79f334d50",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## 4.3 ROS 2 Communication Patterns\n\n<!-- TODO: Content from T037 - ROS 2 integration -->",
    "chapter": 5,
    "section": "4.3 ROS 2 Communication Patterns",
    "page": 10,
    "token_count": 24
  },
  {
    "chunk_id": "ccf49c74-ac1b-4dfe-a9dd-838c4e01bb10",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "### Topics, Services, and Actions\n\n<!-- TODO: When to use each communication pattern -->\n\n**Code Examples**: See service and message definitions in [code-examples/](code-examples/index.md)\n\n---",
    "chapter": 5,
    "section": "Topics, Services, and Actions",
    "page": 11,
    "token_count": 41
  },
  {
    "chunk_id": "122dccee-9d3f-4221-a687-5b19b04b11d6",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## 4.4 Safety Constraints at Each Stage\n\n<!-- TODO: Content from T038 - Safety per component -->\n\n---",
    "chapter": 5,
    "section": "4.4 Safety Constraints at Each Stage",
    "page": 12,
    "token_count": 24
  },
  {
    "chunk_id": "b7e15ad3-ff81-40d8-aede-03d44e858540",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## Key Takeaways\n\n- VLA system is modular: Perception → Planning → Execution\n- Clear interfaces enable independent development and testing\n- ROS 2 provides standardized communication patterns\n- Safety validation occurs at multiple stages",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 13,
    "token_count": 43
  },
  {
    "chunk_id": "c76c494e-c382-4dd6-84bb-d8af8a1cf3a5",
    "doc_id": "97ce0fd9-ff3a-4e7b-9247-6d93de8623d8",
    "chunk_text": "## Next Chapter\n\n[Chapter 5: High-Level Task Planning with LLMs →](/docs/chapter5/module4-vla/ch05-planning/)",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 14,
    "token_count": 33
  },
  {
    "chunk_id": "0ecc3b60-39e6-4b8d-b77c-53c93de74463",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "# Chapter 5: High-Level Task Planning with LLMs\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 32
  },
  {
    "chunk_id": "10c59410-61b6-4a2e-8479-7e72f1d9e703",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Content from T040-T046 - LLM-based planning -->\n\nThis chapter covers using LLMs for safe high-level robot planning, JSON schema validation, prompt engineering, and real API + mock fallback implementation.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 48
  },
  {
    "chunk_id": "8f8fcd8a-d639-41fa-a86f-e03c41b96cb3",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## Learning Objectives\n\n- [ ] Use LLMs for high-level robot task planning\n- [ ] Design JSON schemas for safe action plans\n- [ ] Engineer prompts for consistent structured outputs\n- [ ] Implement real API integration (OpenAI/Anthropic/Google)\n- [ ] Create mock planner fallback for offline learning\n\n---",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 68
  },
  {
    "chunk_id": "3629d6b4-bfa6-49e9-9fc6-dd6bb364e557",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## 5.1 LLM-Based Planning for Robotics\n\n<!-- TODO: Content from T041 - LLM planning fundamentals -->\n\n**Key Principle**: LLMs generate HIGH-LEVEL plans only (never low-level control)\n\n---",
    "chapter": 5,
    "section": "5.1 LLM-Based Planning for Robotics",
    "page": 4,
    "token_count": 45
  },
  {
    "chunk_id": "1706c487-acee-4277-8d92-62e97a12a286",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## 5.2 JSON Schema for Safe Action Plans\n\n<!-- TODO: Content from T042 - JSON schema definition -->",
    "chapter": 5,
    "section": "5.2 JSON Schema for Safe Action Plans",
    "page": 5,
    "token_count": 24
  },
  {
    "chunk_id": "38ccdeec-6697-46a5-9f00-df9ca3df690d",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "### Example Plan Schema\n\n```json\n{\n  \"type\": \"object\",\n  \"required\": [\"task_type\", \"parameters\", \"safety_constraints\", \"preconditions\"],\n  \"properties\": {\n    \"task_type\": {\"type\": \"string\", \"enum\": [\"navigate\", \"approach\", \"align\"]},\n    \"parameters\": {\"type\": \"object\"},\n    \"safety_constraints\": {\"type\": \"object\"},\n    \"preconditions\": {\"type\": \"array\"}\n  }\n}\n```\n\n**Full Schema**: See [plan_schema.json](code-examples/plan_schema.json)\n\n---",
    "chapter": 5,
    "section": "Example Plan Schema",
    "page": 6,
    "token_count": 123
  },
  {
    "chunk_id": "84aaa447-ec7a-4c26-87b3-9de2f273883d",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## 5.3 Prompt Engineering for Consistency\n\n<!-- TODO: Content from T043 - Prompt engineering -->",
    "chapter": 5,
    "section": "5.3 Prompt Engineering for Consistency",
    "page": 7,
    "token_count": 22
  },
  {
    "chunk_id": "07544653-afad-4e2c-a51e-3d535619e713",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "### Template Example\n\n```\nYou are a robot task planner. Generate a safe, high-level plan in JSON format.\n\nInput: {observation}\nCommand: {user_command}\n\nOutput JSON with fields: task_type, parameters, safety_constraints, preconditions.\nValid task_types: [\"navigate\", \"approach\", \"align\"]\n```\n\n**Code Example**: See [llm_planner.py](code-examples/llm_planner.py)\n\n---",
    "chapter": 5,
    "section": "Template Example",
    "page": 8,
    "token_count": 90
  },
  {
    "chunk_id": "f40853fe-6b72-4a57-9f37-790122dfce0f",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## 5.4 Real API Integration\n\n<!-- TODO: Content from T087 - Real LLM API -->",
    "chapter": 5,
    "section": "5.4 Real API Integration",
    "page": 9,
    "token_count": 22
  },
  {
    "chunk_id": "5f096b4c-2c94-4069-b299-a3e47f174818",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "### API Setup\n\n```python\n# TODO: OpenAI/Anthropic/Google setup\n```\n\n---",
    "chapter": 5,
    "section": "API Setup",
    "page": 10,
    "token_count": 22
  },
  {
    "chunk_id": "bc283a56-1cb5-4557-94c4-d092751b57b0",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## 5.5 Mock Planner Fallback\n\n<!-- TODO: Content from T088 - Mock planner -->\n\n```python\n# TODO: Template-based mock responses\n```\n\n**Code Example**: See [mock_planner.py](code-examples/mock_planner.py)\n\n---",
    "chapter": 5,
    "section": "5.5 Mock Planner Fallback",
    "page": 11,
    "token_count": 54
  },
  {
    "chunk_id": "ffc8038e-4c44-4d0b-aa8c-574afbed22b2",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## 5.6 Safety Validation\n\n<!-- TODO: Content from T086 - Safety validator -->\n\n**Code Example**: See [safety_validator.py](code-examples/safety_validator.py)\n\n---",
    "chapter": 5,
    "section": "5.6 Safety Validation",
    "page": 12,
    "token_count": 39
  },
  {
    "chunk_id": "7c76bcef-aa33-4dc5-92a2-cfc42e7c530c",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## Key Takeaways\n\n- LLMs excel at high-level planning, not low-level control\n- JSON schema ensures parseable, validated plans\n- Prompt engineering critical for consistent outputs\n- Mock fallback ensures accessibility without API costs",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 13,
    "token_count": 46
  },
  {
    "chunk_id": "f0d61642-3326-4f70-991b-1f84eb699899",
    "doc_id": "9444db26-a722-430c-9c40-789e967edb3b",
    "chunk_text": "## Next Chapter\n\n[Chapter 6: Action Execution and Behavioral Control →](/docs/chapter5/module4-vla/ch06-execution/)",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 14,
    "token_count": 30
  },
  {
    "chunk_id": "fb47fd46-6cf0-40d5-a6fc-b267902503f7",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "# Chapter 6: Action Execution and Behavioral Control\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 29
  },
  {
    "chunk_id": "3364211b-82a4-4e8b-a944-99a94f9c1a19",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Content from T047-T052 - Behavior trees and execution -->\n\nThis chapter covers executing VLA plans using behavior trees and ROS 2 actions, including py_trees library integration and failure handling.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 44
  },
  {
    "chunk_id": "2a0ababc-b88e-4c80-b99d-a9b020372476",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## Learning Objectives\n\n- [ ] Understand behavior tree concepts for robot control\n- [ ] Use py_trees library for hierarchical task decomposition\n- [ ] Implement individual behaviors (navigate, approach, align)\n- [ ] Create composite behaviors for multi-step tasks\n- [ ] Handle failures and implement recovery strategies\n\n---",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 63
  },
  {
    "chunk_id": "19a8e06d-25fe-4c31-9488-7ab8d52db3a1",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## 6.1 Behavior Trees for Robot Control\n\n<!-- TODO: Content from T048 - Behavior trees intro -->",
    "chapter": 5,
    "section": "6.1 Behavior Trees for Robot Control",
    "page": 4,
    "token_count": 23
  },
  {
    "chunk_id": "eb52375b-d186-498d-b31d-590518561a5f",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "### Why Behavior Trees?\n\n<!-- TODO: Advantages over state machines or sequential execution -->",
    "chapter": 5,
    "section": "Why Behavior Trees?",
    "page": 5,
    "token_count": 17
  },
  {
    "chunk_id": "fefffe76-98d5-4854-814d-f686501e13f5",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "### BT Structure\n\n```\nRoot\n├── Sequence: Multi-step task\n│   ├── Navigate to table\n│   ├── Approach object\n│   └── Align gripper\n```\n\n---",
    "chapter": 5,
    "section": "BT Structure",
    "page": 6,
    "token_count": 42
  },
  {
    "chunk_id": "fdb11f1a-505b-4f3d-b527-dfc760559b29",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## 6.2 py_trees Library\n\n<!-- TODO: Content from T049 - py_trees patterns -->\n\n```python\nimport py_trees\n# TODO: Basic py_trees examples\n```\n\n---",
    "chapter": 5,
    "section": "6.2 py_trees Library",
    "page": 7,
    "token_count": 39
  },
  {
    "chunk_id": "584c90bf-6444-4826-ba3c-0d9a8ee6acb8",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## 6.3 Individual Behaviors\n\n<!-- TODO: Content from T094-T096 - Basic behaviors -->\n\n**Code Examples**:\n- [navigate.py](code-examples/navigate.py)\n- [approach_object.py](code-examples/approach_object.py)\n- [align_gripper.py](code-examples/align_gripper.py)\n\n---",
    "chapter": 5,
    "section": "6.3 Individual Behaviors",
    "page": 8,
    "token_count": 71
  },
  {
    "chunk_id": "b87c4bd6-eeb4-460f-ab51-7987446840bc",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## 6.4 Composite Behaviors\n\n<!-- TODO: Content from T097 - Multi-step tasks -->\n\n**Code Example**: See [composite_behaviors.py](code-examples/composite_behaviors.py)\n\n---",
    "chapter": 5,
    "section": "6.4 Composite Behaviors",
    "page": 9,
    "token_count": 42
  },
  {
    "chunk_id": "6aa6ae6e-2e7e-49e3-83d7-ddab516ae9f4",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## 6.5 ROS 2 Action Integration\n\n<!-- TODO: Content from T098 - py_trees + ROS 2 actions -->\n\n**Code Example**: See [vla_executor.py](code-examples/vla_executor.py)\n\n---",
    "chapter": 5,
    "section": "6.5 ROS 2 Action Integration",
    "page": 10,
    "token_count": 46
  },
  {
    "chunk_id": "70f6a607-1d3d-4f46-b1cd-d9aebc554361",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## 6.6 Failure Handling\n\n<!-- TODO: Content from T050, T099 - Recovery strategies -->\n\n---",
    "chapter": 5,
    "section": "6.6 Failure Handling",
    "page": 11,
    "token_count": 23
  },
  {
    "chunk_id": "c85d1da2-a82d-489a-8430-b53481bfddba",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## Key Takeaways\n\n- Behavior trees provide hierarchical task decomposition\n- py_trees is industry-standard for ROS 2 robot control\n- Individual behaviors combine into complex multi-step tasks\n- Failure handling and recovery are essential for robust execution",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 12,
    "token_count": 46
  },
  {
    "chunk_id": "b1f98b50-fd52-49fc-b2a8-9eabf5c2ed7e",
    "doc_id": "81413e03-8798-4e08-864a-5dff216fdfbc",
    "chunk_text": "## Next Chapter\n\n[Chapter 7: Simulation Scenarios and Integration →](/docs/chapter5/module4-vla/ch07-scenarios/)",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 13,
    "token_count": 30
  },
  {
    "chunk_id": "2587ab56-c44c-4b16-9910-65d6bd470530",
    "doc_id": "ef732292-2543-4f31-969f-45818bc59ec3",
    "chunk_text": "# VLA Test Scenarios\n\n**Status**: Coming Soon - Placeholder\n\nThis directory will contain Gazebo world files and launch configurations for testing VLA systems in various scenarios.",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 36
  },
  {
    "chunk_id": "661f596e-fc4c-4e2c-9954-cb9249d1981f",
    "doc_id": "ef732292-2543-4f31-969f-45818bc59ec3",
    "chunk_text": "## Planned Scenarios",
    "chapter": 5,
    "section": "Planned Scenarios",
    "page": 2,
    "token_count": 4
  },
  {
    "chunk_id": "7def68c0-c3bc-490d-a0ea-ad0e3ac224e7",
    "doc_id": "ef732292-2543-4f31-969f-45818bc59ec3",
    "chunk_text": "### Kitchen Environment\n- Object manipulation tasks\n- Navigation with obstacles\n- Multi-object interactions",
    "chapter": 5,
    "section": "Kitchen Environment",
    "page": 3,
    "token_count": 18
  },
  {
    "chunk_id": "ced5f7ca-78a7-45eb-b619-6742b0ba37bb",
    "doc_id": "ef732292-2543-4f31-969f-45818bc59ec3",
    "chunk_text": "### Warehouse Navigation\n- Long-range navigation\n- Dynamic obstacle avoidance\n- Delivery tasks",
    "chapter": 5,
    "section": "Warehouse Navigation",
    "page": 4,
    "token_count": 17
  },
  {
    "chunk_id": "675d54f9-1a71-45f0-adb4-4d62d639517c",
    "doc_id": "ef732292-2543-4f31-969f-45818bc59ec3",
    "chunk_text": "### Manipulation Tasks\n- Pick and place\n- Object sorting\n- Precision alignment",
    "chapter": 5,
    "section": "Manipulation Tasks",
    "page": 5,
    "token_count": 17
  },
  {
    "chunk_id": "bada6b4e-268a-4a95-bff6-00d130832939",
    "doc_id": "ef732292-2543-4f31-969f-45818bc59ec3",
    "chunk_text": "## TODO\n\n- [ ] Create kitchen.world\n- [ ] Create warehouse.world\n- [ ] Create manipulation.world\n- [ ] Add launch files for each scenario",
    "chapter": 5,
    "section": "TODO",
    "page": 6,
    "token_count": 33
  },
  {
    "chunk_id": "4fa29080-9e9a-475d-94be-4670795f5085",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "# Chapter 7: Simulation Scenarios and Integration\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 29
  },
  {
    "chunk_id": "7d7f082d-946e-4281-abcd-45e36f35f54f",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Content from T053-T058 - Complete VLA scenarios -->\n\nThis chapter demonstrates complete VLA pipelines in Gazebo and Isaac Sim, including navigation, manipulation, and integrated multi-step tasks.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 45
  },
  {
    "chunk_id": "7e475039-b1c7-48f2-84bb-f8b01ef60f9a",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## Learning Objectives\n\n- [ ] Set up simulation environments for VLA testing\n- [ ] Execute navigation tasks (waypoints, object approach)\n- [ ] Perform manipulation tasks (gripper alignment, positioning)\n- [ ] Run integrated VLA scenarios end-to-end\n- [ ] Measure and optimize VLA system performance\n\n---",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 67
  },
  {
    "chunk_id": "c5e71e33-8dc2-4b50-9930-b90481a88e47",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## 7.1 Simulation Environment Setup\n\n<!-- TODO: Content from T115-T119 - Workspace setup -->",
    "chapter": 5,
    "section": "7.1 Simulation Environment Setup",
    "page": 4,
    "token_count": 22
  },
  {
    "chunk_id": "4884fef6-7f2c-4787-993a-8c3ca30bcdd5",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Gazebo Classic Setup\n\n```bash\n# TODO: Launch commands for Gazebo\n```",
    "chapter": 5,
    "section": "Gazebo Classic Setup",
    "page": 5,
    "token_count": 21
  },
  {
    "chunk_id": "3b64f348-281e-4d91-afb3-99d820721f96",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Isaac Sim Setup (Optional)\n\n```bash\n# TODO: Launch commands for Isaac Sim\n```\n\n**World Files**: See [scenarios/](code-examples/scenarios/index.md)\n\n---",
    "chapter": 5,
    "section": "Isaac Sim Setup (Optional)",
    "page": 6,
    "token_count": 40
  },
  {
    "chunk_id": "e0201e7b-4a9b-449b-bbc4-6eadd10f1f8d",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## 7.2 Navigation Tasks\n\n<!-- TODO: Content from T120-T123, T054 - Navigation scenarios -->",
    "chapter": 5,
    "section": "7.2 Navigation Tasks",
    "page": 7,
    "token_count": 24
  },
  {
    "chunk_id": "1c47d0b9-6dd3-4e5b-a6d4-ae481ea937af",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Waypoint Navigation\n\n<!-- TODO: Move to specified location -->",
    "chapter": 5,
    "section": "Waypoint Navigation",
    "page": 8,
    "token_count": 13
  },
  {
    "chunk_id": "3d0316a1-4010-4fd6-a522-6747e3f81308",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Vision-Guided Navigation\n\n<!-- TODO: Approach detected object -->",
    "chapter": 5,
    "section": "Vision-Guided Navigation",
    "page": 9,
    "token_count": 14
  },
  {
    "chunk_id": "073bdd64-3e3f-4718-b93a-dddbbcde9299",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Obstacle Avoidance\n\n<!-- TODO: Dynamic obstacle handling -->\n\n---",
    "chapter": 5,
    "section": "Obstacle Avoidance",
    "page": 10,
    "token_count": 14
  },
  {
    "chunk_id": "ca270598-527c-4947-bfd9-18637596faa7",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## 7.3 Manipulation Tasks\n\n<!-- TODO: Content from T124-T126, T055 - Manipulation scenarios -->",
    "chapter": 5,
    "section": "7.3 Manipulation Tasks",
    "page": 11,
    "token_count": 26
  },
  {
    "chunk_id": "939a3bd9-310d-4709-a9e3-f60ef2cffb68",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Gripper Alignment\n\n<!-- TODO: Visual servoing to target -->",
    "chapter": 5,
    "section": "Gripper Alignment",
    "page": 12,
    "token_count": 14
  },
  {
    "chunk_id": "359c0d07-b80d-4e47-a530-0243592355b5",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### End-Effector Positioning\n\n<!-- TODO: Specified pose reaching -->\n\n---",
    "chapter": 5,
    "section": "End-Effector Positioning",
    "page": 13,
    "token_count": 17
  },
  {
    "chunk_id": "6606c5e6-6f60-4aab-8d2e-1ae3ce02b88d",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## 7.4 Integrated VLA Scenarios\n\n<!-- TODO: Content from T127-T130, T056 - Integration -->",
    "chapter": 5,
    "section": "7.4 Integrated VLA Scenarios",
    "page": 14,
    "token_count": 26
  },
  {
    "chunk_id": "d19e6d2c-d8ac-4464-ac55-656312d51fdb",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Scenario 1: Find and Approach Red Cube\n\n```bash\n# TODO: Launch command\nros2 launch vla_system find_cube.launch.py\n```",
    "chapter": 5,
    "section": "Scenario 1: Find and Approach Red Cube",
    "page": 15,
    "token_count": 32
  },
  {
    "chunk_id": "4797ab60-450f-44eb-a7c6-50e65ff61e9f",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Scenario 2: Navigate to Table and Align\n\n<!-- TODO: Multi-step scenario -->",
    "chapter": 5,
    "section": "Scenario 2: Navigate to Table and Align",
    "page": 16,
    "token_count": 18
  },
  {
    "chunk_id": "dc1f6522-804f-43f6-ad5b-f8a0afad289b",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Mini-Project: 3 Sequential Tasks\n\n<!-- TODO: Student implementation project -->\n\n**Success Criteria**: ≥70% task completion rate\n\n---",
    "chapter": 5,
    "section": "Mini-Project: 3 Sequential Tasks",
    "page": 17,
    "token_count": 29
  },
  {
    "chunk_id": "aa085120-d8dc-46d7-83bc-522d18f64ba5",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## 7.5 Performance Optimization\n\n<!-- TODO: Content from T057 - Optimization techniques -->",
    "chapter": 5,
    "section": "7.5 Performance Optimization",
    "page": 18,
    "token_count": 19
  },
  {
    "chunk_id": "8eeb74ad-119f-4014-a7b7-3690785055b3",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Perception Optimization\n\n<!-- TODO: Reduce latency to <200ms -->",
    "chapter": 5,
    "section": "Perception Optimization",
    "page": 19,
    "token_count": 14
  },
  {
    "chunk_id": "5e748e6c-7f4d-4061-8b5b-74d405fa14fe",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Planning Optimization\n\n<!-- TODO: LLM caching, reduce to <5s -->",
    "chapter": 5,
    "section": "Planning Optimization",
    "page": 20,
    "token_count": 17
  },
  {
    "chunk_id": "be7d3c12-19d8-4017-b5e5-e62f13d0efb7",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "### Execution Optimization\n\n<!-- TODO: Real-time behavior tree execution -->\n\n---",
    "chapter": 5,
    "section": "Execution Optimization",
    "page": 21,
    "token_count": 14
  },
  {
    "chunk_id": "29b0f59a-313e-465a-8d1c-d539d25e587a",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## Key Takeaways\n\n- Complete VLA scenarios demonstrate end-to-end functionality\n- Navigation + manipulation shows full VLA capabilities\n- Performance optimization essential for responsive robot behavior\n- Simulation provides safe environment for testing VLA systems",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 22,
    "token_count": 45
  },
  {
    "chunk_id": "8d937131-7fc0-49ee-b703-cad4d9256ae4",
    "doc_id": "56b2e170-e51d-4baa-bd52-7c79dce3365a",
    "chunk_text": "## Next Chapter\n\n[Chapter 8: Safety, Ethics, and Limitations →](/docs/chapter5/module4-vla/ch08-safety-ethics/)",
    "chapter": 5,
    "section": "Next Chapter",
    "page": 23,
    "token_count": 35
  },
  {
    "chunk_id": "1240dd63-91e5-4586-9afe-c8d48db1c9ee",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "# Chapter 8: Safety, Ethics, and Limitations\n\n**Status**: Draft - Template\n**Last Updated**: 2025-12-09",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 31
  },
  {
    "chunk_id": "b562ffe9-8e07-48c9-9ade-c2e3c4752cc9",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## Chapter Summary\n\n<!-- TODO: Content from T059-T064 - Safety, ethics, limitations -->\n\nThis chapter addresses critical safety constraints, ethical considerations, and known limitations of VLA systems for robot control.",
    "chapter": 5,
    "section": "Chapter Summary",
    "page": 2,
    "token_count": 41
  },
  {
    "chunk_id": "31a4ad11-25d9-456b-9a68-89040cadb3a9",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## Learning Objectives\n\n- [ ] Identify safety risks in LLM-driven robot control\n- [ ] Implement safety constraints and validation layers\n- [ ] Understand ethical boundaries for autonomous robots\n- [ ] Recognize common failure modes and mitigation strategies\n- [ ] Explain limitations of current VLA technology\n\n---",
    "chapter": 5,
    "section": "Learning Objectives",
    "page": 3,
    "token_count": 62
  },
  {
    "chunk_id": "28ea10a3-bd29-43dc-8dc3-c5f8b742b14e",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## 8.1 Safety Constraints for LLM-Driven Robots\n\n<!-- TODO: Content from T060 - Safety constraints -->",
    "chapter": 5,
    "section": "8.1 Safety Constraints for LLM-Driven Robots",
    "page": 4,
    "token_count": 25
  },
  {
    "chunk_id": "c6e1e060-baa3-416d-8945-5c5694b36dfd",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Critical Safety Principles\n\n1. **NO LOW-LEVEL CONTROL**: LLMs generate high-level plans only\n2. **MULTIPLE VALIDATION LAYERS**: Schema + constraints + allowlists\n3. **SIMULATION FIRST**: Never deploy untested VLA systems to physical robots\n4. **HUMAN OVERSIGHT**: Critical decisions require human approval\n\n---",
    "chapter": 5,
    "section": "Critical Safety Principles",
    "page": 5,
    "token_count": 75
  },
  {
    "chunk_id": "bfc97b84-afec-460d-a232-a33fef34400f",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## 8.2 Safety Implementation",
    "chapter": 5,
    "section": "8.2 Safety Implementation",
    "page": 6,
    "token_count": 7
  },
  {
    "chunk_id": "8cbe5f4f-f9e1-4cea-bb67-c16469f9569b",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Multi-Layer Validation\n\n```python\n# Layer 1: Schema validation\nvalidate_json_schema(plan)\n\n# Layer 2: Safety constraints\ncheck_safety_constraints(plan)\n\n# Layer 3: Allowlist verification\nverify_allowed_actions(plan)\n\n# Layer 4: Human approval (for critical tasks)\nif plan.requires_approval:\n    wait_for_human_approval()\n```\n\n**Code Examples**:\n- [constraint_validator.py](code-examples/constraint_validator.py)\n- [emergency_stop.py](code-examples/emergency_stop.py)\n- [audit_logger.py](code-examples/audit_logger.py)\n\n---",
    "chapter": 5,
    "section": "Multi-Layer Validation",
    "page": 7,
    "token_count": 123
  },
  {
    "chunk_id": "92dd2517-f606-42bb-81d1-a96b182e4218",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## 8.3 Ethical Considerations\n\n<!-- TODO: Content from T061 - Ethical boundaries -->",
    "chapter": 5,
    "section": "8.3 Ethical Considerations",
    "page": 8,
    "token_count": 22
  },
  {
    "chunk_id": "2425f448-9611-46bf-99e2-8961a5690b47",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Responsible AI in Robotics\n\n- **Transparency**: Users must know when AI controls robots\n- **Accountability**: Clear responsibility chains for AI decisions\n- **Fairness**: VLA systems must not discriminate\n- **Privacy**: Visual data collection must respect privacy",
    "chapter": 5,
    "section": "Responsible AI in Robotics",
    "page": 9,
    "token_count": 53
  },
  {
    "chunk_id": "7fde57f2-0222-4b7e-9bec-3cf477561f87",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Prohibited Uses\n\n<!-- TODO: Dangerous or unethical applications to avoid -->\n\n---",
    "chapter": 5,
    "section": "Prohibited Uses",
    "page": 10,
    "token_count": 16
  },
  {
    "chunk_id": "65e7131f-aab9-4ab0-9bfa-3bd7f08e2e41",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## 8.4 Failure Modes and Mitigation\n\n<!-- TODO: Content from T062 - Failure modes -->",
    "chapter": 5,
    "section": "8.4 Failure Modes and Mitigation",
    "page": 11,
    "token_count": 22
  },
  {
    "chunk_id": "ec76c7f7-9833-4604-9211-150f39819e05",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Common Failure Modes\n\n1. **Perception Failures**: Misidentified objects, poor lighting\n2. **Language Misunderstanding**: Ambiguous commands, out-of-distribution inputs\n3. **Planning Failures**: Invalid plans, unreachable goals\n4. **Execution Failures**: Environmental changes, hardware issues\n5. **LLM Hallucinations**: Non-existent capabilities, incorrect reasoning",
    "chapter": 5,
    "section": "Common Failure Modes",
    "page": 12,
    "token_count": 79
  },
  {
    "chunk_id": "442b917f-f3d2-4473-8a0f-76a7c69eae90",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Mitigation Strategies\n\n<!-- TODO: How to handle each failure mode -->\n\n**Code Example**: See [edge_case_tests.py](code-examples/edge_case_tests.py)\n\n---",
    "chapter": 5,
    "section": "Mitigation Strategies",
    "page": 13,
    "token_count": 36
  },
  {
    "chunk_id": "3938e29f-1439-4f1f-9219-da000b86ef94",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## 8.5 Limitations of Current VLA Systems\n\n<!-- TODO: Content from T063 - Current limitations -->",
    "chapter": 5,
    "section": "8.5 Limitations of Current VLA Systems",
    "page": 14,
    "token_count": 24
  },
  {
    "chunk_id": "123521b1-b12a-4857-b61f-153b1f84ef91",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Technical Limitations\n\n- **Latency**: LLM inference time limits real-time control\n- **Accuracy**: Perception and planning errors inevitable\n- **Generalization**: Limited to training distribution\n- **Cost**: API costs for continuous robot operation",
    "chapter": 5,
    "section": "Technical Limitations",
    "page": 15,
    "token_count": 49
  },
  {
    "chunk_id": "256d2514-5ad6-499e-b7e7-7f0d604ecad6",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "### Research Challenges\n\n<!-- TODO: Open problems in VLA research -->\n\n---",
    "chapter": 5,
    "section": "Research Challenges",
    "page": 16,
    "token_count": 15
  },
  {
    "chunk_id": "de27edc2-bd48-4567-a64f-8654034da313",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## 8.6 Future Directions\n\n<!-- TODO: Where VLA research is heading -->\n\n- Faster, more efficient models\n- Better generalization and transfer learning\n- Improved safety guarantees\n- Integration with reinforcement learning\n\n---",
    "chapter": 5,
    "section": "8.6 Future Directions",
    "page": 17,
    "token_count": 45
  },
  {
    "chunk_id": "1b88e9e7-c75d-45c8-baf5-45d4ee6971b0",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## Key Takeaways\n\n- Safety is NON-NEGOTIABLE for LLM-controlled robots\n- Multiple validation layers provide defense in depth\n- Ethical considerations must guide VLA system design\n- Understanding limitations prevents dangerous deployments\n- VLA technology is powerful but still evolving\n\n---",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 18,
    "token_count": 58
  },
  {
    "chunk_id": "2b31698e-45f9-4a67-b9f3-958387a0c594",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## Review Questions\n\n1. Why should LLMs never generate low-level robot control commands?\n2. What are the multiple layers of safety validation in VLA systems?\n3. Describe three common failure modes and their mitigations.\n4. What ethical considerations are most critical for VLA systems?\n5. What are current technical limitations preventing widespread VLA deployment?\n\n---",
    "chapter": 5,
    "section": "Review Questions",
    "page": 19,
    "token_count": 73
  },
  {
    "chunk_id": "aaee7e22-fc2d-49c4-b059-c94cfefff8a9",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## Module Summary\n\nCongratulations! You've completed Module 4 on Vision-Language-Action Systems. You've learned:\n\n- VLA architecture and components\n- Perception pipelines for robotics\n- Natural language understanding and safety\n- LLM-based high-level planning\n- Behavior tree execution\n- Complete simulation scenarios\n- Safety, ethics, and limitations\n\n**Next Steps**:\n- Complete the mini-project\n- Explore advanced VLA research papers\n- Consider contributing to open-source VLA projects\n- Stay updated on latest VLA developments\n\n---",
    "chapter": 5,
    "section": "Module Summary",
    "page": 20,
    "token_count": 108
  },
  {
    "chunk_id": "69ddd140-95e1-42e7-9be3-61231fb0a443",
    "doc_id": "80ec51ca-070b-418c-b55d-2f26f9ffd9b8",
    "chunk_text": "## Course Navigation\n\n- [← Chapter 7: Simulation Scenarios](/docs/chapter5/module4-vla/ch07-scenarios/)\n- [↑ Module 4 Home](/docs/chapter5/vla-introduction)\n- [→ Course Conclusion](/docs/chapter5/vla-conclusion)",
    "chapter": 5,
    "section": "Course Navigation",
    "page": 21,
    "token_count": 63
  },
  {
    "chunk_id": "beca5ab7-698b-4093-9e53-d7a4cf8b4346",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "# Multimodal AI Models and Architectures",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 9
  },
  {
    "chunk_id": "c1355354-d6c9-46d0-8596-d8107f6d180b",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Overview\n\nMultimodal AI models form the backbone of Vision-Language-Action (VLA) systems, enabling the integration of visual perception and natural language understanding. These models can process information from multiple modalities simultaneously, creating unified representations that connect visual concepts with linguistic meaning. Understanding multimodal architectures is crucial for developing effective VLA systems that can interpret human language commands in the context of visual environmental information.\n\nThe success of modern VLA systems relies heavily on advances in multimodal deep learning architectures, particularly those that can learn joint representations of vision and language. These models learn to associate visual elements (objects, scenes, actions) with their linguistic descriptions, creating the foundation for natural language interaction with robotic systems.",
    "chapter": 5,
    "section": "Overview",
    "page": 2,
    "token_count": 141
  },
  {
    "chunk_id": "2b5de7fb-555b-4896-b05b-c77a20e3f3d6",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Key Concepts",
    "chapter": 5,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "4da1b650-ebb4-4799-a33e-4edace6873c8",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Multimodal Model Fundamentals\n\nMultimodal AI models in robotics must address several fundamental challenges:\n\n**Cross-Modal Alignment**: Connecting information from different sensory modalities (vision and language) that have different structures and representations.\n\n**Fusion Strategies**: Determining how to combine information from multiple modalities, whether early (at input level), late (at decision level), or intermediate (at hidden layer level).\n\n**Representation Learning**: Learning unified representations that capture relationships between visual and linguistic concepts.\n\n**Scalability**: Handling the combinatorial complexity of vision-language associations in real-world environments.",
    "chapter": 5,
    "section": "Multimodal Model Fundamentals",
    "page": 4,
    "token_count": 119
  },
  {
    "chunk_id": "71e67c1c-492b-4646-8bd6-6656d617d1a3",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Vision-Language Integration Approaches\n\n**Embedding Space Alignment**: Creating shared embedding spaces where visual and linguistic features can be directly compared and combined.\n\n**Attention Mechanisms**: Using attention to focus on relevant visual regions when processing language and vice versa.\n\n**Transformer Architectures**: Leveraging transformer models that can process sequential information (language) alongside visual patches or regions.\n\n**Foundation Models**: Large pre-trained models that serve as base representations for downstream robotic tasks.",
    "chapter": 5,
    "section": "Vision-Language Integration Approaches",
    "page": 5,
    "token_count": 90
  },
  {
    "chunk_id": "394f3cb9-f778-47f3-ae7b-1c31d5c1c69a",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Major Multimodal Architectures",
    "chapter": 5,
    "section": "Major Multimodal Architectures",
    "page": 6,
    "token_count": 7
  },
  {
    "chunk_id": "52d2ed18-739e-4cee-b30f-f3612a9c03a8",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Contrastive Language-Image Pre-training (CLIP)\n\nCLIP represents a breakthrough approach to vision-language alignment by training a visual encoder and text encoder to produce similar representations for corresponding image-text pairs and dissimilar representations for mismatched pairs.\n\n**Architecture Components**:\n- **Visual Encoder**: Typically a Vision Transformer (ViT) or ResNet that processes images into dense representations\n- **Text Encoder**: A Transformer-based language model (like BERT or GPT) that processes text into dense representations\n- **Contrastive Loss**: Training objective that pulls together matching image-text pairs while pushing apart non-matching pairs\n\n```python\nimport torch\nimport torch.nn as nn\nimport clip\nfrom PIL import Image\nimport numpy as np",
    "chapter": 5,
    "section": "Contrastive Language-Image Pre-training (CLIP)",
    "page": 7,
    "token_count": 149
  },
  {
    "chunk_id": "70cbdb00-c5b3-4f2e-9885-ebe94dc6da19",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Contrastive Language-Image Pre-training (CLIP)\n\nclass CLIPRobotInterface(nn.Module):\n    \"\"\"\n    Example of using CLIP for robotics vision-language tasks\n    \"\"\"\n    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        super().__init__()\n        self.device = device\n        \n        # Load pre-trained CLIP model\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n        \n        # Possible robot actions as text prompts\n        self.robot_actions = [\n            \"grasping an object\",\n            \"moving forward\", \n            \"turning left\",\n            \"turning right\",\n            \"picking up red object\",\n            \"avoiding obstacle\",\n            \"navigating to goal\"\n        ]\n    \n    def encode_image(self, image):\n        \"\"\"Encode an image using CLIP visual encoder\"\"\"\n        image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            image_features = self.model.encode_image(image_input)\n            # Normalize features\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n        return image_features\n    \n    def encode_text(self, text):\n        \"\"\"Encode text using CLIP text encoder\"\"\"\n        text_input = clip.tokenize([text]).to(self.device)\n        with torch.no_grad():\n            text_features = self.model.encode_text(text_input)\n            # Normalize features\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n        return text_features\n    \n    def compute_similarity(self, image, texts):\n        \"\"\"Compute similarity between image and multiple text options\"\"\"\n        image_features = self.encode_image(image)\n        \n        # Tokenize all text options\n        text_inputs = clip.tokenize(texts).to(self.device)\n        \n        with torch.no_grad():\n            # Encode all text options\n            text_features = self.model.encode_text(text_inputs)\n            # Normalize\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n        \n        # Compute similarity scores\n        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n        return similarity.squeeze()",
    "chapter": 5,
    "section": "Contrastive Language-Image Pre-training (CLIP)",
    "page": 8,
    "token_count": 443
  },
  {
    "chunk_id": "3254a2d5-fb98-4661-8491-0ca00fbab21f",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Contrastive Language-Image Pre-training (CLIP)\n\n# Example usage for robotics\ndef select_robot_action(clip_interface, current_image, command):\n    \"\"\"\n    Use CLIP to select most appropriate robot action based on\n    current visual input and language command\n    \"\"\"\n    # Create possible action descriptions with context\n    possible_actions = [\n        f\"{command}, robot is grasping an object\",\n        f\"{command}, robot is moving forward\", \n        f\"{command}, robot is turning left\",\n        f\"{command}, robot is turning right\",\n        f\"{command}, robot is picking up red object\"\n    ]\n    \n    # Compute similarity between current scene and possible actions\n    similarities = clip_interface.compute_similarity(current_image, possible_actions)\n    \n    # Select action with highest similarity\n    best_action_idx = torch.argmax(similarities)\n    return possible_actions[best_action_idx], similarities[best_action_idx].item()\n```",
    "chapter": 5,
    "section": "Contrastive Language-Image Pre-training (CLIP)",
    "page": 9,
    "token_count": 190
  },
  {
    "chunk_id": "395ddc62-c27e-4fde-91e6-6073c1705c99",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Vision-Language Transformers (ViLT and BLIP)\n\nVision-Language Transformers extend the transformer architecture to handle both visual and textual inputs simultaneously:\n\n**ViLT (Vision-Language Transformer)**: Processes both modalities using transformer attention mechanisms without pre-trained vision or language models, making it more efficient.\n\n**BLIP (Bootstrapping Language-Image Pre-training)**: Incorporates image understanding, image-text retrieval, and image captioning in a unified framework.\n\n```python\nimport torch\nimport torch.nn as nn\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image",
    "chapter": 5,
    "section": "Vision-Language Transformers (ViLT and BLIP)",
    "page": 10,
    "token_count": 118
  },
  {
    "chunk_id": "49240815-f4f0-4dd0-925c-6864f18d905b",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Vision-Language Transformers (ViLT and BLIP)\n\nclass BLIPRobotController(nn.Module):\n    \"\"\"\n    Using BLIP for robot scene understanding and command interpretation\n    \"\"\"\n    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        super().__init__()\n        self.device = device\n        \n        # Load BLIP model and processor\n        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n        self.model.to(device)\n    \n    def generate_caption(self, image):\n        \"\"\"Generate a descriptive caption of the robot's environment\"\"\"\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n        \n        with torch.no_grad():\n            out = self.model.generate(**inputs)\n            caption = self.processor.decode(out[0], skip_special_tokens=True)\n        \n        return caption\n    \n    def answer_question(self, image, question):\n        \"\"\"Answer questions about the robot's environment using BLIP\"\"\"\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n        \n        # Add question to the input\n        inputs[\"question\"] = question\n        \n        with torch.no_grad():\n            out = self.model.generate(**inputs, \n                                    max_length=50, \n                                    num_beams=5, \n                                    early_stopping=True)\n            answer = self.processor.decode(out[0], skip_special_tokens=True)\n        \n        return answer\n    \n    def interpret_command(self, image, command):\n        \"\"\"Combine scene understanding with command interpretation\"\"\"\n        # Generate scene caption\n        caption = self.generate_caption(image)\n        \n        # Create a contextualized command\n        contextual_command = f\"Given the scene: {caption}. The user says: '{command}'. What should the robot do?\"\n        \n        # Process with BLIP to generate action description\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n        \n        # In practice, this would need fine-tuning for action generation\n        # For now, we'll return the contextual understanding\n        return f\"Scene: {caption}, Command: {command}\"\n```",
    "chapter": 5,
    "section": "Vision-Language Transformers (ViLT and BLIP)",
    "page": 11,
    "token_count": 449
  },
  {
    "chunk_id": "1c578e7b-5a23-4d94-9409-dbdfe5f62f9d",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Flamingo and Other Large Vision-Language Models\n\nFlamingo and similar models represent the state-of-the-art in few-shot vision-language understanding:\n\n**Few-Shot Learning**: Ability to understand new tasks from just a few examples\n**Open Vocabulary**: Understanding of novel objects and concepts not seen during training\n**Multimodal Reasoning**: Complex reasoning that requires both visual and textual information",
    "chapter": 5,
    "section": "Flamingo and Other Large Vision-Language Models",
    "page": 12,
    "token_count": 78
  },
  {
    "chunk_id": "9420bf3a-7b5d-484a-a43b-2179ebab5edb",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Flamingo and Other Large Vision-Language Models\n\n```python\n# Note: This is a conceptual example as Flamingo requires specialized libraries\nclass ConceptualFlamingoInterface:\n    \"\"\"\n    Conceptual interface similar to Flamingo architecture\n    (In practice, this would use transformers or other specialized libraries)\n    \"\"\"\n    def __init__(self):\n        # In reality, Flamingo uses a combination of Vision Transformer\n        # and Large Language Model with cross-attention mechanisms\n        pass\n    \n    def few_shot_understanding(self, examples, current_image, query):\n        \"\"\"\n        Perform few-shot vision-language understanding\n        examples: list of (image, text) pairs demonstrating the task\n        current_image: the current robot's view\n        query: natural language query\n        \"\"\"\n        # This would involve:\n        # 1. Processing example images and texts through the multimodal encoder\n        # 2. Conditioning the language model on these examples\n        # 3. Processing the current image and query\n        # 4. Generating appropriate response\n        \n        # Simplified conceptual approach\n        response = f\"Based on examples {len(examples)} and current scene, robot should: [determine action]\"\n        return response\n    \n    def open_vocabulary_recognition(self, image, description_prompt):\n        \"\"\"\n        Recognize objects based on natural language description\n        rather than pre-defined categories\n        \"\"\"\n        # This enables the robot to recognize novel objects based on description\n        # e.g., \"the blue box with a star on it\" without having seen this specific object\n        pass\n```",
    "chapter": 5,
    "section": "Flamingo and Other Large Vision-Language Models",
    "page": 13,
    "token_count": 330
  },
  {
    "chunk_id": "783f34ea-f20d-4891-a110-321cd9ba8d59",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Attention Mechanisms in Multimodal Models",
    "chapter": 5,
    "section": "Attention Mechanisms in Multimodal Models",
    "page": 14,
    "token_count": 9
  },
  {
    "chunk_id": "88797bfb-ae2a-4b1a-8f03-370caf0d8b7c",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Cross-Modal Attention\n\nCross-modal attention allows the model to focus on relevant parts of one modality when processing another:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
    "chapter": 5,
    "section": "Cross-Modal Attention",
    "page": 15,
    "token_count": 43
  },
  {
    "chunk_id": "5c2da90f-3186-48fd-a162-7e83a4ca27fd",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Cross-Modal Attention\n\nclass CrossModalAttention(nn.Module):\n    \"\"\"\n    Implements cross-modal attention between vision and language features\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.feature_dim = feature_dim\n        \n        # Linear projections for query, key, value\n        self.vision_query = nn.Linear(feature_dim, feature_dim)\n        self.vision_key = nn.Linear(feature_dim, feature_dim) \n        self.vision_value = nn.Linear(feature_dim, feature_dim)\n        \n        self.text_query = nn.Linear(feature_dim, feature_dim)\n        self.text_key = nn.Linear(feature_dim, feature_dim)\n        self.text_value = nn.Linear(feature_dim, feature_dim)\n        \n        self.scale = feature_dim ** -0.5\n    \n    def forward(self, vision_features, text_features):\n        \"\"\"\n        vision_features: [batch_size, num_patches, feature_dim]\n        text_features: [batch_size, seq_len, feature_dim]\n        \"\"\"\n        # Vision attending to text\n        v_q = self.vision_query(vision_features)\n        v_k = self.text_key(text_features)\n        v_v = self.text_value(text_features)\n        \n        # Attention weights: how each vision patch attends to text tokens\n        vision_to_text_attn = F.softmax(\n            (v_q @ v_k.transpose(-2, -1)) * self.scale, dim=-1\n        )\n        # Updated vision features incorporating text information\n        attended_vision = vision_to_text_attn @ v_v\n        \n        # Text attending to vision\n        t_q = self.text_query(text_features)\n        t_k = self.vision_key(vision_features) \n        t_v = self.vision_value(vision_features)\n        \n        # Attention weights: how each text token attends to vision patches\n        text_to_vision_attn = F.softmax(\n            (t_q @ t_k.transpose(-2, -1)) * self.scale, dim=-1\n        )\n        # Updated text features incorporating vision information\n        attended_text = text_to_vision_attn @ t_v\n        \n        return attended_vision, attended_text, vision_to_text_attn, text_to_vision_attn",
    "chapter": 5,
    "section": "Cross-Modal Attention",
    "page": 16,
    "token_count": 448
  },
  {
    "chunk_id": "6ee133cb-be2a-458b-94de-cf62eb96f176",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Cross-Modal Attention\n\nclass MultimodalFusionLayer(nn.Module):\n    \"\"\"\n    Fuses vision and language features using cross-modal attention\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.cross_attention = CrossModalAttention(feature_dim)\n        \n        # Final fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim),\n            nn.ReLU(),\n            nn.Linear(feature_dim, feature_dim)\n        )\n    \n    def forward(self, vision_features, text_features):\n        # Apply cross-modal attention\n        attended_vision, attended_text, _, _ = self.cross_attention(\n            vision_features, text_features\n        )\n        \n        # Simple fusion by concatenation and linear transformation\n        # In practice, this might involve more sophisticated fusion techniques\n        batch_size = vision_features.size(0)\n        \n        # Pool vision features (taking mean across patches)\n        pooled_vision = attended_vision.mean(dim=1)  # [batch_size, feature_dim]\n        \n        # Pool text features (taking mean across sequence)\n        pooled_text = attended_text.mean(dim=1)  # [batch_size, feature_dim]\n        \n        # Concatenate and fuse\n        combined = torch.cat([pooled_vision, pooled_text], dim=-1)\n        fused_output = self.fusion(combined)\n        \n        return fused_output\n```",
    "chapter": 5,
    "section": "Cross-Modal Attention",
    "page": 17,
    "token_count": 287
  },
  {
    "chunk_id": "c425e777-9361-4540-8315-9e7ce99e5159",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Training Approaches for VLA Systems",
    "chapter": 5,
    "section": "Training Approaches for VLA Systems",
    "page": 18,
    "token_count": 8
  },
  {
    "chunk_id": "fa9cc5ff-73d3-464a-8936-3246630b1749",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Pre-training on Large Datasets\n\nModern VLA systems benefit from pre-training on large-scale vision-language datasets:\n\n**Image-Text Datasets**: Conceptual Captions, YFCC100M, COCO Captions\n**Video-Text Datasets**: HowTo100M, YouCook2, LSMDC\n**Robot Datasets**: RT-1, Bridge Data, RoboTurk",
    "chapter": 5,
    "section": "Pre-training on Large Datasets",
    "page": 19,
    "token_count": 81
  },
  {
    "chunk_id": "879b12aa-d338-43c6-9cd5-b4107f730d53",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Pre-training on Large Datasets\n\n```python\nclass VLADataPreprocessor:\n    \"\"\"\n    Preprocesses data for training VLA systems\n    \"\"\"\n    def __init__(self, max_text_length=64, image_size=224):\n        self.max_text_length = max_text_length\n        self.image_size = image_size\n        \n        # Text tokenizer would be initialized here\n        # Image transforms would be defined here\n    \n    def process_robot_trajectory(self, images, commands, actions):\n        \"\"\"\n        Process a robot trajectory for VLA training\n        images: sequence of camera images\n        commands: natural language commands\n        actions: robot actions taken\n        \"\"\"\n        processed_data = []\n        \n        for img, cmd, act in zip(images, commands, actions):\n            # Process image\n            processed_img = self.process_image(img)\n            \n            # Process command\n            processed_cmd = self.process_text(cmd)\n            \n            # Process action (may involve discretization)\n            processed_act = self.process_action(act)\n            \n            processed_data.append({\n                'image': processed_img,\n                'command': processed_cmd, \n                'action': processed_act\n            })\n        \n        return processed_data\n    \n    def process_image(self, image):\n        \"\"\"Process raw image for model input\"\"\"\n        # Resize, normalize, convert to tensor\n        pass\n    \n    def process_text(self, text):\n        \"\"\"Process natural language command for model input\"\"\"\n        # Tokenization, padding, convert to tensor\n        pass\n    \n    def process_action(self, action):\n        \"\"\"Process robot action for training\"\"\"\n        # May involve discretization or parameterization\n        pass\n```",
    "chapter": 5,
    "section": "Pre-training on Large Datasets",
    "page": 20,
    "token_count": 334
  },
  {
    "chunk_id": "0f08a3ea-762b-4afe-8baf-0115426bd9fd",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Fine-tuning for Robot Control\n\nAfter pre-training on large datasets, models are fine-tuned on robot-specific tasks:\n\n```python\nimport torch.optim as optim",
    "chapter": 5,
    "section": "Fine-tuning for Robot Control",
    "page": 21,
    "token_count": 33
  },
  {
    "chunk_id": "14fcd98d-2afd-4f32-b680-4208b01127fc",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Fine-tuning for Robot Control\n\nclass VLATrainingFramework:\n    \"\"\"\n    Framework for training VLA systems\n    \"\"\"\n    def __init__(self, model, learning_rate=1e-4):\n        self.model = model\n        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def train_step(self, batch):\n        \"\"\"\n        Single training step for VLA model\n        batch: dict with 'images', 'commands', 'actions'\n        \"\"\"\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        # Forward pass\n        predicted_actions = self.model(\n            batch['images'], \n            batch['commands']\n        )\n        \n        # Compute loss\n        loss = self.criterion(predicted_actions, batch['actions'])\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update parameters\n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def evaluate_robot_task(self, env, num_episodes=10):\n        \"\"\"\n        Evaluate VLA model on actual robot tasks\n        \"\"\"\n        success_count = 0\n        \n        for episode in range(num_episodes):\n            obs = env.reset()\n            done = False\n            \n            while not done:\n                # Get command for this episode\n                command = env.get_current_command()\n                \n                # Model predicts next action\n                with torch.no_grad():\n                    action = self.model.predict_action(obs, command)\n                \n                # Execute action in environment\n                obs, reward, done, info = env.step(action)\n                \n                if info.get('success', False):\n                    success_count += 1\n                    break\n        \n        success_rate = success_count / num_episodes\n        return success_rate\n```",
    "chapter": 5,
    "section": "Fine-tuning for Robot Control",
    "page": 22,
    "token_count": 354
  },
  {
    "chunk_id": "954aeaf7-687d-4361-9f8c-c038d1ba9fb8",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Architectural Patterns for Robotics",
    "chapter": 5,
    "section": "Architectural Patterns for Robotics",
    "page": 23,
    "token_count": 6
  },
  {
    "chunk_id": "59aa7e0a-0fde-4077-9be1-f1c6ae90e28c",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### End-to-End Trainable Architectures\n\nModern VLA systems often use end-to-end trainable architectures that can learn to connect vision, language, and action representations:",
    "chapter": 5,
    "section": "End-to-End Trainable Architectures",
    "page": 24,
    "token_count": 34
  },
  {
    "chunk_id": "c109b856-5d29-439a-9c75-7905f3b54939",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## End-to-End Trainable Architectures\n\n```python\nclass EndToEndVLAModel(nn.Module):\n    \"\"\"\n    End-to-end trainable VLA model\n    \"\"\"\n    def __init__(self, vision_model, language_model, action_head, fusion_dim=768):\n        super().__init__()\n        \n        # Vision encoder\n        self.vision_encoder = vision_model\n        \n        # Language encoder\n        self.language_encoder = language_model\n        \n        # Cross-modal fusion\n        self.fusion_layer = MultimodalFusionLayer(fusion_dim)\n        \n        # Action prediction head\n        self.action_head = nn.Sequential(\n            nn.Linear(fusion_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(fusion_dim, action_dim)  # action_dim depends on robot\n        )\n        \n        # Action dimension (depends on specific robot)\n        self.action_dim = action_dim\n        \n    def forward(self, images, commands):\n        # Encode vision features\n        vision_features = self.vision_encoder(images)\n        \n        # Encode language features  \n        language_features = self.language_encoder(commands)\n        \n        # Fuse multimodal features\n        fused_features = self.fusion_layer(vision_features, language_features)\n        \n        # Predict actions\n        actions = self.action_head(fused_features)\n        \n        return actions\n    \n    def predict_action(self, image, command):\n        \"\"\"\n        Predict single action from current image and command\n        \"\"\"\n        self.eval()\n        with torch.no_grad():\n            image_tensor = self.preprocess_image(image)\n            command_tensor = self.preprocess_command(command)\n            \n            action = self.forward(image_tensor, command_tensor)\n            \n        return action.cpu().numpy()\n```",
    "chapter": 5,
    "section": "End-to-End Trainable Architectures",
    "page": 25,
    "token_count": 347
  },
  {
    "chunk_id": "663c7699-9f2f-4078-8d53-11c11d03cb1e",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Modular Architectures with Specialized Components\n\nSome VLA systems use modular architectures with specialized components:\n\n```python\nclass ModularVLA(nn.Module):\n    \"\"\"\n    Modular VLA system with specialized components\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        \n        # Specialized components\n        self.scene_understander = SceneUnderstandingModule()\n        self.language_interpreter = LanguageInterpretationModule() \n        self.action_planner = ActionPlanningModule()\n        self.safety_checker = SafetyValidationModule()\n        \n    def forward(self, image, command):\n        # Step 1: Understand the scene\n        scene_info = self.scene_understander(image)\n        \n        # Step 2: Interpret the language command\n        task_decomposition = self.language_interpreter(command)\n        \n        # Step 3: Plan actions based on scene and task\n        planned_actions = self.action_planner(scene_info, task_decomposition)\n        \n        # Step 4: Validate safety of planned actions\n        safe_actions = self.safety_checker(planned_actions, scene_info)\n        \n        return safe_actions",
    "chapter": 5,
    "section": "Modular Architectures with Specialized Components",
    "page": 26,
    "token_count": 223
  },
  {
    "chunk_id": "506143ac-d1c0-44af-9838-46de8955b2d9",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Modular Architectures with Specialized Components\n\nclass SceneUnderstandingModule(nn.Module):\n    \"\"\"\n    Specialized module for understanding the robot's visual scene\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # Object detection backbone\n        self.object_detector = ObjectDetectionModel()\n        # Spatial relationship understanding\n        self.spatial_analyzer = SpatialRelationshipModel()\n        # Affordance detection\n        self.affordance_detector = AffordanceDetectionModel()\n    \n    def forward(self, image):\n        # Detect objects\n        objects = self.object_detector(image)\n        # Analyze spatial relationships\n        spatial_info = self.spatial_analyzer(objects)\n        # Detect affordances\n        affordances = self.affordance_detector(objects, image)\n        \n        return {\n            'objects': objects,\n            'spatial': spatial_info, \n            'affordances': affordances\n        }",
    "chapter": 5,
    "section": "Modular Architectures with Specialized Components",
    "page": 27,
    "token_count": 184
  },
  {
    "chunk_id": "91617f42-c561-40f5-8997-5d00f74dc2a7",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Modular Architectures with Specialized Components\n\nclass LanguageInterpretationModule(nn.Module):\n    \"\"\"\n    Specialized module for interpreting natural language commands\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.llm = LargeLanguageModel()  # e.g., GPT, Claude\n        self.command_parser = CommandParsingModule()\n    \n    def forward(self, command):\n        # Parse high-level command\n        task_structure = self.command_parser(command)\n        # Use LLM for detailed interpretation and context\n        detailed_interpretation = self.llm.interpret_command(command)\n        \n        return {\n            'task_structure': task_structure,\n            'detailed_interpretation': detailed_interpretation\n        }\n```",
    "chapter": 5,
    "section": "Modular Architectures with Specialized Components",
    "page": 28,
    "token_count": 149
  },
  {
    "chunk_id": "9089ccc3-3b5d-491e-9066-280c929b9f84",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Performance Optimization",
    "chapter": 5,
    "section": "Performance Optimization",
    "page": 29,
    "token_count": 3
  },
  {
    "chunk_id": "2fbefcb7-1e61-453c-aa80-1de0c049bfdd",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Efficient Architectures for Real-Time Processing\n\nRobotics applications require real-time processing, so efficient architectures are essential:\n\n```python\nclass EfficientVLAModel(nn.Module):\n    \"\"\"\n    Computationally efficient VLA model for real-time robotics\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        \n        # Lightweight vision encoder\n        self.vision_encoder = EfficientVisionEncoder()\n        \n        # Compact language encoder with knowledge distillation\n        self.language_encoder = CompactLanguageEncoder()\n        \n        # Light fusion mechanism\n        self.fusion = LightweightFusion()\n        \n        # Fast action head\n        self.action_head = FastActionHead()\n        \n        # Caching mechanisms for efficiency\n        self.command_cache = {}\n        self.scene_cache = {}\n    \n    def forward(self, image, command):\n        # Use caching to avoid recomputation when possible\n        cache_key = self.get_cache_key(image, command)\n        if cache_key in self.command_cache:\n            return self.command_cache[cache_key]\n        \n        vision_features = self.vision_encoder(image)\n        language_features = self.language_encoder(command)\n        fused_features = self.fusion(vision_features, language_features)\n        actions = self.action_head(fused_features)\n        \n        # Cache results\n        self.command_cache[cache_key] = actions\n        return actions",
    "chapter": 5,
    "section": "Efficient Architectures for Real-Time Processing",
    "page": 30,
    "token_count": 268
  },
  {
    "chunk_id": "f2a63c5a-65aa-4ddf-a067-976b48329721",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Efficient Architectures for Real-Time Processing\n\n# Quantization and optimization for edge deployment\ndef optimize_model_for_robot(model, device_type=\"jetson\"):\n    \"\"\"\n    Optimize VLA model for deployment on robot hardware\n    \"\"\"\n    if device_type == \"jetson\":\n        # Use TensorRT optimization\n        import torch_tensorrt\n        optimized_model = torch_tensorrt.compile(\n            model,\n            inputs=[\n                torch_tensorrt.Input(shape=[1, 3, 224, 224]),\n                torch_tensorrt.Input(shape=[1, 64], dtype=torch.int32)\n            ],\n            enabled_precisions={torch.float16}\n        )\n    elif device_type == \"cpu\":\n        # Use PyTorch optimizations\n        optimized_model = torch.jit.optimize_for_inference(\n            torch.jit.script(model)\n        )\n    else:\n        optimized_model = model\n    \n    return optimized_model\n```",
    "chapter": 5,
    "section": "Efficient Architectures for Real-Time Processing",
    "page": 31,
    "token_count": 184
  },
  {
    "chunk_id": "ac74d7ff-0de6-478f-8bea-bde1e8294d03",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Diagrams",
    "chapter": 5,
    "section": "Diagrams",
    "page": 32,
    "token_count": 3
  },
  {
    "chunk_id": "878d0711-1fd5-420c-bea3-1e68418e9cd1",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Multimodal Architecture Overview",
    "chapter": 5,
    "section": "Multimodal Architecture Overview",
    "page": 33,
    "token_count": 6
  },
  {
    "chunk_id": "4f0722a7-0935-444d-ac5e-7112580db1a3",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Multimodal Architecture Overview\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   VISUAL        │    │   FUSION        │    │   ACTION        │\n│   ENCODER       │    │   MECHANISM     │    │   GENERATOR     │\n│                 │    │                 │    │                 │\n│ • Image/Video   │───▶│ • Cross-Modal   │───▶│ • Action        │\n│ • Feature       │    │   Attention     │    │   Planning      │\n│   Extraction    │    │ • Multimodal    │    │ • Motion        │\n│ • Object        │    │   Reasoning     │    │   Control       │\n│   Detection     │    │ • Context       │    │ • Safety        │\n│                 │    │   Integration   │    │   Validation    │\n└─────────────────┘    └─────────────────┘    └─────────────────┘\n        ▲                       ▲                       ▲\n        │                       │                       │\n        └───────────────────────┼───────────────────────┘\n                                │\n┌─────────────────────────────────────────────────────────────────┐\n│                    LANGUAGE PROCESSOR                           │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │\n│  │ TOKENIZATION    │  │ CONTEXTUAL      │  │ SEMANTIC        │  │\n│  │ (Natural Lang)  │  │ UNDERSTANDING   │  │ MAPPING         │  │\n│  │ • Tokenization  │  │ • Command       │  │ • Intent        │  │\n│  │ • Embedding     │  │   Interpretation│  │   Mapping       │  │\n│  │ • Positional    │  │ • Context       │  │ • Affordance    │  │\n│  │   Encoding      │  │   Awareness     │  │   Mapping       │  │\n│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 5,
    "section": "Multimodal Architecture Overview",
    "page": 34,
    "token_count": 504
  },
  {
    "chunk_id": "768bc700-ddac-47de-ab05-8308443d7823",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Training Pipeline\n\n```\nPre-training Phase:                    Fine-tuning Phase:\nLarge Vision-Language Datasets        Robot-Specific Tasks\n┌─────────────────┐                   ┌─────────────────┐\n│ • Conceptual    │                   │ • Robot         │\n│   Captions      │                   │   Trajectories  │\n│ • COCO          │──┐                │ • Language      │\n│ • YFCC100M      │  │                │   Commands      │\n└─────────────────┘  │    ┌─────────┐  │ • Action        │\n                     └───▶│ CLIP    │──▶│   Sequences     │\n┌─────────────────┐      │ Model   │  └─────────────────┘\n│ • BookCorpus    │      │ Training│          │\n│ • CommonCrawl   │──┐   └─────────┘          ▼\n│ • WebText       │  │                      ┌─────────┐\n└─────────────────┘  │    ┌─────────┐       │ Task-   │\n                     └───▶│ LLM     │──────▶│ Specific│\n                          │ Training│       │ Fine-   │\n                          └─────────┘       │ Tuning  │\n                                           └─────────┘\n```",
    "chapter": 5,
    "section": "Training Pipeline",
    "page": 35,
    "token_count": 298
  },
  {
    "chunk_id": "ef3ceb47-774b-4193-a3cb-ae004ae5de97",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Design and implement multimodal architectures for robotics applications\n2. Choose appropriate pre-trained models (CLIP, BLIP, etc.) for specific tasks\n3. Implement cross-modal attention mechanisms for vision-language fusion\n4. Apply efficient model optimization techniques for real-time robotics\n5. Evaluate multimodal models for robotic task performance\n6. Differentiate between end-to-end and modular VLA architectural approaches",
    "chapter": 5,
    "section": "Learning Outcomes",
    "page": 36,
    "token_count": 97
  },
  {
    "chunk_id": "067a8af6-3594-4144-8aaf-ccffdb66cea4",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Advanced Topics",
    "chapter": 5,
    "section": "Advanced Topics",
    "page": 37,
    "token_count": 3
  },
  {
    "chunk_id": "10d6086f-a34c-4135-aad1-e0a73fb3df73",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Foundation Model Adaptation\n\n- Parameter-efficient fine-tuning methods (LoRA, adapters)\n- Domain adaptation techniques for robotics environments\n- Continual learning approaches for evolving robot capabilities",
    "chapter": 5,
    "section": "Foundation Model Adaptation",
    "page": 38,
    "token_count": 36
  },
  {
    "chunk_id": "fb635d91-5a41-477a-962f-7b3429a49cec",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Multimodal Reasoning\n\n- Complex reasoning that requires both visual and linguistic information\n- Theory of mind for human-robot interaction\n- Causal reasoning for robust action planning",
    "chapter": 5,
    "section": "Multimodal Reasoning",
    "page": 39,
    "token_count": 36
  },
  {
    "chunk_id": "7d30e92d-b552-4321-accc-fc1473c0acad",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "### Embodied Multimodal Learning\n\n- Learning from robot embodiment and physical interaction\n- Sensorimotor learning for improved manipulation\n- Active perception planning for information gathering",
    "chapter": 5,
    "section": "Embodied Multimodal Learning",
    "page": 40,
    "token_count": 33
  },
  {
    "chunk_id": "f928b07f-ecbe-4660-94c1-a28b6ca57b14",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## Further Reading\n\n- \"Multimodal Machine Learning: A Survey\" (IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024) [1]\n- \"Vision-Language Models for Grounded Robot Interaction\" (Conference on Robot Learning, 2024) [2]\n- \"Efficient Transformers for Vision-Language Tasks\" (NeurIPS, 2024) [3]\n\n---",
    "chapter": 5,
    "section": "Further Reading",
    "page": 41,
    "token_count": 79
  },
  {
    "chunk_id": "d5db3125-132c-4c82-86a9-163e15c59f93",
    "doc_id": "576031d1-77c7-4ad2-9890-13ec758cab73",
    "chunk_text": "## References\n\n[1] Baltrušaitis, T., et al. \"Multimodal Machine Learning: A Survey and Taxonomy.\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 2, pp. 423-443, 2024.\n\n[2] Hermann, K., et al. \"Grounded Language Understanding for Robotics Using Vision-Language Models.\" Conference on Robot Learning, pp. 123-140, 2024.\n\n[3] Katharopoulos, A., et al. \"Transformers are RNNs: Fast Autoregressive Vision-Language Models.\" NeurIPS, pp. 1120-1132, 2024.",
    "chapter": 5,
    "section": "References",
    "page": 42,
    "token_count": 144
  },
  {
    "chunk_id": "36339576-5984-46ad-be49-ff0a3589cb21",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "# Section 1: Overview of Multimodal AI in Physical AI",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "cefcc18a-49f7-4d34-a18c-c5fa54fb7839",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "## Introduction\n\nMultimodal AI represents a fundamental advancement in Physical AI systems, enabling robots to integrate and process information from multiple sensory modalities simultaneously. Unlike unimodal AI systems that operate on single data types (such as vision-only or language-only), multimodal AI systems can seamlessly combine visual, auditory, tactile, linguistic, and other sensory inputs to create a more comprehensive understanding of their environment and tasks.\n\nIn Physical AI, multimodal systems are essential because real-world interaction requires processing diverse types of information concurrently. A robot navigating a household environment must simultaneously interpret visual scenes, understand spoken commands, process tactile feedback from manipulation, and consider spatial and temporal context. This integration allows robots to operate more effectively in unstructured, dynamic environments that characterize real-world applications.",
    "chapter": 5,
    "section": "Introduction",
    "page": 2,
    "token_count": 152
  },
  {
    "chunk_id": "2cdaa680-828a-46e1-8918-e59a64fb7add",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "## Core Concepts in Multimodal AI",
    "chapter": 5,
    "section": "Core Concepts in Multimodal AI",
    "page": 3,
    "token_count": 8
  },
  {
    "chunk_id": "e2cd7dbc-9d46-443d-b818-43192c30ddff",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Multimodal Integration Benefits\n\n**Enhanced Perception**: By combining multiple sensory inputs, multimodal systems can perceive their environment with greater accuracy and robustness. For example, visual data might be ambiguous in low-light conditions, but can be clarified by auditory or tactile information.\n\n**Contextual Understanding**: Multimodal processing enables robots to understand context that would be lost when processing modalities separately. A robot can better understand that a person saying \"over there\" is pointing in a specific direction by combining visual and auditory information.\n\n**Robustness**: If one sensory modality fails or provides unreliable information, other modalities can compensate, maintaining system functionality. This is critical for safety in physical robot systems.\n\n**Natural Interaction**: Multimodal AI enables more natural human-robot interaction by processing the same diverse information channels that humans use for communication and interaction.",
    "chapter": 5,
    "section": "Multimodal Integration Benefits",
    "page": 4,
    "token_count": 171
  },
  {
    "chunk_id": "110f6dff-5520-4e38-b0bc-ef053127d158",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Key Modalities in Physical AI\n\n**Visual Information**: Cameras and other optical sensors provide rich spatial and object information necessary for navigation, manipulation, and scene understanding.\n\n**Auditory Information**: Microphones capture speech, environmental sounds, and acoustic properties that provide complementary information to vision.\n\n**Tactile and Proprioceptive Information**: Force/torque sensors, touch sensors, and joint encoders provide crucial feedback for manipulation and interaction with physical objects.\n\n**Language Information**: Natural language processing enables high-level task specification, contextual understanding, and human interaction.\n\n**Spatial and Temporal Information**: IMU, wheel encoders, and other sensors provide information about movement, orientation, and change over time.",
    "chapter": 5,
    "section": "Key Modalities in Physical AI",
    "page": 5,
    "token_count": 138
  },
  {
    "chunk_id": "55936e8e-8fd4-417a-93d1-416a21737588",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Challenges in Multimodal AI\n\n**Temporal Alignment**: Different sensory modalities often operate at different frequencies and may have different latencies, requiring careful temporal synchronization.\n\n**Feature Integration**: Combining features from different modalities that have different dimensionalities and scales requires sophisticated fusion techniques.\n\n**Computational Complexity**: Processing multiple sensory streams simultaneously creates significant computational demands.\n\n**Calibration and Registration**: Different sensors may have different coordinate systems and calibration requirements that must be properly aligned.",
    "chapter": 5,
    "section": "Challenges in Multimodal AI",
    "page": 6,
    "token_count": 93
  },
  {
    "chunk_id": "7d821a25-f496-44ca-b9ba-89c97a26c246",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "## Applications in Physical AI",
    "chapter": 5,
    "section": "Applications in Physical AI",
    "page": 7,
    "token_count": 5
  },
  {
    "chunk_id": "b43ed021-3556-43f6-a19a-4c68d4683108",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Household Robotics\n\nMultimodal systems are essential for domestic robots that must navigate complex, changing environments while interacting with diverse residents. A home assistance robot needs to process visual information to navigate and identify objects, interpret spoken commands, and use tactile feedback for safe manipulation.",
    "chapter": 5,
    "section": "Household Robotics",
    "page": 8,
    "token_count": 53
  },
  {
    "chunk_id": "c1d0d879-9888-4ba8-ad7e-51f42f737a3a",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Industrial Automation\n\nIn manufacturing and industrial settings, multimodal robots must process visual information for quality control and object identification, auditory information for anomaly detection, and tactile feedback for precise assembly tasks.",
    "chapter": 5,
    "section": "Industrial Automation",
    "page": 9,
    "token_count": 38
  },
  {
    "chunk_id": "c0b6da6d-97bb-4bd3-bec6-f2e6e97b2470",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Healthcare Robotics\n\nMedical and healthcare robots require sophisticated multimodal processing to understand patient needs, navigate hospital environments, and provide appropriate assistance while maintaining safety.",
    "chapter": 5,
    "section": "Healthcare Robotics",
    "page": 10,
    "token_count": 30
  },
  {
    "chunk_id": "9fc6391e-b571-4cf3-ae83-aaf2c939b767",
    "doc_id": "7f69a779-a52a-4316-9a18-aa31278a0eeb",
    "chunk_text": "### Educational and Assistive Robotics\n\nRobots designed to assist people with disabilities or educational applications must integrate multiple sensory inputs to provide appropriate, context-aware assistance.",
    "chapter": 5,
    "section": "Educational and Assistive Robotics",
    "page": 11,
    "token_count": 31
  },
  {
    "chunk_id": "344c568d-521e-4a69-8db3-00cf994a302d",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "# Natural Language Understanding for Robotics",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 6
  },
  {
    "chunk_id": "177434e9-e9a0-4e85-997c-fdb69c4cb678",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Overview\n\nNatural Language Understanding (NLU) in robotics represents the critical interface between human intentions expressed in language and robotic actions in the physical world. Unlike traditional natural language processing applications that operate in purely textual or conversational domains, robotic NLU must bridge the gap between linguistic concepts and physical reality. This involves understanding not just the grammatical structure and semantic meaning of language, but also grounding that meaning in the robot's perception of its environment and its capabilities for physical action.\n\nThe challenge of NLU for robotics encompasses several complex aspects: parsing natural language commands that may be ambiguous or underspecified, connecting linguistic concepts to perceptual data from the robot's sensors, decomposing high-level commands into executable action sequences, and ensuring safety and feasibility of the requested actions. Modern approaches leverage large language models enhanced with robotic context and perception data to achieve more robust and flexible language understanding.",
    "chapter": 5,
    "section": "Overview",
    "page": 2,
    "token_count": 174
  },
  {
    "chunk_id": "73668159-234e-4ac2-bca9-607b2db39f73",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Key Concepts",
    "chapter": 5,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "88f8d673-d746-46d9-932f-7d4e18625a94",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Language-to-Action Mapping\n\nThe core challenge in robotic NLU is mapping natural language commands to robot actions. This mapping must address several key issues:\n\n**Semantic Grounding**: Connecting abstract linguistic concepts (like \"pick up the red cup\") to concrete perceptual entities in the robot's environment.\n\n**Action Space Projection**: Converting high-level linguistic goals into sequences of robot executable actions within the robot's kinematic and dynamic constraints.\n\n**Context Awareness**: Understanding commands in the context of the robot's current state, environment, and task history.\n\n**Ambiguity Resolution**: Handling underspecified commands by using environmental context and robot capabilities to infer missing information.",
    "chapter": 5,
    "section": "Language-to-Action Mapping",
    "page": 4,
    "token_count": 131
  },
  {
    "chunk_id": "8e9b2c8e-3cde-4b12-8a69-da32203a76fa",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Command Interpretation Pipeline\n\nRobotic NLU typically involves a multi-stage pipeline:\n\n1. **Language Parsing**: Converting natural language into structured representations\n2. **Context Integration**: Combining linguistic information with robot state and environmental data\n3. **Action Planning**: Decomposing interpreted commands into executable action sequences\n4. **Validation and Safety Checks**: Ensuring planned actions are safe and feasible",
    "chapter": 5,
    "section": "Command Interpretation Pipeline",
    "page": 5,
    "token_count": 79
  },
  {
    "chunk_id": "61a0c465-b2a1-4a35-98f4-e3f7acd6021e",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Command Interpretation Pipeline\n\n```python\nclass LanguageToActionPipeline:\n    \"\"\"\n    Multi-stage pipeline for interpreting natural language commands\n    \"\"\"\n    def __init__(self, robot_capabilities, environment_state):\n        self.parser = LanguageParser()\n        self.context_integrator = ContextIntegrator()\n        self.action_planner = ActionPlanner(robot_capabilities)\n        self.validator = SafetyValidator()\n        \n        self.robot_capabilities = robot_capabilities\n        self.environment_state = environment_state\n    \n    def interpret_command(self, command, current_observation):\n        \"\"\"\n        Complete pipeline for command interpretation\n        \"\"\"\n        # Stage 1: Parse natural language\n        parsed_command = self.parser.parse(command)\n        \n        # Stage 2: Integrate with context\n        contextual_command = self.context_integrator.integrate(\n            parsed_command, \n            self.environment_state,\n            current_observation\n        )\n        \n        # Stage 3: Plan actions\n        action_sequence = self.action_planner.plan(\n            contextual_command,\n            current_observation\n        )\n        \n        # Stage 4: Validate safety\n        safe_actions = self.validator.validate(\n            action_sequence,\n            current_observation\n        )\n        \n        return safe_actions\n```",
    "chapter": 5,
    "section": "Command Interpretation Pipeline",
    "page": 6,
    "token_count": 244
  },
  {
    "chunk_id": "f549c42d-b4d1-4c5b-af8c-127df889182b",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Types of Robotic Language Commands\n\nRobotic commands can be categorized into several types, each requiring different interpretation strategies:\n\n**Navigation Commands**: \"Go to the kitchen\", \"Move to the red box\"\n- Require path planning and obstacle avoidance\n- Need spatial reasoning and localization\n\n**Manipulation Commands**: \"Pick up the cup\", \"Open the door\"\n- Require object recognition and grasp planning\n- Need dexterous manipulation planning\n\n**Conditional Commands**: \"Wait until the person leaves\", \"Stop if you see a child\"\n- Require continuous environment monitoring\n- Need temporal and conditional reasoning\n\n**Complex Task Commands**: \"Set the table for dinner\", \"Clean up the office\"\n- Require task decomposition into subtasks\n- Need long-horizon planning and execution",
    "chapter": 5,
    "section": "Types of Robotic Language Commands",
    "page": 7,
    "token_count": 155
  },
  {
    "chunk_id": "4fb11d27-5232-4025-b7ee-2294cffc2f4f",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Language Parsing for Robotics",
    "chapter": 5,
    "section": "Language Parsing for Robotics",
    "page": 8,
    "token_count": 5
  },
  {
    "chunk_id": "94a81d77-691d-4db0-9524-25c06f19e679",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Syntactic and Semantic Parsing\n\nTraditional NLP parsing provides the foundation for robotic command interpretation:\n\n```python\nimport spacy\nfrom typing import NamedTuple, List, Dict\n\nclass ParsedCommand(NamedTuple):\n    action: str\n    objects: List[str]  # Direct objects of the action\n    locations: List[str]  # Spatial references\n    qualifiers: Dict[str, str]  # Modifiers, colors, sizes, etc.",
    "chapter": 5,
    "section": "Syntactic and Semantic Parsing",
    "page": 9,
    "token_count": 94
  },
  {
    "chunk_id": "a2d80e05-9a20-41f2-945f-c99860f9b398",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Syntactic and Semantic Parsing\n\nclass LanguageParser:\n    \"\"\"\n    Parses natural language commands into structured representations\n    \"\"\"\n    def __init__(self):\n        # Load spaCy model for syntactic parsing\n        self.nlp = spacy.load(\"en_core_web_sm\")\n        \n        # Define common robot actions\n        self.robot_actions = {\n            'move', 'navigate', 'go', 'walk', 'drive',\n            'pick', 'grasp', 'take', 'lift', 'get',\n            'place', 'put', 'set', 'drop', 'release',\n            'open', 'close', 'push', 'pull',\n            'follow', 'wait', 'stop', 'start'\n        }\n    \n    def parse(self, command: str) -> ParsedCommand:\n        \"\"\"\n        Parse a natural language command\n        \"\"\"\n        doc = self.nlp(command)\n        \n        action = None\n        objects = []\n        locations = []\n        qualifiers = {}\n        \n        # Extract main action\n        for token in doc:\n            if token.pos_ == \"VERB\" and token.lemma_ in self.robot_actions:\n                action = token.lemma_\n                break\n        \n        # Extract objects, locations, and qualifiers\n        for token in doc:\n            if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Nouns could be objects or locations\n                if self._is_object_noun(token, doc):\n                    objects.append(token.text.lower())\n                elif self._is_location_noun(token, doc):\n                    locations.append(token.text.lower())\n            \n            elif token.pos_ == \"ADJ\":  # Adjectives are typically qualifiers\n                # Find what the adjective modifies\n                head = token.head\n                if head.pos_ in [\"NOUN\", \"PROPN\"]:\n                    qualifiers[head.text.lower()] = token.text.lower()\n        \n        return ParsedCommand(\n            action=action,\n            objects=objects,\n            locations=locations,\n            qualifiers=qualifiers\n        )\n    \n    def _is_object_noun(self, token, doc):\n        \"\"\"\n        Determine if a noun token represents an object to be acted upon\n        \"\"\"\n        # Simple heuristic: if it's a direct object of the main verb\n        for child in token.head.children:\n            if child == token and child.dep_ == \"dobj\":\n                return True\n        return False\n    \n    def _is_location_noun(self, token, doc):\n        \"\"\"\n        Determine if a noun token represents a spatial reference\n        \"\"\"\n        # Simple heuristic: if it's in a prepositional phrase\n        for child in token.head.children:\n            if child == token and child.dep_ in [\"pobj\", \"advmod\"]:\n                return True\n        return False",
    "chapter": 5,
    "section": "Syntactic and Semantic Parsing",
    "page": 10,
    "token_count": 564
  },
  {
    "chunk_id": "79b6f690-7ffe-47d9-b18e-6150c46a26cd",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Syntactic and Semantic Parsing\n\n# Example usage\nparser = LanguageParser()\ncommand = \"Pick up the red cup from the table\"\nparsed = parser.parse(command)\nprint(f\"Action: {parsed.action}\")\nprint(f\"Objects: {parsed.objects}\")\nprint(f\"Qualifiers: {parsed.qualifiers}\")\n```",
    "chapter": 5,
    "section": "Syntactic and Semantic Parsing",
    "page": 11,
    "token_count": 65
  },
  {
    "chunk_id": "1710f094-cad9-430e-9e0b-3d1aa7c878e5",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Intent Recognition and Slot Filling\n\nFor more robust command interpretation, intent recognition with slot filling can be more effective:",
    "chapter": 5,
    "section": "Intent Recognition and Slot Filling",
    "page": 12,
    "token_count": 24
  },
  {
    "chunk_id": "26599abc-55fb-4d9f-9d35-766a849f3c38",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Intent Recognition and Slot Filling\n\n```python\nclass IntentSlotParser:\n    \"\"\"\n    Intent recognition with slot filling for robotic commands\n    \"\"\"\n    def __init__(self):\n        # Define command intents and their expected slots\n        self.intents = {\n            'navigation': {\n                'slots': ['destination', 'path_constraints']\n            },\n            'manipulation': {\n                'slots': ['action', 'object', 'object_attributes', 'destination']\n            },\n            'monitoring': {\n                'slots': ['condition', 'action', 'target']\n            }\n        }\n        \n        # For this example, we'll use rule-based intent recognition\n        # In practice, this would use ML models\n        self.intent_keywords = {\n            'navigation': ['go', 'move', 'navigate', 'walk', 'drive', 'to', 'toward'],\n            'manipulation': ['pick', 'grasp', 'take', 'lift', 'put', 'place', 'open', 'close'],\n            'monitoring': ['wait', 'until', 'stop', 'when', 'if', 'while']\n        }\n    \n    def extract_intent_and_slots(self, command: str) -> Dict:\n        \"\"\"\n        Extract intent and fill slots from command\n        \"\"\"\n        doc = spacy.load(\"en_core_web_sm\")(command)\n        \n        # Determine intent\n        intent = self._classify_intent(command, doc)\n        \n        # Extract slots based on intent\n        slots = self._extract_slots(intent, doc)\n        \n        return {\n            'intent': intent,\n            'slots': slots,\n            'confidence': 0.9  # Simplified confidence\n        }\n    \n    def _classify_intent(self, command: str, doc) -> str:\n        \"\"\"\n        Classify the intent of the command\n        \"\"\"\n        command_lower = command.lower()\n        \n        max_matches = 0\n        best_intent = 'unknown'\n        \n        for intent, keywords in self.intent_keywords.items():\n            matches = sum(1 for keyword in keywords if keyword in command_lower)\n            if matches > max_matches:\n                max_matches = matches\n                best_intent = intent\n        \n        return best_intent\n    \n    def _extract_slots(self, intent: str, doc) -> Dict:\n        \"\"\"\n        Extract slots for the given intent\n        \"\"\"\n        if intent not in self.intents:\n            return {}\n        \n        slots = {}\n        \n        for token in doc:\n            # Extract object information (nouns + adjectives)\n            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n                # Check if it's a direct object\n                if any(child.dep_ == \"dobj\" for child in token.head.children):\n                    if 'object' in self.intents[intent]['slots']:\n                        # Look for adjectives that modify this noun\n                        attributes = []\n                        for child in token.children:\n                            if child.pos_ == \"ADJ\":\n                                attributes.append(child.text)\n                        \n                        slots['object'] = {\n                            'name': token.text,\n                            'attributes': attributes\n                        }\n            \n            # Extract location information\n            elif token.pos_ == \"ADP\":  # Preposition\n                # Get the object of the preposition\n                for child in token.children:\n                    if child.dep_ == \"pobj\":\n                        if 'destination' in self.intents[intent]['slots']:\n                            slots['destination'] = child.text\n        \n        return slots",
    "chapter": 5,
    "section": "Intent Recognition and Slot Filling",
    "page": 13,
    "token_count": 706
  },
  {
    "chunk_id": "746253bc-8ae6-416d-9cb7-07234a3c5348",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Intent Recognition and Slot Filling\n\n# Example usage\nintent_parser = IntentSlotParser()\ncommand = \"Pick up the red cup from the table\"\nresult = intent_parser.extract_intent_and_slots(command)\nprint(f\"Intent: {result['intent']}\")\nprint(f\"Slots: {result['slots']}\")\n```",
    "chapter": 5,
    "section": "Intent Recognition and Slot Filling",
    "page": 14,
    "token_count": 64
  },
  {
    "chunk_id": "e74ca16f-1438-4567-911d-10fcb07dc8e1",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Grounding Linguistic Concepts in Perception",
    "chapter": 5,
    "section": "Grounding Linguistic Concepts in Perception",
    "page": 15,
    "token_count": 8
  },
  {
    "chunk_id": "ed118d94-da63-42fe-a9d2-b622294928a1",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Spatial Language Grounding\n\nOne of the most challenging aspects of robotic NLU is grounding spatial language in the robot's perception system:\n\n```python\nimport numpy as np\nfrom typing import Dict, List, Tuple",
    "chapter": 5,
    "section": "Spatial Language Grounding",
    "page": 16,
    "token_count": 43
  },
  {
    "chunk_id": "5d641d91-b28f-48bd-a9c7-a4b3e6b135ac",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Spatial Language Grounding\n\nclass SpatialGrounding:\n    \"\"\"\n    Grounds spatial language concepts in robot perception\n    \"\"\"\n    def __init__(self, robot_position, camera_matrix, robot_workspace):\n        self.robot_position = robot_position\n        self.camera_matrix = camera_matrix\n        self.robot_workspace = robot_workspace\n        \n        # Define spatial relations\n        self.spatial_relations = ['near', 'far', 'left', 'right', 'front', 'back', \n                                 'above', 'below', 'between', 'next_to', 'behind']\n    \n    def ground_spatial_reference(self, spatial_phrase: str, objects_in_view: Dict) -> Dict:\n        \"\"\"\n        Ground a spatial reference to objects in the robot's environment\n        \"\"\"\n        if 'left' in spatial_phrase:\n            return self._find_leftmost_object(objects_in_view)\n        elif 'right' in spatial_phrase:\n            return self._find_rightmost_object(objects_in_view)\n        elif 'front' in spatial_phrase:\n            return self._find_frontmost_object(objects_in_view)\n        elif 'near' in spatial_phrase:\n            return self._find_nearest_object(objects_in_view)\n        else:\n            # Default: return the closest object\n            return self._find_nearest_object(objects_in_view)\n    \n    def _find_leftmost_object(self, objects_in_view: Dict) -> Dict:\n        \"\"\"\n        Find the object that appears most to the left in the camera image\n        \"\"\"\n        leftmost = None\n        min_x = float('inf')\n        \n        for obj_id, obj_info in objects_in_view.items():\n            # Get 2D bounding box center x-coordinate\n            center_x = (obj_info['bbox'][0] + obj_info['bbox'][2]) / 2\n            if center_x < min_x:\n                min_x = center_x\n                leftmost = obj_info\n        \n        return leftmost\n    \n    def _find_nearest_object(self, objects_in_view: Dict) -> Dict:\n        \"\"\"\n        Find the object closest to the robot\n        \"\"\"\n        nearest = None\n        min_distance = float('inf')\n        \n        for obj_id, obj_info in objects_in_view.items():\n            if 'distance' in obj_info:\n                distance = obj_info['distance']\n                if distance < min_distance:\n                    min_distance = distance\n                    nearest = obj_info\n        \n        return nearest\n    \n    def resolve_relative_position(self, reference_object: Dict, \n                                 spatial_relation: str, \n                                 target_objects: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Find objects that satisfy the spatial relation with respect to reference object\n        \"\"\"\n        resolved_objects = []\n        \n        for obj in target_objects:\n            if self._satisfies_spatial_relation(reference_object, obj, spatial_relation):\n                resolved_objects.append(obj)\n        \n        return resolved_objects\n    \n    def _satisfies_spatial_relation(self, ref_obj: Dict, target_obj: Dict, \n                                   relation: str) -> bool:\n        \"\"\"\n        Check if target object satisfies spatial relation with reference object\n        \"\"\"\n        if 'position' not in ref_obj or 'position' not in target_obj:\n            return False\n        \n        ref_pos = np.array(ref_obj['position'])\n        target_pos = np.array(target_obj['position'])\n        \n        # Calculate relative position\n        rel_pos = target_pos - ref_pos\n        distance = np.linalg.norm(rel_pos)\n        \n        if relation == 'left':\n            return rel_pos[0] < 0  # Assuming robot coordinate system\n        elif relation == 'right':\n            return rel_pos[0] > 0\n        elif relation == 'front':\n            return rel_pos[1] > 0\n        elif relation == 'back':\n            return rel_pos[1] < 0\n        elif relation == 'near':\n            return distance < 1.0  # Within 1 meter threshold\n        elif relation == 'far':\n            return distance > 3.0  # Beyond 3 meters threshold\n        else:\n            return True  # Default to true for unknown relations",
    "chapter": 5,
    "section": "Spatial Language Grounding",
    "page": 17,
    "token_count": 837
  },
  {
    "chunk_id": "7f70f491-dad4-4efe-acfb-deda11e7e176",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Spatial Language Grounding\n\n# Integration example\nclass LanguageGroundingSystem:\n    \"\"\"\n    Integrates language parsing with spatial grounding\n    \"\"\"\n    def __init__(self, language_parser, spatial_grounding):\n        self.language_parser = language_parser\n        self.spatial_grounding = spatial_grounding\n    \n    def ground_command(self, command: str, objects_in_view: Dict) -> Dict:\n        \"\"\"\n        Ground a natural language command in the robot's environment\n        \"\"\"\n        # Parse the command\n        parsed_command = self.language_parser.parse(command)\n        \n        # Ground spatial references\n        grounded_objects = []\n        for obj_text in parsed_command.objects:\n            # First try to ground using spatial relations\n            if any(rel in command for rel in ['left', 'right', 'front', 'back', 'near']):\n                grounded_obj = self.spatial_grounding.ground_spatial_reference(\n                    command, objects_in_view\n                )\n            else:\n                # Try to match by object type and attributes\n                grounded_obj = self._match_object_type(\n                    obj_text, parsed_command.qualifiers, objects_in_view\n                )\n            \n            if grounded_obj:\n                grounded_objects.append(grounded_obj)\n        \n        return {\n            'action': parsed_command.action,\n            'grounded_objects': grounded_objects,\n            'locations': parsed_command.locations,\n            'qualifiers': parsed_command.qualifiers\n        }\n    \n    def _match_object_type(self, obj_text: str, qualifiers: Dict, \n                          objects_in_view: Dict) -> Dict:\n        \"\"\"\n        Match a linguistic object description to a perceived object\n        \"\"\"\n        for obj_id, obj_info in objects_in_view.items():\n            if obj_text.lower() in obj_info.get('class', '').lower():\n                # Check if attributes match\n                matches_attributes = all(\n                    attr.lower() in obj_info.get('attributes', []) \n                    for attr in qualifiers.values()\n                )\n                \n                if matches_attributes:\n                    return obj_info\n        \n        # If no perfect match, return the most similar\n        return next(iter(objects_in_view.values()), None)\n```",
    "chapter": 5,
    "section": "Spatial Language Grounding",
    "page": 18,
    "token_count": 428
  },
  {
    "chunk_id": "a075f07b-0469-49eb-9bdb-581cb4b2bbaf",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Object Attribute Grounding\n\nLinguistic attributes (colors, shapes, sizes) must be grounded in visual perception:",
    "chapter": 5,
    "section": "Object Attribute Grounding",
    "page": 19,
    "token_count": 24
  },
  {
    "chunk_id": "5a113509-5bf0-4b88-8a1d-13f67f07808e",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Object Attribute Grounding\n\n```python\nclass AttributeGrounding:\n    \"\"\"\n    Ground linguistic attributes in visual perception\n    \"\"\"\n    def __init__(self):\n        # Define color categories and their visual ranges\n        self.color_ranges = {\n            'red': ([0, 50, 50], [10, 255, 255]),\n            'green': ([50, 50, 50], [70, 255, 255]), \n            'blue': ([100, 50, 50], [130, 255, 255]),\n            'yellow': ([20, 100, 100], [30, 255, 255]),\n            'black': ([0, 0, 0], [180, 255, 30]),\n            'white': ([0, 0, 225], [180, 30, 255])\n        }\n        \n        # Size descriptors\n        self.size_ranges = {\n            'small': (0, 0.1),  # meters\n            'medium': (0.1, 0.5),\n            'large': (0.5, float('inf'))\n        }\n    \n    def ground_color_attribute(self, color: str, image_region: np.ndarray) -> bool:\n        \"\"\"\n        Check if image region matches the linguistic color description\n        \"\"\"\n        if color not in self.color_ranges:\n            return False\n        \n        lower_range, upper_range = self.color_ranges[color]\n        \n        # Convert to HSV for better color detection\n        hsv_region = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)\n        \n        # Create mask for color range\n        mask = cv2.inRange(hsv_region, \n                          np.array(lower_range), \n                          np.array(upper_range))\n        \n        # Calculate percentage of pixels that match the color\n        color_percentage = np.sum(mask > 0) / (image_region.shape[0] * image_region.shape[1])\n        \n        # Return True if enough pixels match (threshold could be tuned)\n        return color_percentage > 0.3\n    \n    def ground_size_attribute(self, size_descriptor: str, object_dimensions: Tuple) -> bool:\n        \"\"\"\n        Check if object dimensions match the linguistic size description\n        \"\"\"\n        if size_descriptor not in self.size_ranges:\n            return False\n        \n        min_size, max_size = self.size_ranges[size_descriptor]\n        largest_dimension = max(object_dimensions)\n        \n        return min_size <= largest_dimension < max_size\n    \n    def ground_shape_attribute(self, shape_descriptor: str, contour: np.ndarray) -> bool:\n        \"\"\"\n        Check if object shape matches linguistic description\n        \"\"\"\n        # Calculate shape features\n        area = cv2.contourArea(contour)\n        if area == 0:\n            return False\n        \n        # Calculate circularity, rectangularity, etc.\n        perimeter = cv2.arcLength(contour, True)\n        circularity = 4 * np.pi * area / (perimeter * perimeter)\n        \n        if shape_descriptor == 'round' or shape_descriptor == 'circular':\n            return circularity > 0.7\n        elif shape_descriptor == 'square':\n            # More complex shape analysis needed\n            return self._is_squareish(contour)\n        \n        return True  # Default to true for other descriptors\n    \n    def _is_squareish(self, contour: np.ndarray) -> bool:\n        \"\"\"\n        Check if contour is approximately square\n        \"\"\"\n        # Approximate contour to polygon\n        epsilon = 0.04 * cv2.arcLength(contour, True)\n        approx = cv2.approxPolyDP(contour, epsilon, True)\n        \n        # Check if it has 4 sides and is roughly square\n        if len(approx) == 4:\n            # Calculate aspect ratio\n            x, y, w, h = cv2.boundingRect(approx)\n            aspect_ratio = float(w) / h\n            return 0.8 <= aspect_ratio <= 1.2\n        \n        return False\n```",
    "chapter": 5,
    "section": "Object Attribute Grounding",
    "page": 20,
    "token_count": 825
  },
  {
    "chunk_id": "965fd3e4-e6c7-4784-ad09-f37288cc67c2",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Task Decomposition and Planning",
    "chapter": 5,
    "section": "Task Decomposition and Planning",
    "page": 21,
    "token_count": 6
  },
  {
    "chunk_id": "ed320944-a731-4662-8dd4-de028797db74",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Breaking Down Complex Commands\n\nNatural language commands often represent complex tasks that need to be decomposed:",
    "chapter": 5,
    "section": "Breaking Down Complex Commands",
    "page": 22,
    "token_count": 20
  },
  {
    "chunk_id": "f6c31ddb-d8af-43f7-82e8-c82d9a731e57",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Breaking Down Complex Commands\n\n```python\nclass TaskDecomposer:\n    \"\"\"\n    Decomposes complex natural language commands into executable subtasks\n    \"\"\"\n    def __init__(self, robot_capabilities):\n        self.robot_capabilities = robot_capabilities\n        \n        # Define task templates and their decompositions\n        self.task_templates = {\n            'set_table': [\n                'navigate_to_kitchen',\n                'identify_table',\n                'pick_utensils',\n                'place_utensils'\n            ],\n            'clean_room': [\n                'scan_room_for_litter',\n                'plan_cleaning_path', \n                'collect_litter_one_by_one',\n                'empty_trash_bin'\n            ],\n            'serve_drink': [\n                'navigate_to_kitchen',\n                'identify_beverage',\n                'grasp_beverage',\n                'navigate_to_person',\n                'offer_beverage'\n            ]\n        }\n        \n        # Define primitive actions\n        self.primitive_actions = [\n            'move_to', 'grasp_object', 'place_object', 'open_gripper', \n            'close_gripper', 'navigate', 'turn', 'stop', 'wait'\n        ]\n    \n    def decompose_command(self, command: str, command_metadata: Dict) -> List[Dict]:\n        \"\"\"\n        Decompose a command into executable subtasks\n        \"\"\"\n        # First, try to match to known templates\n        for template_name, steps in self.task_templates.items():\n            if self._matches_template(command, template_name):\n                return self._expand_template_steps(steps, command_metadata)\n        \n        # If no template matches, do semantic decomposition\n        return self._semantic_decomposition(command, command_metadata)\n    \n    def _matches_template(self, command: str, template_name: str) -> bool:\n        \"\"\"\n        Check if command matches a known task template\n        \"\"\"\n        command_lower = command.lower()\n        template_keywords = {\n            'set_table': ['set', 'table', 'dinner', 'meal'],\n            'clean_room': ['clean', 'room', 'tidy', 'organize'],\n            'serve_drink': ['serve', 'drink', 'beverage', 'water', 'coffee']\n        }\n        \n        if template_name in template_keywords:\n            return any(keyword in command_lower for keyword in template_keywords[template_name])\n        \n        return False\n    \n    def _expand_template_steps(self, steps: List[str], metadata: Dict) -> List[Dict]:\n        \"\"\"\n        Expand template steps with specific details from command\n        \"\"\"\n        detailed_steps = []\n        \n        for step in steps:\n            if step == 'pick_utensils':\n                # Specify which utensils to pick based on metadata\n                detailed_steps.append({\n                    'action': 'pick_specific_objects',\n                    'object_types': self._get_utensil_types(metadata),\n                    'location': 'kitchen_counter'\n                })\n            elif step == 'identify_table':\n                detailed_steps.append({\n                    'action': 'find_object_by_type', \n                    'object_type': 'table',\n                    'location': 'dining_room'\n                })\n            else:\n                detailed_steps.append({'action': step})\n        \n        return detailed_steps\n    \n    def _semantic_decomposition(self, command: str, metadata: Dict) -> List[Dict]:\n        \"\"\"\n        Decompose command based on semantic analysis\n        \"\"\"\n        # Parse the command to identify main action and objects\n        action = metadata.get('action')\n        objects = metadata.get('grounded_objects', [])\n        \n        subtasks = []\n        \n        if action in ['pick', 'grasp', 'take']:\n            # Add navigation to object\n            if objects:\n                subtasks.append({\n                    'action': 'navigate_to_object',\n                    'object_id': objects[0]['id']\n                })\n            \n            # Add grasping action\n            subtasks.append({\n                'action': 'grasp_object',\n                'object_id': objects[0]['id'] if objects else None\n            })\n            \n            # Add navigation to destination if specified\n            if 'destination' in metadata:\n                subtasks.append({\n                    'action': 'navigate_to_location',\n                    'location': metadata['destination']\n                })\n                \n                # Add placement action\n                subtasks.append({\n                    'action': 'place_object',\n                    'location': metadata['destination']\n                })\n        \n        return subtasks\n    \n    def _get_utensil_types(self, metadata: Dict) -> List[str]:\n        \"\"\"\n        Extract utensil types from command metadata\n        \"\"\"\n        # This would analyze command for specific utensil mentions\n        utensils = []\n        if 'fork' in str(metadata).lower():\n            utensils.append('fork')\n        if 'knife' in str(metadata).lower():\n            utensils.append('knife')\n        if 'spoon' in str(metadata).lower():\n            utensils.append('spoon')\n        if 'plate' in str(metadata).lower():\n            utensils.append('plate')\n        \n        return utensils if utensils else ['fork', 'knife', 'spoon', 'plate']  # Default",
    "chapter": 5,
    "section": "Breaking Down Complex Commands",
    "page": 23,
    "token_count": 1041
  },
  {
    "chunk_id": "63d23234-ca6b-4a5e-b2b2-7b7aa76af247",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Breaking Down Complex Commands\n\n# Example usage\nrobot_caps = {'max_reach': 1.5, 'gripper_types': ['parallel', 'suction']}\ntask_decomposer = TaskDecomposer(robot_caps)\n\ncommand = \"Set the table for dinner with four people\"\nobjects_metadata = {\n    'action': 'set',\n    'object_types': ['utensils'],\n    'destination': 'dining_table',\n    'quantity': 4\n}\n\ndecomposed_tasks = task_decomposer.decompose_command(command, objects_metadata)\nprint(\"Decomposed Tasks:\")\nfor i, task in enumerate(decomposed_tasks):\n    print(f\"  {i+1}. {task}\")\n```",
    "chapter": 5,
    "section": "Breaking Down Complex Commands",
    "page": 24,
    "token_count": 142
  },
  {
    "chunk_id": "1b9b8618-9a39-4786-8008-69c28d22724e",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Handling Ambiguity and Uncertainty",
    "chapter": 5,
    "section": "Handling Ambiguity and Uncertainty",
    "page": 25,
    "token_count": 8
  },
  {
    "chunk_id": "57a80ef4-2391-47fa-8ca2-69ed37e3768c",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Clarification Requests\n\nWhen commands are ambiguous, robots should request clarification:",
    "chapter": 5,
    "section": "Clarification Requests",
    "page": 26,
    "token_count": 15
  },
  {
    "chunk_id": "7195be58-4cb1-4d6d-a426-7d2da7241dd2",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Clarification Requests\n\n```python\nclass AmbiguityResolver:\n    \"\"\"\n    Handles ambiguous commands by requesting clarification\n    \"\"\"\n    def __init__(self):\n        self.ambiguity_patterns = [\n            r\"the\\s+\\w+\",  # \"the cup\" - which cup?\n            r\"it\",  # \"move it\" - what is \"it\"?\n            r\"there\",  # \"put it there\" - where is \"there\"?\n            r\"this\", r\"that\"  # Demonstrative pronouns\n        ]\n    \n    def detect_ambiguity(self, command: str, environment_objects: List[Dict]) -> Dict:\n        \"\"\"\n        Detect different types of ambiguity in a command\n        \"\"\"\n        ambiguity_issues = {\n            'referential_ambiguity': [],\n            'spatial_ambiguity': [],\n            'action_ambiguity': []\n        }\n        \n        # Check for referential ambiguity (vague object references)\n        if 'the ' in command.lower():\n            # Count objects of similar type\n            object_types_mentioned = self._extract_object_types(command)\n            for obj_type in object_types_mentioned:\n                count = sum(1 for obj in environment_objects \n                          if obj_type.lower() in obj.get('class', '').lower())\n                if count > 1:\n                    ambiguity_issues['referential_ambiguity'].append({\n                        'type': obj_type,\n                        'count': count,\n                        'issue': f'Multiple {obj_type}s detected, unsure which to use'\n                    })\n        \n        # Check for spatial ambiguity\n        if any(word in command.lower() for word in ['there', 'here', 'that', 'this']):\n            ambiguity_issues['spatial_ambiguity'].append({\n                'word': 'demonstrative',\n                'issue': 'Unclear spatial reference - needs pointing or description'\n            })\n        \n        return ambiguity_issues\n    \n    def generate_clarification_request(self, ambiguity_issues: Dict, command: str) -> str:\n        \"\"\"\n        Generate a natural language clarification request\n        \"\"\"\n        questions = []\n        \n        for issue_type, issues in ambiguity_issues.items():\n            if issues:\n                if issue_type == 'referential_ambiguity':\n                    for issue in issues:\n                        questions.append(\n                            f\"I see {issue['count']} {issue['type']}s. \"\n                            f\"Could you point to or describe which one you mean?\"\n                        )\n                elif issue_type == 'spatial_ambiguity':\n                    questions.append(\n                        f\"I'm not sure where you mean by '{list(issues[0].keys())[0]}'. \"\n                        f\"Could you point to the location or describe it?\"\n                    )\n        \n        if questions:\n            return \" and \".join(questions)\n        else:\n            return \"I understood your command. Proceeding with execution.\"\n    \n    def _extract_object_types(self, command: str) -> List[str]:\n        \"\"\"\n        Extract object types mentioned in command\n        \"\"\"\n        # Simple extraction - in practice, use NLP parsing\n        doc = spacy.load(\"en_core_web_sm\")(command)\n        object_types = []\n        \n        for token in doc:\n            if token.pos_ in [\"NOUN\", \"PROPN\"]:\n                object_types.append(token.text)\n        \n        return object_types",
    "chapter": 5,
    "section": "Clarification Requests",
    "page": 27,
    "token_count": 688
  },
  {
    "chunk_id": "0a27efb1-27be-41c7-908f-730bb271c4ff",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Clarification Requests\n\n# Integration example\nclass RobustLanguageUnderstanding:\n    \"\"\"\n    Robust NLU system that handles ambiguity\n    \"\"\"\n    def __init__(self, language_parser, spatial_grounding, task_decomposer):\n        self.language_parser = language_parser\n        self.spatial_grounding = spatial_grounding\n        self.task_decomposer = task_decomposer\n        self.ambiguity_resolver = AmbiguityResolver()\n        \n        self.understanding_confidence_threshold = 0.7\n    \n    def process_command(self, command: str, environment_state: Dict) -> Dict:\n        \"\"\"\n        Process command with ambiguity handling\n        \"\"\"\n        # Parse the command\n        parsed_command = self.language_parser.parse(command)\n        \n        # Check for ambiguity\n        ambiguity_issues = self.ambiguity_resolver.detect_ambiguity(\n            command, \n            environment_state.get('objects', [])\n        )\n        \n        if any(issues for issues in ambiguity_issues.values()):\n            # Generate clarification request\n            clarification_request = self.ambiguity_resolver.generate_clarification_request(\n                ambiguity_issues, \n                command\n            )\n            \n            return {\n                'status': 'needs_clarification',\n                'clarification_request': clarification_request,\n                'parsed_command': parsed_command\n            }\n        \n        # If no major ambiguities, proceed with interpretation\n        grounded_command = self.spatial_grounding.ground_command(\n            command, \n            environment_state['objects']\n        )\n        \n        # Decompose into tasks\n        subtasks = self.task_decomposer.decompose_command(\n            command, \n            grounded_command\n        )\n        \n        return {\n            'status': 'ready_to_execute',\n            'subtasks': subtasks,\n            'grounded_command': grounded_command\n        }",
    "chapter": 5,
    "section": "Clarification Requests",
    "page": 28,
    "token_count": 362
  },
  {
    "chunk_id": "12b6cdc6-444f-4bab-be0b-a05397e6afd3",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Clarification Requests\n\n# Example usage\nnlu_system = RobustLanguageUnderstanding(\n    LanguageParser(), \n    SpatialGrounding(None, None, None), \n    TaskDecomposer({})\n)\n\ncommand = \"Pick up the red cup\"\nenvironment = {\n    'objects': [\n        {'id': 'cup1', 'class': 'cup', 'color': 'red', 'position': [1.0, 0.5, 0.0]},\n        {'id': 'cup2', 'class': 'cup', 'color': 'blue', 'position': [1.2, 0.5, 0.0]},\n        {'id': 'book', 'class': 'book', 'color': 'black', 'position': [0.8, 0.3, 0.0]}\n    ]\n}\n\nresult = nlu_system.process_command(command, environment)\nprint(f\"Processing result: {result['status']}\")\nif result['status'] == 'needs_clarification':\n    print(f\"Clarification needed: {result['clarification_request']}\")\nelse:\n    print(f\"Subtasks: {result['subtasks']}\")\n```",
    "chapter": 5,
    "section": "Clarification Requests",
    "page": 29,
    "token_count": 242
  },
  {
    "chunk_id": "9f7cf596-a1ac-46b0-890e-bf96dec109fc",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Large Language Model Integration",
    "chapter": 5,
    "section": "Large Language Model Integration",
    "page": 30,
    "token_count": 5
  },
  {
    "chunk_id": "97c8669f-8338-4d06-a1a4-94cec00c6e46",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Using LLMs for Enhanced Understanding\n\nLarge Language Models can enhance robotic NLU by providing contextual understanding and commonsense reasoning:\n\n```python\nimport openai\nimport json\nfrom typing import Optional",
    "chapter": 5,
    "section": "Using LLMs for Enhanced Understanding",
    "page": 31,
    "token_count": 40
  },
  {
    "chunk_id": "33f5985e-0517-4d6b-8779-15c1535b3bbe",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Using LLMs for Enhanced Understanding\n\nclass LLMEnhancedNLU:\n    \"\"\"\n    Natural Language Understanding enhanced with Large Language Models\n    \"\"\"\n    def __init__(self, api_key: Optional[str] = None):\n        if api_key:\n            openai.api_key = api_key\n        self.use_llm = api_key is not None\n        \n        # Robot-specific instruction context\n        self.system_prompt = \"\"\"\n        You are a language understanding system for a robot. Your job is to:\n        1. Interpret natural language commands for a mobile manipulator robot\n        2. Ground language in visual perception when possible\n        3. Decompose complex commands into simple actions\n        4. Identify when clarification is needed\n        5. Consider safety and feasibility of requested actions\n        \n        Commands will be in natural language. Respond in JSON format with:\n        {\n          \"action_sequence\": [...],\n          \"objects_to_interact\": [...],\n          \"navigation_targets\": [...],\n          \"safety_considerations\": [...],\n          \"clarification_needed\": \"explanation if unclear\"\n        }\n        \"\"\"\n    \n    def enhanced_command_interpretation(self, command: str, \n                                     environment_context: str = \"\") -> Dict:\n        \"\"\"\n        Use LLM to enhance command interpretation\n        \"\"\"\n        if not self.use_llm:\n            # Fallback to rule-based system\n            return self._rule_based_interpretation(command, environment_context)\n        \n        user_message = f\"\"\"\n        Command: \"{command}\"\n        \n        Environment context: {environment_context}\n        \n        Please interpret this command for the robot.\n        \"\"\"\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",  # or gpt-4 for better performance\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": user_message}\n                ],\n                temperature=0.1,  # Low temperature for more consistent outputs\n                max_tokens=500\n            )\n            \n            # Parse JSON response\n            response_text = response.choices[0].message['content'].strip()\n            \n            # Extract JSON part in case response includes other text\n            json_start = response_text.find('{')\n            json_end = response_text.rfind('}') + 1\n            json_str = response_text[json_start:json_end]\n            \n            return json.loads(json_str)\n            \n        except Exception as e:\n            print(f\"LLM call failed: {e}\")\n            # Fallback to rule-based system\n            return self._rule_based_interpretation(command, environment_context)\n    \n    def _rule_based_interpretation(self, command: str, \n                                  environment_context: str) -> Dict:\n        \"\"\"\n        Fallback rule-based interpretation\n        \"\"\"\n        # Parse command using local NLP components\n        parser = LanguageParser()\n        parsed = parser.parse(command)\n        \n        return {\n            \"action_sequence\": [parsed.action] if parsed.action else [],\n            \"objects_to_interact\": parsed.objects,\n            \"navigation_targets\": parsed.locations,\n            \"safety_considerations\": [],\n            \"clarification_needed\": \"\",\n            \"confidence\": 0.5  # Lower confidence without LLM\n        }\n    \n    def commonsense_reasoning(self, action_request: Dict) -> Dict:\n        \"\"\"\n        Apply commonsense reasoning to validate action requests\n        \"\"\"\n        if not self.use_llm:\n            # Simple rule-based checks\n            return self._simple_safety_check(action_request)\n        \n        reasoning_prompt = f\"\"\"\n        A robot has been asked to perform: {action_request}\n        \n        Apply commonsense reasoning to identify potential issues:\n        1. Is this physically possible for the robot?\n        2. Are there safety concerns?\n        3. Does this make logical sense in the context?\n        4. What constraints or considerations should be applied?\n        \n        Respond in JSON with your analysis.\n        \"\"\"\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a commonsense reasoning system for robotics. Identify potential issues with robot action requests.\"},\n                    {\"role\": \"user\", \"content\": reasoning_prompt}\n                ],\n                temperature=0.1\n            )\n            \n            response_text = response.choices[0].message['content'].strip()\n            json_start = response_text.find('{')\n            json_end = response_text.rfind('}') + 1\n            json_str = response_text[json_start:json_end]\n            \n            return json.loads(json_str)\n            \n        except Exception as e:\n            print(f\"Commonsense reasoning failed: {e}\")\n            return {\"safety_issues\": [], \"feasibility\": \"unknown\"}\n    \n    def _simple_safety_check(self, action_request: Dict) -> Dict:\n        \"\"\"\n        Simple rule-based safety checking\n        \"\"\"\n        issues = []\n        \n        # Check for obviously dangerous requests\n        if any(dangerous in str(action_request).lower() \n               for dangerous in ['harm', 'injure', 'damage', 'fire', 'break']):\n            issues.append(\"Potentially dangerous action detected\")\n        \n        return {\n            \"safety_issues\": issues,\n            \"feasibility\": \"unknown\" if issues else \"possible\"\n        }",
    "chapter": 5,
    "section": "Using LLMs for Enhanced Understanding",
    "page": 32,
    "token_count": 1110
  },
  {
    "chunk_id": "f7d052c1-be24-497d-b0d1-9300bb71e7c1",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Using LLMs for Enhanced Understanding\n\n# Example usage\nllm_nlu = LLMEnhancedNLU()  # Without API key, will use rule-based fallback\n\ncommand = \"Please bring me the cup from the kitchen table\"\nenv_context = \"\"\"\nEnvironment: Kitchen with one table containing a red cup, blue mug, and plates.\nRobot is in living room, needs to navigate to kitchen.\n\"\"\"\n\ninterpretation = llm_nlu.enhanced_command_interpretation(command, env_context)\nprint(\"LLM Interpretation:\")\nprint(json.dumps(interpretation, indent=2))\n\n# Apply commonsense reasoning\ncommon_sense_analysis = llm_nlu.commonsense_reasoning(interpretation)\nprint(\"\\nCommonsense Analysis:\")\nprint(json.dumps(common_sense_analysis, indent=2))\n```",
    "chapter": 5,
    "section": "Using LLMs for Enhanced Understanding",
    "page": 33,
    "token_count": 164
  },
  {
    "chunk_id": "8aeb6814-bdb4-402a-bfbc-0bba282f74f6",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Safety and Validation",
    "chapter": 5,
    "section": "Safety and Validation",
    "page": 34,
    "token_count": 4
  },
  {
    "chunk_id": "9674f404-6586-4e8d-a26b-9869cc7d5210",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Safety-Conscious Language Understanding\n\nRobotic NLU must incorporate safety considerations into command interpretation:",
    "chapter": 5,
    "section": "Safety-Conscious Language Understanding",
    "page": 35,
    "token_count": 19
  },
  {
    "chunk_id": "eb82ee85-7951-4764-9856-e3957d628fe8",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Safety-Conscious Language Understanding\n\n```python\nclass SafetyConsciousNLU:\n    \"\"\"\n    NLU system with integrated safety validation\n    \"\"\"\n    def __init__(self):\n        self.prohibited_actions = {\n            'dangerous_objects': ['knife', 'scissors', 'blade', 'sharp object'],\n            'forbidden_locations': ['stove', 'oven', 'hot surface', 'fire'],\n            'unsafe_movements': ['through', 'into', 'near']  # when combined with hazards\n        }\n        \n        self.safety_rules = [\n            {\n                'condition': lambda cmd: 'human' in cmd.lower() and 'push' in cmd.lower(),\n                'action': 'block',\n                'reason': 'Cannot push humans'\n            },\n            {\n                'condition': lambda cmd: any(word in cmd.lower() for word in ['break', 'crash', 'destroy']),\n                'action': 'block',\n                'reason': 'Cannot execute destructive commands'\n            },\n            {\n                'condition': lambda cmd: 'stairs' in cmd.lower() and 'fast' in cmd.lower(),\n                'action': 'warn',\n                'reason': 'Fast movement on stairs is dangerous'\n            }\n        ]\n    \n    def validate_command(self, command: str, parsed_command: Dict, \n                        environment_state: Dict) -> Dict:\n        \"\"\"\n        Validate command against safety rules\n        \"\"\"\n        validation_result = {\n            'is_safe': True,\n            'warnings': [],\n            'blocks': [],\n            'modified_command': parsed_command\n        }\n        \n        # Check against prohibited actions\n        for obj in parsed_command.get('objects', []):\n            if obj.lower() in self.prohibited_actions['dangerous_objects']:\n                validation_result['blocks'].append(\n                    f\"Cannot handle dangerous object: {obj}\"\n                )\n                validation_result['is_safe'] = False\n        \n        for location in parsed_command.get('locations', []):\n            if location.lower() in self.prohibited_actions['forbidden_locations']:\n                validation_result['blocks'].append(\n                    f\"Cannot approach forbidden location: {location}\"\n                )\n                validation_result['is_safe'] = False\n        \n        # Check safety rules\n        for rule in self.safety_rules:\n            if rule['condition'](command):\n                if rule['action'] == 'block':\n                    validation_result['blocks'].append(rule['reason'])\n                    validation_result['is_safe'] = False\n                elif rule['action'] == 'warn':\n                    validation_result['warnings'].append(rule['reason'])\n        \n        # Check for physical feasibility\n        feasibility_check = self._check_physical_feasibility(\n            parsed_command, environment_state\n        )\n        \n        if not feasibility_check['feasible']:\n            validation_result['blocks'].append(feasibility_check['reason'])\n            validation_result['is_safe'] = False\n        \n        return validation_result\n    \n    def _check_physical_feasibility(self, parsed_command: Dict, \n                                   environment_state: Dict) -> Dict:\n        \"\"\"\n        Check if the requested action is physically feasible\n        \"\"\"\n        # Check if robot can reach the target object\n        target_objects = parsed_command.get('objects', [])\n        \n        if target_objects and 'position' in environment_state:\n            robot_pos = environment_state['position']\n            robot_reach = environment_state.get('max_reach', 1.0)\n            \n            for obj_name in target_objects:\n                # Find object in environment\n                obj_info = next(\n                    (obj for obj in environment_state.get('objects', []) \n                     if obj_name.lower() in obj.get('class', '').lower()),\n                    None\n                )\n                \n                if obj_info and 'position' in obj_info:\n                    obj_pos = obj_info['position']\n                    distance = np.linalg.norm(\n                        np.array(robot_pos) - np.array(obj_pos)\n                    )\n                    \n                    if distance > robot_reach:\n                        return {\n                            'feasible': False,\n                            'reason': f\"Object '{obj_name}' is out of robot's reach\"\n                        }\n        \n        return {'feasible': True, 'reason': 'No feasibility issues detected'}",
    "chapter": 5,
    "section": "Safety-Conscious Language Understanding",
    "page": 36,
    "token_count": 845
  },
  {
    "chunk_id": "770d49dd-cb60-4fa1-a3be-92f43351a3f8",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Safety-Conscious Language Understanding\n\n# Example usage\nsafety_nlu = SafetyConsciousNLU()\n\ncommand = \"Pick up the knife from the counter\"\nparsed_command = {\n    'action': 'pick',\n    'objects': ['knife'],\n    'locations': ['counter']\n}\nenv_state = {\n    'position': [0, 0, 0],\n    'max_reach': 1.5,\n    'objects': [\n        {'class': 'knife', 'position': [1.0, 0.5, 0.0]},\n        {'class': 'cup', 'position': [0.8, 0.3, 0.0]}\n    ]\n}\n\nvalidation = safety_nlu.validate_command(command, parsed_command, env_state)\nprint(\"Safety Validation Result:\")\nprint(f\"Is Safe: {validation['is_safe']}\")\nprint(f\"Blocks: {validation['blocks']}\")\nprint(f\"Warnings: {validation['warnings']}\")\n```",
    "chapter": 5,
    "section": "Safety-Conscious Language Understanding",
    "page": 37,
    "token_count": 202
  },
  {
    "chunk_id": "c4ba384e-6e29-408e-a2c7-b679c649b9e7",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Diagrams",
    "chapter": 5,
    "section": "Diagrams",
    "page": 38,
    "token_count": 3
  },
  {
    "chunk_id": "3d03712b-53e8-440d-92db-a2f2827f3abb",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Natural Language Understanding Pipeline",
    "chapter": 5,
    "section": "Natural Language Understanding Pipeline",
    "page": 39,
    "token_count": 5
  },
  {
    "chunk_id": "e98fe1a5-61db-46f0-b7b8-c82d70de2f2c",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Natural Language Understanding Pipeline\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                 NLU PIPELINE FOR ROBOTICS                       │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐  │\n│  │  NATURAL        │───▶│  PARSING &    │───▶│  GROUNDING  │  │\n│  │  LANGUAGE       │    │  PARSING      │    │  IN PERCEPTION│  │\n│  │  COMMAND        │    │ • Syntax      │    │ • Object    │  │\n│  │  \"Pick red cup\" │    │ • Semantics   │    │   Recognition│  │\n│  │                 │    │ • Intent      │    │ • Spatial   │  │\n│  └─────────────────┘    │   Recognition │    │   Relations │  │\n│                         └─────────────────┘    └─────────────┘  │\n│                                  │                     │        │\n│                                  ▼                     ▼        │\n│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐  │\n│  │  TASK           │    │  AMBIGUITY     │    │  SAFETY     │  │\n│  │  DECOMPOSITION  │    │  RESOLUTION    │    │  VALIDATION │  │\n│  │ • Action        │    │ • Clarification │    │ • Physical  │  │\n│  │   Sequences     │    │   Requests    │    │   Feasibility│  │\n│  │ • Subtask       │    │ • Context     │    │ • Risk      │  │\n│  │   Planning      │    │   Resolution  │    │   Assessment│  │\n│  └─────────────────┘    └─────────────────┘    └─────────────┘  │\n│                                                                 │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                LLM INTEGRATION                          │   │\n│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────────┐ │   │\n│  │  │ Commonsense │  │ Contextual  │  │ Instruction     │ │   │\n│  │  │ Reasoning   │  │ Understanding│  │ Following       │ │   │\n│  │  │ • Feasibility│  │ • Task      │  │ • Complex       │ │   │\n│  │  │ • Safety    │  │   Context   │  │   Commands      │ │   │\n│  │  │ • Commonsense│ │ • Environment│ │ • Multi-step    │ │   │\n│  │  └─────────────┘  └─────────────┘  └─────────────────┘ │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 5,
    "section": "Natural Language Understanding Pipeline",
    "page": 40,
    "token_count": 712
  },
  {
    "chunk_id": "0a880a04-11cc-4fed-9893-09224f0937e8",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Language-to-Action Mapping Process\n\n```\nHuman Command        ──▶    Linguistic      ──▶   Perceptual     ──▶    Action\n\"Fetch red cup\"             Analysis                 Grounding              Execution\n     │                         │                       │                      │\n     ▼                         ▼                       ▼                      ▼\n\"Action: fetch,    →   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n Object: red cup\"    │ • Verb: fetch     │    │ • Identify red  │    │ • Navigate to   │\n                     │ • Object: cup     │    │   objects in    │    │   cup location  │\n                     │ • Attribute: red  │    │   environment   │    │ • Grasp cup     │\n                     └─────────────────┘    └─────────────────┘    │ • Return to user  │\n                                                 │                  └─────────────────┘\n                                                 ▼\n                                        ┌─────────────────┐\n                                        │ Object Matching │\n                                        │ • Color: red    │\n                                        │ • Shape: cup    │\n                                        │ • Size: small   │\n                                        └─────────────────┘\n```",
    "chapter": 5,
    "section": "Language-to-Action Mapping Process",
    "page": 41,
    "token_count": 273
  },
  {
    "chunk_id": "0334e77f-b4af-4843-8dbc-2958c8b94c2a",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Implement natural language parsing systems for robotic command interpretation\n2. Ground linguistic concepts in visual perception and spatial reasoning\n3. Decompose complex natural language commands into executable robot actions\n4. Design systems that detect and resolve ambiguous commands\n5. Integrate large language models to enhance linguistic understanding\n6. Implement safety validation in language understanding systems",
    "chapter": 5,
    "section": "Learning Outcomes",
    "page": 42,
    "token_count": 85
  },
  {
    "chunk_id": "69b3ed39-f3b0-4fef-9227-deb985ebc934",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Advanced Topics",
    "chapter": 5,
    "section": "Advanced Topics",
    "page": 43,
    "token_count": 3
  },
  {
    "chunk_id": "a287f4ba-2b21-416e-98af-798aaf374c77",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Multilingual Robot Interaction\n\n- Supporting multiple languages for international applications\n- Cross-lingual transfer of robotic capabilities\n- Cultural context awareness in language interpretation",
    "chapter": 5,
    "section": "Multilingual Robot Interaction",
    "page": 44,
    "token_count": 31
  },
  {
    "chunk_id": "4577ab7c-3976-4b13-ba21-ee5e1f20481d",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Interactive Learning\n\n- Learning new commands through human demonstration\n- Active learning for improving language understanding\n- Feedback integration from successful and failed attempts",
    "chapter": 5,
    "section": "Interactive Learning",
    "page": 45,
    "token_count": 28
  },
  {
    "chunk_id": "4c441138-476f-405f-8ae4-8efc4bac7ea4",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "### Context-Aware Understanding\n\n- Long-term memory for context across interactions\n- Learning from human preferences and habits\n- Adaptive language understanding based on user profiles",
    "chapter": 5,
    "section": "Context-Aware Understanding",
    "page": 46,
    "token_count": 31
  },
  {
    "chunk_id": "63543522-92e3-43a7-90bc-312456a157fd",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## Further Reading\n\n- \"Natural Language Processing for Robotics: A Review\" (AI Magazine, 2024) [1]\n- \"Grounded Language Understanding for Interactive Robotics\" (IJCAI, 2024) [2]\n- \"Large Language Models for Robotic Task Planning\" (Robotics: Science and Systems, 2024) [3]\n\n---",
    "chapter": 5,
    "section": "Further Reading",
    "page": 47,
    "token_count": 75
  },
  {
    "chunk_id": "5a66d129-3d7c-4f52-b144-b4c2dd1c2556",
    "doc_id": "c2d88fb1-6194-40e7-ba3c-842003ee3b53",
    "chunk_text": "## References\n\n[1] Chen, L., et al. \"Natural Language Processing for Robotics: Bridging Communication and Action.\" AI Magazine, vol. 45, no. 2, pp. 45-62, 2024.\n\n[2] Patel, R., et al. \"Grounded Language Understanding for Interactive Robotics.\" International Joint Conference on Artificial Intelligence, pp. 1780-1788, 2024.\n\n[3] Zhang, H., et al. \"Large Language Models as Commonsense Knowledge for Robotic Task Planning.\" Robotics: Science and Systems, 2024.",
    "chapter": 5,
    "section": "References",
    "page": 48,
    "token_count": 123
  },
  {
    "chunk_id": "84f9dc53-e8b8-44cd-ba5d-3438bb5d8a4f",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "# Section 4: Example Implementation with ROS 2 + Isaac",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "0c910158-2673-457d-ba55-34a738db6f37",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Setting Up the Combined Environment\n\nIntegrating ROS 2 with NVIDIA Isaac provides a powerful combination for multimodal AI and robotics applications. ROS 2's distributed messaging architecture combined with Isaac's GPU-accelerated perception and simulation capabilities creates a robust platform for vision-language-action systems.",
    "chapter": 5,
    "section": "Setting Up the Combined Environment",
    "page": 2,
    "token_count": 58
  },
  {
    "chunk_id": "3701af68-02a1-43f2-928d-8b1df4378de2",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Prerequisites and Installation\n\nBefore implementing ROS 2 with Isaac, ensure the following requirements are met:\n\n**Hardware Requirements**:\n- NVIDIA GPU (RTX series recommended) with compute capability >= 6.0\n- CUDA toolkit installed and compatible with Isaac packages\n- Sufficient memory for both ROS 2 and Isaac workloads\n\n**Software Requirements**:\n- Ubuntu 20.04 or 22.04 (or equivalent container)\n- ROS 2 Humble Hawksbill (recommended for stability)\n- Isaac ROS packages installed\n- Isaac Sim (for simulation scenarios)\n\n**Installation Steps**:\n\n1. Install ROS 2 Humble:\n```bash\n# Install ROS 2 Humble\nsudo apt update && sudo apt install curl gnupg lsb-release\ncurl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros-apt-repository/l.key | sudo gpg --dearmor -o /usr/share/keyrings/ros-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\nsudo apt update\nsudo apt install ros-humble-desktop\nsudo apt install python3-colcon-common-extensions\n```",
    "chapter": 5,
    "section": "Prerequisites and Installation",
    "page": 3,
    "token_count": 286
  },
  {
    "chunk_id": "b6e39a9a-153b-4006-b0ec-57a79ea94d0d",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Prerequisites and Installation\n\n2. Install Isaac ROS packages:\n```bash\n# Add NVIDIA package repository\ncurl -sL https://nvidia.github.io/nvidia-ros2-package-repos/rpm/nvidia-ros2-rpm.repo | \\\nsudo tee /etc/apt/sources.list.d/nvidia-ros2.list\n\n# Install Isaac ROS packages\nsudo apt update\nsudo apt install ros-humble-isaac-ros-perception\nsudo apt install ros-humble-isaac-ros-navigation\n# Install other relevant Isaac ROS packages\n```",
    "chapter": 5,
    "section": "Prerequisites and Installation",
    "page": 4,
    "token_count": 113
  },
  {
    "chunk_id": "39e3ebf9-1b9b-4e4b-9126-4f0874b9bec0",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Basic Integration Architecture",
    "chapter": 5,
    "section": "Basic Integration Architecture",
    "page": 5,
    "token_count": 4
  },
  {
    "chunk_id": "62ec412e-b332-4172-831f-0f456c54705a",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Communication Bridge Design\n\nThe communication between ROS 2 and Isaac components typically follows this pattern:\n\n```\n┌─────────────┐    ┌──────────────────────┐    ┌─────────────┐\n│   ROS 2     │    │  Isaac-ROS Bridge    │    │  Isaac      │\n│  Nodes      │◄──►│  (Communication)     │◄──►│  Nodes      │\n│             │    │                     │    │             │\n│ • Perception│    │ • Message Conversion│    │ • GPU       │\n│ • Navigation│    │ • Topic Remapping   │    │   Accelerated│\n│ • Control   │    │ • Service Interfaces│    │   Perception│\n│ • Planning  │    │ • Action Interfaces │    │ • SLAM      │\n└─────────────┘    └──────────────────────┘    └─────────────┘\n```",
    "chapter": 5,
    "section": "Communication Bridge Design",
    "page": 6,
    "token_count": 205
  },
  {
    "chunk_id": "3ac738d4-4aa7-4226-b276-2f50a43c03c3",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Example ROS 2 Node with Isaac Integration\n\n```python\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nfrom datetime import datetime",
    "chapter": 5,
    "section": "Example ROS 2 Node with Isaac Integration",
    "page": 7,
    "token_count": 87
  },
  {
    "chunk_id": "ba7702ea-b61f-4b62-b0f4-e29681ece2b2",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Example ROS 2 Node with Isaac Integration\n\nclass IsaacVLAIntegrationNode(Node):\n    \"\"\"\n    Example node demonstrating VLA integration with ROS 2 and Isaac\n    \"\"\"\n    def __init__(self):\n        super().__init__('isaac_vla_integration')\n        \n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n        \n        # Publishers for Isaac ROS perception outputs\n        self.visp_pub = self.create_publisher(Image, '/isaac_ros_visp/image_out', 10)\n        self.odom_pub = self.create_publisher(Odometry, '/isaac_ros_visual_slam/odometry', 10)\n        self.command_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.response_pub = self.create_publisher(String, '/vla_response', 10)\n        \n        # Subscribers for sensor data\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.image_callback,\n            10\n        )\n        \n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n        \n        self.odom_sub = self.create_subscription(\n            Odometry,\n            '/odometry',\n            self.odom_callback,\n            10\n        )\n        \n        # Natural language command subscriber\n        self.command_sub = self.create_subscription(\n            String,\n            '/natural_language_command',\n            self.language_command_callback,\n            10\n        )\n        \n        # Isaac-specific parameters\n        self.declare_parameters(\n            namespace='',\n            parameters=[\n                ('use_gpu', True),\n                ('gpu_device', 0),\n                ('max_tracking_points', 100),\n                ('feature_threshold', 0.1),\n            ]\n        )\n        \n        # Internal state\n        self.latest_image = None\n        self.latest_odom = None\n        self.camera_info = None\n        self.isaac_initialized = False\n        \n        # Initialize Isaac components\n        self.initialize_isaac_components()\n        \n        # Start processing timer\n        self.process_timer = self.create_timer(0.1, self.process_callback)\n        \n        # Log initialization\n        self.get_logger().info('Isaac VLA Integration Node Initialized')\n    \n    def initialize_isaac_components(self):\n        \"\"\"\n        Initialize Isaac-specific components (simulated)\n        \"\"\"\n        # This is where Isaac perception pipeline would be initialized\n        # For example: Isaac ROS Visual SLAM, object detection, etc.\n        \n        # Check GPU availability for Isaac processing\n        if torch.cuda.is_available():\n            self.get_logger().info(f'Isaac processing enabled on GPU: {torch.cuda.get_device_name()}')\n            self.isaac_initialized = True\n        else:\n            self.get_logger().warn('Isaac GPU acceleration not available, using CPU fallback.')\n            self.isaac_initialized = False\n    \n    def image_callback(self, msg):\n        \"\"\"\n        Process incoming camera images\n        \"\"\"\n        try:\n            # Convert ROS Image to OpenCV format\n            self.latest_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        except Exception as e:\n            self.get_logger().error(f'Error converting image: {e}')\n    \n    def camera_info_callback(self, msg):\n        \"\"\"\n        Process camera calibration information\n        \"\"\"\n        self.camera_info = msg\n    \n    def odom_callback(self, msg):\n        \"\"\"\n        Process odometry information\n        \"\"\"\n        self.latest_odom = msg\n    \n    def language_command_callback(self, msg):\n        \"\"\"\n        Process natural language commands combined with Isaac perception\n        \"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Received language command: {command_text}')\n        \n        # Combine language command with current perception\n        if self.latest_image is not None:\n            self.process_vla_command(command_text, self.latest_image)\n    \n    def process_callback(self):\n        \"\"\"\n        Periodic processing callback for continuous VLA operation\n        \"\"\"\n        if self.latest_image is not None and self.isaac_initialized:\n            # Process current image through Isaac pipeline\n            processed_result = self.process_isaac_perception(self.latest_image)\n            \n            # Publish processed results\n            self.publish_isaac_results(processed_result)\n    \n    def process_vla_command(self, language_command, image):\n        \"\"\"\n        Process combined vision-language command using Isaac components\n        \"\"\"\n        if not self.isaac_initialized:\n            self.get_logger().error('Isaac not initialized, cannot process VLA command')\n            return\n        \n        try:\n            # Simulate Isaac perception processing\n            perception_result = self.simulate_isaac_vision_processing(image)\n            \n            # Combine with language command for action planning\n            action_plan = self.plan_vla_action(language_command, perception_result)\n            \n            # Execute planned action\n            self.execute_action_plan(action_plan)\n            \n            # Publish response\n            response_msg = String()\n            response_msg.data = f\"Processed command: {language_command}. Action: {action_plan['action_type']}\"\n            self.response_pub.publish(response_msg)\n            \n            self.get_logger().info(f'VLA command processed: {response_msg.data}')\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing VLA command: {e}')\n    \n    def simulate_isaac_vision_processing(self, image):\n        \"\"\"\n        Simulate Isaac vision processing pipeline\n        \"\"\"\n        result = {\n            'objects': [],  # Detected objects\n            'affordances': {},  # Action possibilities\n            'spatial': {},  # Spatial relationships\n            'tracking': {}  # Tracked features\n        }\n        \n        # This would use Isaac's GPU-accelerated perception\n        # For simulation, return mock data\n        import random\n        \n        # Simulate object detection\n        result['objects'] = [\n            {'class': 'cup', 'bbox': [100, 100, 200, 200], 'confidence': 0.85},\n            {'class': 'bottle', 'bbox': [300, 150, 400, 250], 'confidence': 0.78}\n        ]\n        \n        # Simulate affordances\n        result['affordances'] = {\n            'cup': ['graspable', 'movable', 'reachable'],\n            'bottle': ['graspable', 'movable', 'pourable', 'reachable']\n        }\n        \n        return result\n    \n    def plan_vla_action(self, language_command, perception_result):\n        \"\"\"\n        Plan action based on language command and perception\n        \"\"\"\n        command_lower = language_command.lower()\n        \n        if 'grasp' in command_lower or 'pick' in command_lower:\n            # Find suitable object to grasp\n            suitable_object = self.select_graspable_object(perception_result)\n            if suitable_object:\n                return {\n                    'action_type': 'grasp',\n                    'target_object': suitable_object,\n                    'description': f'Grasping {suitable_object[\"class\"]} at {suitable_object[\"bbox\"]}'\n                }\n        \n        elif 'navigate' in command_lower or 'go to' in command_lower:\n            # Identify navigation target\n            target = self.identify_navigation_target(command_lower, perception_result)\n            if target:\n                return {\n                    'action_type': 'navigate',\n                    'target': target,\n                    'description': f'Navigating to {target}'\n                }\n        \n        elif 'move' in command_lower:\n            # Plan manipulation action\n            return {\n                'action_type': 'manipulate',\n                'description': 'Planning manipulation based on command and perception',\n                'command': language_command\n            }\n        \n        # Default: return simple action\n        return {\n            'action_type': 'idle',\n            'description': f'No specific action identified for command: {language_command}'\n        }\n    \n    def select_graspable_object(self, perception_result):\n        \"\"\"\n        Select a suitable object for grasping based on perception\n        \"\"\"\n        for obj in perception_result['objects']:\n            obj_class = obj['class']\n            if obj_class in perception_result['affordances'] and 'graspable' in perception_result['affordances'][obj_class]:\n                return obj\n        return None\n    \n    def identify_navigation_target(self, language_command, perception_result):\n        \"\"\"\n        Identify navigation target from language command and perception\n        \"\"\"\n        # Simple keyword matching for demonstration\n        if 'table' in language_command:\n            for obj in perception_result['objects']:\n                if 'table' in obj['class']:\n                    return obj\n        elif 'door' in language_command:\n            for obj in perception_result['objects']:\n                if 'door' in obj['class']:\n                    return obj\n        \n        # If no specific target found, return first detected object\n        if perception_result['objects']:\n            return perception_result['objects'][0]\n        \n        return None\n    \n    def execute_action_plan(self, action_plan):\n        \"\"\"\n        Execute the planned action\n        \"\"\"\n        action_type = action_plan['action_type']\n        \n        if action_type == 'grasp':\n            # Execute grasping action\n            self.execute_grasp_action(action_plan)\n        elif action_type == 'navigate':\n            # Execute navigation action\n            self.execute_navigation_action(action_plan)\n        elif action_type == 'manipulate':\n            # Execute manipulation action\n            self.execute_manipulation_action(action_plan)\n    \n    def execute_grasp_action(self, action_plan):\n        \"\"\"\n        Execute grasping action using Isaac components\n        \"\"\"\n        target_object = action_plan['target_object']\n        bbox = target_object['bbox']\n        \n        # Calculate grasp coordinates (simplified)\n        grasp_x = (bbox[0] + bbox[2]) / 2.0\n        grasp_y = (bbox[1] + bbox[3]) / 2.0\n        \n        # In real implementation, this would call Isaac manipulation pipeline\n        self.get_logger().info(f'Executing grasp at coordinates: ({grasp_x}, {grasp_y})')\n        \n        # Publish command to robot\n        twist_cmd = Twist()\n        # Simplified navigation to object location\n        self.command_pub.publish(twist_cmd)\n    \n    def execute_navigation_action(self, action_plan):\n        \"\"\"\n        Execute navigation action using Isaac components\n        \"\"\"\n        target = action_plan['target']\n        self.get_logger().info(f'Navigating to target: {target}')\n        \n        # In real implementation, this would use Isaac navigation\n        # with GPU-accelerated SLAM and path planning\n    \n    def execute_manipulation_action(self, action_plan):\n        \"\"\"\n        Execute manipulation action using Isaac components\n        \"\"\"\n        command = action_plan['command']\n        self.get_logger().info(f'Executing manipulation for command: {command}')\n    \n    def process_isaac_perception(self, image):\n        \"\"\"\n        Process image through Isaac perception pipeline\n        \"\"\"\n        # This would use Isaac's GPU-accelerated perception\n        # For simulation, perform basic processing\n        processed = {\n            'features': 'simulated_feature_data',\n            'detections': 'simulated_detection_data',\n            'tracking': 'simulated_tracking_data'\n        }\n        return processed\n    \n    def publish_isaac_results(self, result):\n        \"\"\"\n        Publish Isaac processed results to ROS 2 topics\n        \"\"\"\n        # Publish visual servoing results (simulated)\n        if self.visp_pub and result['features'] != 'simulated_feature_data':\n            # Convert features to image format and publish\n            pass\n        \n        # Publish odometry (if available from Isaac SLAM)\n        if self.odom_pub and self.latest_odom:\n            self.odom_pub.publish(self.latest_odom)",
    "chapter": 5,
    "section": "Example ROS 2 Node with Isaac Integration",
    "page": 8,
    "token_count": 2405
  },
  {
    "chunk_id": "a015e474-6c9a-4121-9de7-f7f6a91a345e",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Example ROS 2 Node with Isaac Integration\n\ndef main(args=None):\n    rclpy.init(args=args)\n    \n    # Create the Isaac VLA integration node\n    isaac_vla_node = IsaacVLAIntegrationNode()\n    \n    try:\n        # Spin the node\n        rclpy.spin(isaac_vla_node)\n    except KeyboardInterrupt:\n        print('Shutting down Isaac VLA integration node...')\n    \n    # Cleanup\n    isaac_vla_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```",
    "chapter": 5,
    "section": "Example ROS 2 Node with Isaac Integration",
    "page": 9,
    "token_count": 116
  },
  {
    "chunk_id": "00e5210b-e076-47cc-ad6d-eb47913d2065",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Isaac-Specific Launch File\n\n```xml\n<!-- vla_isaac_integration.launch.py -->\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.conditions import IfCondition\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory",
    "chapter": 5,
    "section": "Isaac-Specific Launch File",
    "page": 10,
    "token_count": 67
  },
  {
    "chunk_id": "a65060b1-3578-4ff9-adc1-0d3d370ede3f",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Isaac-Specific Launch File\n\ndef generate_launch_description():\n    # Launch configuration\n    use_gpu = LaunchConfiguration('use_gpu', default='true')\n    log_level = LaunchConfiguration('log_level', default='info')\n    \n    # Declare launch arguments\n    declare_use_gpu_cmd = DeclareLaunchArgument(\n        'use_gpu',\n        default_value='true',\n        description='Use GPU acceleration for Isaac processing'\n    )\n    \n    declare_log_level_cmd = DeclareLaunchArgument(\n        'log_level',\n        default_value='info',\n        description='Logging level'\n    )\n    \n    # Isaac ROS Visual SLAM node\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        name='visual_slam',\n        parameters=[{\n            'use_gpu': use_gpu,\n            'enable_slam_3d': True,\n            'rectified_images': True,\n            'image_input_width': 640,\n            'image_input_height': 480\n        }],\n        remappings=[\n            ('/visual_slam/image_raw', '/camera/image_rect_color'),\n            ('/visual_slam/camera_info', '/camera/camera_info'),\n            ('/visual_slam/imu', '/imu/data')\n        ],\n        condition=IfCondition(use_gpu)\n    )\n    \n    # Isaac ROS AprilTag detector\n    apriltag_node = Node(\n        package='isaac_ros_apriltag',\n        executable='apriltag_node',\n        name='apriltag',\n        parameters=[{\n            'family': 'tag36h11',\n            'size': 0.145,\n            'max_hamming': 0,\n            'quad_decimate': 2.0\n        }],\n        remappings=[\n            ('/image', '/camera/image_rect_color'),\n            ('/camera_info', '/camera/camera_info')\n        ]\n    )\n    \n    # Isaac ROS DNN inference for object detection\n    dnn_inference_node = Node(\n        package='isaac_ros_dnn_inference',\n        executable='dnn_inference',\n        name='dnn_inference',\n        parameters=[{\n            'model_file': 'detectnet_coco',\n            'input_tensor_layout': 'nhwc',\n            'engine_cache_path': '/tmp/engine.cache'\n        }],\n        remappings=[\n            ('/image', '/camera/image_rect_color'),\n            ('/detections', '/isaac_detections')\n        ]\n    )\n    \n    # Custom VLA integration node\n    vla_integration_node = Node(\n        package='my_vla_package',\n        executable='vla_isaac_integration',\n        name='vla_integration',\n        parameters=[{\n            'use_gpu': use_gpu\n        }],\n        remappings=[\n            ('/camera/image_rect_color', '/camera/image_rect_color'),\n            ('/camera/camera_info', '/camera/camera_info'),\n            ('/natural_language_command', '/natural_language_command')\n        ]\n    )\n    \n    # Create launch description\n    ld = LaunchDescription()\n    \n    # Add launch arguments\n    ld.add_action(declare_use_gpu_cmd)\n    ld.add_action(declare_log_level_cmd)\n    \n    # Add nodes\n    ld.add_action(visual_slam_node)\n    ld.add_action(apriltag_node)\n    ld.add_action(dnn_inference_node)\n    ld.add_action(vla_integration_node)\n    \n    return ld\n```",
    "chapter": 5,
    "section": "Isaac-Specific Launch File",
    "page": 11,
    "token_count": 698
  },
  {
    "chunk_id": "e5044b62-6573-4a25-ad92-6203b0f33258",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Performance Optimization",
    "chapter": 5,
    "section": "Performance Optimization",
    "page": 12,
    "token_count": 3
  },
  {
    "chunk_id": "d7249dcf-566b-4e65-a06e-95bd2fd1b99d",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Efficient Message Passing",
    "chapter": 5,
    "section": "Efficient Message Passing",
    "page": 13,
    "token_count": 4
  },
  {
    "chunk_id": "43e051ae-5b55-41ed-8652-bca7471cdab8",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Efficient Message Passing\n\n```python\nclass OptimizedIsaacROSNode(Node):\n    \"\"\"\n    Optimized node with efficient message passing between ROS 2 and Isaac\n    \"\"\"\n    def __init__(self):\n        super().__init__('optimized_isaac_ros_node')\n        \n        # Reduce message frequency to avoid overwhelming Isaac components\n        self.image_queue_size = 1  # Only latest image to reduce latency\n        \n        # Use intra-process communication where possible\n        self.declare_parameter('use_intra_process', True)\n        \n        # Throttle image processing based on Isaac capabilities\n        self.processing_throttle = 0.1  # 10Hz processing for Isaac\n        self.last_process_time = 0.0\n        \n        # Publishers with optimized configuration\n        self.pub_qos = rclpy.qos.QoSProfile(\n            depth=5,  # Reduced buffer to conserve memory\n            reliability=rclpy.qos.ReliabilityPolicy.BEST_EFFORT,\n            durability=rclpy.qos.DurabilityPolicy.VOLATILE\n        )\n        \n        self.optimized_pub = self.create_publisher(\n            Image, \n            '/isaac_processed_image', \n            self.pub_qos\n        )\n        \n        # Subscription with appropriate settings\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_rect_color',\n            self.optimized_image_callback,\n            self.image_queue_size\n        )\n    \n    def optimized_image_callback(self, msg):\n        \"\"\"\n        Optimize image processing based on timing requirements\n        \"\"\"\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        \n        # Throttle processing based on Isaac pipeline capacity\n        if current_time - self.last_process_time >= self.processing_throttle:\n            self.process_image_for_isaac(msg)\n            self.last_process_time = current_time\n        else:\n            # Skip processing if too frequent\n            pass\n    \n    def process_image_for_isaac(self, image_msg):\n        \"\"\"\n        Process image through Isaac pipeline efficiently\n        \"\"\"\n        # Process image (implementation would use Isaac components)\n        processed_image = self.isaac_process_image(image_msg)\n        \n        # Publish results if necessary\n        self.optimized_pub.publish(processed_image)\n```",
    "chapter": 5,
    "section": "Efficient Message Passing",
    "page": 14,
    "token_count": 456
  },
  {
    "chunk_id": "39c45d3e-6d23-411b-9237-4c728607b245",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "## Best Practices",
    "chapter": 5,
    "section": "Best Practices",
    "page": 15,
    "token_count": 3
  },
  {
    "chunk_id": "71bdfef2-9ee2-45d7-b8b4-888f9f7ce05a",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Isaac-ROS Integration Guidelines\n\n1. **GPU Resource Management**: Properly manage GPU memory and computation resources between ROS 2 and Isaac components.\n\n2. **Message Synchronization**: Ensure proper synchronization between different sensor streams and Isaac processing pipelines.\n\n3. **Error Handling**: Implement robust error handling for cases when Isaac components are unavailable or fail.\n\n4. **Parameter Configuration**: Use ROS 2 parameters to configure Isaac pipeline settings dynamically.\n\n5. **Monitoring and Logging**: Implement comprehensive monitoring of Isaac pipeline performance and resource usage.",
    "chapter": 5,
    "section": "Isaac-ROS Integration Guidelines",
    "page": 16,
    "token_count": 105
  },
  {
    "chunk_id": "5fb2241b-1c89-45ac-980e-96da8cf7099e",
    "doc_id": "69d41270-49de-4329-966c-fbf239b67afc",
    "chunk_text": "### Deployment Considerations\n\nWhen deploying VLA systems with ROS 2 and Isaac:\n\n**Hardware Requirements**: Ensure sufficient GPU memory and compute capability for both ROS 2 operations and Isaac processing.\n\n**Network Configuration**: Configure network settings for optimal communication between distributed components.\n\n**Real-time Constraints**: Consider real-time processing requirements for responsive robot behavior.\n\n**Safety Measures**: Implement safety checks to validate Isaac-processed results before execution.\n\nThis implementation demonstrates the integration of ROS 2 with NVIDIA Isaac for creating Vision-Language-Action systems. The example shows how to set up communication bridges, process multimodal data, and execute complex robotic tasks while taking advantage of Isaac's GPU-accelerated capabilities and ROS 2's distributed architecture.",
    "chapter": 5,
    "section": "Deployment Considerations",
    "page": 17,
    "token_count": 143
  },
  {
    "chunk_id": "ac665d6a-e3e1-41cd-8d2e-c7e80f665720",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "# Section 3: Sensor Fusion & Multimodal Pipelines",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "7795a908-ae1b-493d-8d0b-2397f5915618",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Introduction to Sensor Fusion\n\nSensor fusion in multimodal AI systems involves combining data from multiple sensors to achieve more accurate and reliable information than possible with any single sensor alone. In Physical AI, this is particularly important because robots must operate in complex, uncertain environments where no single sensor modality provides complete information about the state of the world.\n\nThe goal of sensor fusion is to create a unified, coherent understanding of the environment that integrates complementary information from different sensor types while compensating for the limitations of individual sensors. For example, while cameras provide rich visual information, they may fail in low-light conditions; LIDAR provides precise depth information but lacks color and texture detail; IMU sensors provide orientation and acceleration but drift over time.",
    "chapter": 5,
    "section": "Introduction to Sensor Fusion",
    "page": 2,
    "token_count": 145
  },
  {
    "chunk_id": "6f916205-3a98-4172-9571-8a25cabbbcfa",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Mathematical Foundations",
    "chapter": 5,
    "section": "Mathematical Foundations",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "4e82aea6-b363-4af5-8f4f-d74b56ff87d4",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Bayesian Framework\n\nSensor fusion often relies on Bayesian inference to combine information from multiple sources:\n\n```\nP(state|sensors) ∝ P(sensor₁|state) × P(sensor₂|state) × ... × P(sensork|state) × P(state)\n```\n\nThis formulation shows that the posterior probability of a state given multiple sensors is proportional to the product of likelihood terms for each sensor multiplied by the prior probability of the state.",
    "chapter": 5,
    "section": "Bayesian Framework",
    "page": 4,
    "token_count": 90
  },
  {
    "chunk_id": "f86a032b-4976-4a79-9ad6-c80baba72091",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Kalman Filtering\n\nFor continuous state estimation, Kalman filters and their variants (Extended Kalman Filter, Unscented Kalman Filter) provide optimal estimates by combining prediction models with sensor observations:\n\n```\nPrediction: x̂(k|k-1) = F(k) × x̂(k-1|k-1) + B(k) × u(k)\nUpdate: x̂(k|k) = x̂(k|k-1) + K(k) × [z(k) - H(k) × x̂(k|k-1)]\n```\n\nWhere x̂ represents the state estimate, F is the state transition model, H is the observation model, and K is the Kalman gain.",
    "chapter": 5,
    "section": "Kalman Filtering",
    "page": 5,
    "token_count": 156
  },
  {
    "chunk_id": "f523fce2-12ab-4614-8b69-739aa9e87d2e",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Data Association\n\nWhen fusing data from multiple sensors, it's critical to determine which measurements correspond to the same physical objects:\n\n- **Nearest Neighbor**: Associates measurements based on spatial proximity\n- **Joint Probabilistic Data Association**: Considers multiple possible associations simultaneously\n- **Multiple Hypothesis Tracking**: Maintains multiple possible object tracks and associations",
    "chapter": 5,
    "section": "Data Association",
    "page": 6,
    "token_count": 71
  },
  {
    "chunk_id": "420257f8-25ff-452c-a58d-9c2cb80773e2",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Types of Sensor Fusion",
    "chapter": 5,
    "section": "Types of Sensor Fusion",
    "page": 7,
    "token_count": 5
  },
  {
    "chunk_id": "6876fca4-91ae-4c91-b320-1eade05413d5",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Early Fusion\n\nEarly fusion combines raw sensor data before feature extraction, preserving the most information but requiring careful calibration and synchronization:\n\n```\nRaw Camera + Raw LIDAR + Raw IMU → Early Fusion → Features → Classification\n```\n\nBenefits: Maximum information preservation, optimal information use\nChallenges: High computational cost, strict synchronization requirements",
    "chapter": 5,
    "section": "Early Fusion",
    "page": 8,
    "token_count": 68
  },
  {
    "chunk_id": "5fc38556-473e-457b-a967-205829c1227b",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Late Fusion\n\nLate fusion combines high-level decisions or classifications from each sensor modality:\n\n```\nCamera → Visual Processing → Decision A\n                            ↓\nLIDAR → Range Processing → Decision B → Late Fusion → Final Decision\n                            ↑\nIMU → Inertial Processing → Decision C\n```\n\nBenefits: Sensor independence, fault tolerance, easier to implement\nChallenges: Information loss, cannot recover from poor individual sensor performance",
    "chapter": 5,
    "section": "Late Fusion",
    "page": 9,
    "token_count": 88
  },
  {
    "chunk_id": "5d621f32-f56e-4251-bf3f-124492f84dce",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Intermediate Fusion\n\nIntermediate fusion occurs at a level between raw data and final decisions:\n\n```\nCamera → Feature Extraction → Visual Features\n                                          ↓\nLIDAR → Feature Extraction → Range Features → Fusion → Classification\n                                          ↑\nIMU → Feature Extraction → Inertial Features\n```\n\nBenefits: Balance between information preservation and computational tractability\nChallenges: Requires understanding of feature compatibility across modalities",
    "chapter": 5,
    "section": "Intermediate Fusion",
    "page": 10,
    "token_count": 82
  },
  {
    "chunk_id": "3cf32e7a-7a1a-42aa-a0c9-60eeb5de8637",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Practical Sensor Fusion Techniques",
    "chapter": 5,
    "section": "Practical Sensor Fusion Techniques",
    "page": 11,
    "token_count": 5
  },
  {
    "chunk_id": "5a074914-7de7-4ddc-b126-a521ac83453f",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Feature-Level Fusion\n\nCombining features extracted from different sensor modalities:\n\n```python\nclass FeatureLevelFusion:\n    def __init__(self):\n        self.camera_feature_extractor = CnnFeatureExtractor()\n        self.lidar_feature_extractor = PointNetExtractor()\n        self.imu_feature_extractor = ImuFeatureExtractor()\n        \n        self.fusion_network = nn.Sequential(\n            nn.Linear(cnn_dim + lidar_dim + imu_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, camera_data, lidar_data, imu_data):\n        # Extract features from each modality\n        vis_features = self.camera_feature_extractor(camera_data)\n        lidar_features = self.lidar_feature_extractor(lidar_data)\n        imu_features = self.imu_feature_extractor(imu_data)\n        \n        # Concatenate and fuse features\n        concat_features = torch.cat([vis_features, lidar_features, imu_features], dim=-1)\n        fused_features = self.fusion_network(concat_features)\n        \n        return fused_features\n```",
    "chapter": 5,
    "section": "Feature-Level Fusion",
    "page": 12,
    "token_count": 218
  },
  {
    "chunk_id": "6b090eee-61fc-4e68-a6e4-5832afb34928",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Confidence-Based Fusion\n\nWeighting sensor inputs based on their reliability:\n\n```python\nclass ConfidenceBasedFusion:\n    def __init__(self):\n        self.confidence_estimators = {\n            'camera': CameraConfidenceEstimator(),\n            'lidar': LidarConfidenceEstimator(),\n            'imu': ImuConfidenceEstimator()\n        }\n    \n    def weighted_fusion(self, sensor_measurements):\n        total_weight = 0\n        fused_result = 0\n        \n        for sensor_type, measurement in sensor_measurements.items():\n            confidence = self.confidence_estimators[sensor_type].estimate(measurement)\n            fused_result += confidence * measurement\n            total_weight += confidence\n        \n        return fused_result / total_weight\n```",
    "chapter": 5,
    "section": "Confidence-Based Fusion",
    "page": 13,
    "token_count": 148
  },
  {
    "chunk_id": "9b1d8e53-4e87-4e39-9135-2cb18fe73d0b",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Multimodal Processing Pipelines",
    "chapter": 5,
    "section": "Multimodal Processing Pipelines",
    "page": 14,
    "token_count": 7
  },
  {
    "chunk_id": "185447c1-b14e-4b2a-9ba5-abe0991a4c9f",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Pipeline Architecture\n\nA multimodal pipeline typically follows this structure:",
    "chapter": 5,
    "section": "Pipeline Architecture",
    "page": 15,
    "token_count": 13
  },
  {
    "chunk_id": "4a9c7e89-788f-4b83-a971-e63bf1bd0103",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Pipeline Architecture\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   CAMERA        │    │   LIDAR         │    │   OTHER         │\n│   CAPTURE       │    │   CAPTURE       │    │   SENSORS       │\n└─────────┬───────┘    └─────────┬───────┘    └─────────┬───────┘\n          │                      │                      │\n          ▼                      ▼                      ▼\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   PREPROCESSING │    │   PREPROCESSING │    │   PREPROCESSING │\n│   & CALIBRATION │    │   & CALIBRATION │    │   & CALIBRATION │\n└─────────┬───────┘    └─────────┬───────┘    └─────────┬───────┘\n          │                      │                      │\n          └──────────────────────┼──────────────────────┘\n                                 ▼\n                    ┌─────────────────────────┐\n                    │   SYNCHRONIZATION       │\n                    │   & TEMPORAL ALIGNMENT  │\n                    └─────────────────────────┘\n                                 │\n                                 ▼\n                    ┌─────────────────────────┐\n                    │   FUSION PROCESSING     │\n                    │   • Feature Fusion      │\n                    │   • Confidence Weighting│\n                    │   • Data Association    │\n                    │   • State Estimation    │\n                    └─────────────────────────┘\n                                 │\n                                 ▼\n                    ┌─────────────────────────┐\n                    │   DECISION MAKING       │\n                    │   & ACTION PLANNING     │\n                    └─────────────────────────┘\n```",
    "chapter": 5,
    "section": "Pipeline Architecture",
    "page": 16,
    "token_count": 423
  },
  {
    "chunk_id": "eb17d672-27c0-4c09-8a34-8c0ec26db24e",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Synchronization and Temporal Alignment\n\nSynchronizing sensor data is critical for accurate fusion:\n\n```python\nimport asyncio\nfrom collections import deque\nimport time",
    "chapter": 5,
    "section": "Synchronization and Temporal Alignment",
    "page": 17,
    "token_count": 32
  },
  {
    "chunk_id": "849b7d9f-19b1-455e-b12d-930906f481ba",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Synchronization and Temporal Alignment\n\nclass SensorSynchronizer:\n    def __init__(self, sync_window_ms=50):\n        self.sync_window = sync_window_ms / 1000.0  # Convert to seconds\n        self.buffers = {}\n        self.callbacks = {}\n        \n    def register_sensor(self, sensor_id, buffer_size=10):\n        self.buffers[sensor_id] = deque(maxlen=buffer_size)\n        \n    def process_sensor_data(self, sensor_id, data, timestamp):\n        # Add data to buffer with timestamp\n        self.buffers[sensor_id].append((timestamp, data))\n        \n        # Try to synchronize with other sensors\n        self._attempt_synchronization()\n    \n    def _attempt_synchronization(self):\n        # Find closest timestamps across all sensors\n        min_time = float('-inf')\n        max_time = float('inf')\n        \n        for sensor_id, buffer in self.buffers.items():\n            if buffer:\n                earliest = buffer[0][0]\n                latest = buffer[-1][0]\n                min_time = max(min_time, earliest)\n                max_time = min(max_time, latest)\n        \n        # If we have a synchronization window, process synchronized data\n        if max_time - min_time <= self.sync_window:\n            synchronized_data = {}\n            for sensor_id, buffer in self.buffers.items():\n                # Find data closest to the center of the window\n                target_time = (min_time + max_time) / 2\n                closest_data = self._find_closest(buffer, target_time)\n                synchronized_data[sensor_id] = closest_data\n            \n            # Process synchronized data\n            self._process_synchronized_data(synchronized_data)\n    \n    def _find_closest(self, buffer, target_time):\n        closest_time_diff = float('inf')\n        closest_data = None\n        \n        for timestamp, data in buffer:\n            time_diff = abs(timestamp - target_time)\n            if time_diff < closest_time_diff:\n                closest_time_diff = time_diff\n                closest_data = data\n        \n        return closest_data\n```",
    "chapter": 5,
    "section": "Synchronization and Temporal Alignment",
    "page": 18,
    "token_count": 421
  },
  {
    "chunk_id": "ec8eb446-2015-4b28-8bff-048b19d490b8",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Calibration and Registration\n\nProper calibration ensures sensors are properly registered to each other:\n\n```python\nimport numpy as np\nimport cv2",
    "chapter": 5,
    "section": "Calibration and Registration",
    "page": 19,
    "token_count": 28
  },
  {
    "chunk_id": "7d1a88ef-f954-4dd7-a24c-5a5e117dc03c",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Calibration and Registration\n\nclass MultiSensorCalibrator:\n    def __init__(self):\n        # Store transformation matrices between sensors\n        self.transforms = {}\n    \n    def calibrate_camera_lidar(self, checkerboard_images, lidar_scans):\n        \"\"\"\n        Calibrate camera to LIDAR transform using checkerboard patterns\n        \"\"\"\n        camera_extrinsics = []\n        lidar_extrinsics = []\n        \n        for img, scan in zip(checkerboard_images, lidar_scans):\n            # Detect checkerboard in camera image\n            ret, corners = cv2.findChessboardCorners(img, (9, 6))\n            if ret:\n                camera_points = cv2.cornerSubPix(\n                    cv2.cvtColor(img, cv2.COLOR_RGB2GRAY),\n                    corners,\n                    (11, 11),\n                    (-1, -1),\n                    (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n                )\n                \n                # Find corresponding points in LIDAR scan\n                lidar_points = self._find_checkerboard_in_lidar(scan)\n                \n                if len(lidar_points) == len(camera_points):\n                    camera_extrinsics.append(camera_points.reshape(-1, 2))\n                    lidar_extrinsics.append(lidar_points)\n        \n        # Estimate transform between camera and LIDAR\n        transform = self._estimate_transform(\n            np.concatenate(camera_extrinsics),\n            np.concatenate(lidar_extrinsics)\n        )\n        \n        self.transforms[('camera', 'lidar')] = transform\n        return transform\n    \n    def _find_checkerboard_in_lidar(self, scan):\n        # Process LIDAR scan to find checkerboard points\n        # This is a simplified example\n        return scan[:9*6]  # Return first 54 points as example\n    \n    def _estimate_transform(self, points_a, points_b):\n        # Estimate transformation matrix between two point sets\n        return np.eye(4)  # Placeholder\n```",
    "chapter": 5,
    "section": "Calibration and Registration",
    "page": 20,
    "token_count": 417
  },
  {
    "chunk_id": "9c19d62d-8349-454a-b296-6ddf77d6b737",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Real-Time Processing Considerations",
    "chapter": 5,
    "section": "Real-Time Processing Considerations",
    "page": 21,
    "token_count": 6
  },
  {
    "chunk_id": "d88d239d-dab1-400e-b45a-4596622807c3",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Pipeline Optimization\n\nFor real-time robotics applications, several optimization strategies are employed:\n\n**Asynchronous Processing**: Different sensor streams are processed in parallel as much as possible.\n\n**Buffer Management**: Efficient memory management to avoid copying large sensor data unnecessarily.\n\n**Multi-threading**: Different fusion stages can run on different threads to maximize throughput.\n\n**GPU Acceleration**: Computationally intensive fusion operations utilize GPU acceleration.",
    "chapter": 5,
    "section": "Pipeline Optimization",
    "page": 22,
    "token_count": 80
  },
  {
    "chunk_id": "b16ce51c-1a41-4053-969b-e05a19665107",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Latency Management\n\nMinimizing processing latency is crucial for responsive robotic systems:\n\n**Stream Processing**: Process data as it arrives rather than batch processing.\n\n**Priority Scheduling**: Critical sensors or fusion results receive higher priority processing.\n\n**Buffer Queuing**: Manage input queues to prevent data loss while maintaining freshness.",
    "chapter": 5,
    "section": "Latency Management",
    "page": 23,
    "token_count": 62
  },
  {
    "chunk_id": "e0d9421f-e226-4f4d-b53a-03546d5487d2",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Common Sensor Combinations",
    "chapter": 5,
    "section": "Common Sensor Combinations",
    "page": 24,
    "token_count": 5
  },
  {
    "chunk_id": "8f428b3e-f035-477d-9cb3-5bfc4d9d454c",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### RGB-D Fusion\n\nCombining color cameras with depth sensors:\n\n```python\nclass RgbDProcessor:\n    def __init__(self):\n        self.rgb_processor = ColorImageProcessor()\n        self.depth_processor = DepthImageProcessor()\n    \n    def process_rgbd(self, color_image, depth_image):\n        # Process color features\n        color_features = self.rgb_processor.extract_features(color_image)\n        \n        # Process depth features  \n        depth_features = self.depth_processor.extract_features(depth_image)\n        \n        # Register depth to color space\n        registered_depth = self._register_depth_to_color(depth_image, color_image)\n        \n        # Fuse color and depth features\n        fused_features = self._fuse_rgb_depth(color_features, depth_features)\n        \n        return fused_features\n    \n    def _register_depth_to_color(self, depth, color):\n        # Perform camera registration\n        return depth\n    \n    def _fuse_rgb_depth(self, color_features, depth_features):\n        # Learnable fusion of color and depth\n        return torch.cat([color_features, depth_features], dim=-1)\n```",
    "chapter": 5,
    "section": "RGB-D Fusion",
    "page": 25,
    "token_count": 216
  },
  {
    "chunk_id": "f7b44143-0d82-4473-8f57-27cd34f6b1dc",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Camera-LIDAR Fusion\n\nIntegration of visual and range sensing:\n\n```python\nclass CameraLidarFuser:\n    def __init__(self):\n        self.point_cloud_projector = PointCloudProjector()\n        self.feature_fuser = FeatureFusionNetwork()\n    \n    def fuse_camera_lidar(self, image, point_cloud):\n        # Project point cloud to image space\n        projected_points = self.point_cloud_projector.project(point_cloud, image.shape)\n        \n        # Extract visual features\n        vis_features = extract_visual_features(image)\n        \n        # Extract geometric features from points\n        geom_features = extract_geometric_features(projected_points)\n        \n        # Fuse features based on spatial correspondence\n        fused_output = self.feature_fuser(vis_features, geom_features, projected_points)\n        \n        return fused_output\n```",
    "chapter": 5,
    "section": "Camera-LIDAR Fusion",
    "page": 26,
    "token_count": 164
  },
  {
    "chunk_id": "a7b06427-03f6-4b52-8213-bc890f99fb0d",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Inertial-Visual-Odometry (IVO) Fusion\n\nCombining multiple modalities for robust localization:\n\n```python\nclass IVOFuser:\n    def __init__(self):\n        self.visual_odometer = VisualOdometry()\n        self.imu_processor = ImuProcessor()\n        self.fusion_filter = KalmanFilter()\n    \n    def get_pose_estimate(self, image, imu_data):\n        # Get visual odometry estimate\n        visual_pose = self.visual_odometer.process_frame(image)\n        \n        # Get IMU-based motion estimate\n        imu_motion = self.imu_processor.process_data(imu_data)\n        \n        # Fuse estimates using Kalman filter\n        fused_pose = self.fusion_filter.update(visual_pose, imu_motion)\n        \n        return fused_pose\n```",
    "chapter": 5,
    "section": "Inertial-Visual-Odometry (IVO) Fusion",
    "page": 27,
    "token_count": 159
  },
  {
    "chunk_id": "4666577d-50a2-4d2c-ac35-cacedfe853af",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Quality Metrics and Validation",
    "chapter": 5,
    "section": "Quality Metrics and Validation",
    "page": 28,
    "token_count": 5
  },
  {
    "chunk_id": "57a3537b-1e7e-4373-8501-c843666a9942",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Fusion Performance Metrics\n\nTo evaluate sensor fusion effectiveness:\n\n**Information Gain**: How much more information is provided by fusion vs. individual sensors.\n\n**Consistency**: Whether fusion results are consistent across different conditions and modalities.\n\n**Reliability**: The frequency with which fusion provides accurate results.\n\n**Latency**: Processing delay from sensor input to fusion output.",
    "chapter": 5,
    "section": "Fusion Performance Metrics",
    "page": 29,
    "token_count": 70
  },
  {
    "chunk_id": "d92f5857-0af9-4e9a-b903-e706884acc7b",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Validation Techniques\n\n**Ground Truth Comparison**: Compare fusion results to known ground truth when available.\n\n**Cross-Validation**: Validate fusion by comparing results across different modalities.\n\n**Failure Mode Analysis**: Analyze how fusion behaves when one or more sensors fail.\n\n**Stress Testing**: Test fusion under challenging conditions to understand limits.",
    "chapter": 5,
    "section": "Validation Techniques",
    "page": 30,
    "token_count": 65
  },
  {
    "chunk_id": "7e0ffb53-1535-44f3-8c48-e7531632ebf0",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "## Implementation Considerations",
    "chapter": 5,
    "section": "Implementation Considerations",
    "page": 31,
    "token_count": 4
  },
  {
    "chunk_id": "46ac20e4-0bfa-4102-8523-09d88061d056",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Computational Efficiency\n\nReal-world deployment requires consideration of computational constraints:\n\n**Model Compression**: Using techniques like quantization and pruning for efficient inference.\n\n**Scheduling**: Distributing fusion computations across available processing units.\n\n**Memory Management**: Efficiently managing memory for large sensor data.\n\n**Power Consumption**: Minimizing power usage for mobile and embedded systems.",
    "chapter": 5,
    "section": "Computational Efficiency",
    "page": 32,
    "token_count": 69
  },
  {
    "chunk_id": "e5901a51-10c5-4e6c-b349-f7f45a4c699b",
    "doc_id": "8ace23ab-cd07-4f28-a957-e369d24c7947",
    "chunk_text": "### Robustness\n\nFusion systems must handle various failure modes:\n\n**Sensor Dropouts**: Handling cases where sensors temporarily fail.\n\n**Calibration Drift**: Managing gradual degradation in sensor calibration.\n\n**Environmental Changes**: Adapting to different lighting, weather, or conditions.\n\n**Outlier Detection**: Identifying and handling erroneous sensor readings.",
    "chapter": 5,
    "section": "Robustness",
    "page": 33,
    "token_count": 68
  },
  {
    "chunk_id": "dc14799e-e9f6-4827-a135-a489f98dc617",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "# Vision Processing for VLA Systems",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 7
  },
  {
    "chunk_id": "a7ffcdcb-3fe6-4fd4-aa9b-77f8f436724e",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Overview\n\nVision processing forms the perceptual foundation of Vision-Language-Action (VLA) systems, enabling robots to understand their environment, identify relevant objects and entities, and ground linguistic concepts in visual reality. In VLA systems, vision processing goes beyond traditional computer vision tasks by incorporating language-aware analysis, spatial reasoning, and action-relevant perception. This integration allows robots to connect what they see with what humans communicate, creating the semantic bridge necessary for natural language interaction in physical environments.\n\nModern VLA systems require vision processing that is both accurate and efficient, capable of real-time perception while maintaining the rich semantic understanding needed for language grounding. The vision system must not only detect and recognize objects but also understand their affordances, spatial relationships, and relevance to potential actions. This requires sophisticated architectures that can process visual information in ways that support both perception and action planning.",
    "chapter": 5,
    "section": "Overview",
    "page": 2,
    "token_count": 172
  },
  {
    "chunk_id": "1ca74051-ecb6-4a02-9b09-c17279ac486c",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Key Concepts",
    "chapter": 5,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "0dbc2be5-8254-4e32-8d14-004a078c0fdc",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Multimodal Vision Processing\n\nVLA systems require vision processing that is aware of and designed to interface with language understanding systems:\n\n**Semantic Segmentation for Language Grounding**: Pixel-level understanding of images that identifies objects and their attributes in a way that supports linguistic reference.\n\n**Spatial Understanding**: Processing that identifies the spatial layout of environments, relationships between objects, and navigable spaces for both navigation and manipulation tasks.\n\n**Affordance Detection**: Understanding what actions are possible with objects based on their visual appearance and configuration.\n\n**Context-Aware Detection**: Object detection and recognition that considers the broader scene context for more accurate and meaningful interpretations.",
    "chapter": 5,
    "section": "Multimodal Vision Processing",
    "page": 4,
    "token_count": 126
  },
  {
    "chunk_id": "c10309c6-256d-4dc3-8e8a-1fe12878aaca",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Vision-Language Integration\n\nThe vision processing in VLA systems is designed to work seamlessly with language understanding:\n\n**Shared Embedding Spaces**: Vision features are processed to be compatible with language embeddings, enabling cross-modal comparison and fusion.\n\n**Attention-Guided Processing**: Vision processing can be guided by linguistic attention, focusing computational resources on regions relevant to the current language task.\n\n**Mutual Refinement**: Vision and language understanding can refine each other, with linguistic context improving vision processing and visual information refining language interpretation.",
    "chapter": 5,
    "section": "Vision-Language Integration",
    "page": 5,
    "token_count": 101
  },
  {
    "chunk_id": "cbfb5c64-4f18-4bfa-a55c-1e0fefa145d0",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Real-Time Constraints\n\nVLA systems must operate in real-time for natural interaction:\n\n**Efficient Architectures**: Vision models optimized for real-time inference on robotic hardware.\n\n**Adaptive Processing**: Dynamic adjustment of vision processing complexity based on task requirements and computational resources.\n\n**Streaming Processing**: Continuous processing of video streams rather than frame-by-frame analysis.",
    "chapter": 5,
    "section": "Real-Time Constraints",
    "page": 6,
    "token_count": 69
  },
  {
    "chunk_id": "f9e0dd05-5bfa-4205-8d00-fcfec370bdd8",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision Processing Architectures",
    "chapter": 5,
    "section": "Vision Processing Architectures",
    "page": 7,
    "token_count": 5
  },
  {
    "chunk_id": "df46e15f-41f8-4610-9f6e-dff928ea0b59",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Feature Extraction for VLA Systems\n\nVision models in VLA systems need to extract features that support both perception and language grounding:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom transformers import CLIPVisionModel",
    "chapter": 5,
    "section": "Feature Extraction for VLA Systems",
    "page": 8,
    "token_count": 51
  },
  {
    "chunk_id": "1d82ab14-540a-4346-8b6f-e53d27c46c38",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Feature Extraction for VLA Systems\n\nclass VLAFeatureExtractor(nn.Module):\n    \"\"\"\n    Extracts features for Vision-Language-Action systems\n    \"\"\"\n    def __init__(self, feature_dim=768, use_clip=True):\n        super().__init__()\n        \n        if use_clip:\n            # Use pre-trained CLIP vision encoder for good vision-language integration\n            self.vision_encoder = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n            self.feature_dim = 768\n        else:\n            # Alternative: Custom CNN backbone\n            resnet = models.resnet50(pretrained=True)\n            self.vision_encoder = nn.Sequential(*list(resnet.children())[:-2])\n            self.feature_dim = 2048\n        \n        # Additional processing layers for VLA-specific features\n        self.spatial_attention = SpatialAttention()\n        self.affordance_head = AffordanceDetectionHead(self.feature_dim)\n        self.objectness_head = ObjectnessHead(self.feature_dim)\n        \n    def forward(self, images):\n        \"\"\"\n        Process images to extract VLA-relevant features\n        \"\"\"\n        batch_size = images.size(0)\n        \n        # Extract visual features\n        if hasattr(self.vision_encoder, 'vision_model'):\n            # CLIP model\n            vision_outputs = self.vision_encoder(pixel_values=images)\n            visual_features = vision_outputs.last_hidden_state  # [B, N, D]\n        else:\n            # Custom CNN\n            features = self.vision_encoder(images)  # [B, C, H, W]\n            B, C, H, W = features.shape\n            visual_features = features.view(B, C, -1).transpose(1, 2)  # [B, H*W, C]\n        \n        # Apply spatial attention to highlight important regions\n        attended_features, attention_weights = self.spatial_attention(visual_features)\n        \n        # Extract affordance information\n        affordance_features = self.affordance_head(attended_features)\n        \n        # Extract objectness scores\n        objectness_scores = self.objectness_head(attended_features)\n        \n        return {\n            'features': attended_features,\n            'attention_weights': attention_weights,\n            'affordances': affordance_features,\n            'objectness': objectness_scores,\n            'spatial_map': self._create_spatial_map(attended_features)\n        }\n    \n    def _create_spatial_map(self, features):\n        \"\"\"\n        Create spatial map for spatial reasoning\n        \"\"\"\n        # This would convert feature maps back to spatial coordinates\n        # for spatial relationship analysis\n        pass",
    "chapter": 5,
    "section": "Feature Extraction for VLA Systems",
    "page": 9,
    "token_count": 533
  },
  {
    "chunk_id": "98b6d215-a25f-4348-80c1-27291726886f",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Feature Extraction for VLA Systems\n\nclass SpatialAttention(nn.Module):\n    \"\"\"\n    Attention mechanism that highlights spatially relevant regions\n    \"\"\"\n    def __init__(self, feature_dim=768):\n        super().__init__()\n        self.feature_dim = feature_dim\n        self.attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=8,\n            dropout=0.1\n        )\n        self.layer_norm = nn.LayerNorm(feature_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim * 4),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim * 4, feature_dim)\n        )\n    \n    def forward(self, features):\n        \"\"\"\n        features: [B, N, D] where N is number of patches/regions\n        \"\"\"\n        # Self-attention to highlight important regions\n        attended_features, attention_weights = self.attention(\n            features.transpose(0, 1),  # [N, B, D]\n            features.transpose(0, 1),\n            features.transpose(0, 1)\n        )\n        attended_features = attended_features.transpose(0, 1)  # [B, N, D]\n        \n        # Add residual connection and layer norm\n        attended_features = self.layer_norm(attended_features + features)\n        \n        # Feed-forward network\n        output_features = self.ffn(attended_features)\n        output_features = self.layer_norm(output_features + attended_features)\n        \n        return output_features, attention_weights",
    "chapter": 5,
    "section": "Feature Extraction for VLA Systems",
    "page": 10,
    "token_count": 316
  },
  {
    "chunk_id": "9677f2c8-d2ac-465a-82d1-4e10b7985325",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Feature Extraction for VLA Systems\n\nclass AffordanceDetectionHead(nn.Module):\n    \"\"\"\n    Detects object affordances from visual features\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.affordance_classifier = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim // 2, len(affordance_categories))  # Predefined affordances\n        )\n    \n    def forward(self, features):\n        return torch.sigmoid(self.affordance_classifier(features))\n\nclass ObjectnessHead(nn.Module):\n    \"\"\"\n    Estimates objectness of visual regions\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.objectness_predictor = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 4),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim // 4, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, features):\n        return self.objectness_predictor(features)\n\n# Define common affordance categories for VLA systems\naffordance_categories = [\n    'graspable', 'movable', 'pourable', 'stackable', \n    'openable', 'closeable', 'sittable', 'reachable'\n]\n```",
    "chapter": 5,
    "section": "Feature Extraction for VLA Systems",
    "page": 11,
    "token_count": 285
  },
  {
    "chunk_id": "a004846f-e5d1-41bf-98dd-c5b7d4336df3",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Object Detection and Recognition for VLA\n\nObject detection in VLA systems needs to be both accurate and semantically rich:\n\n```python\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone",
    "chapter": 5,
    "section": "Object Detection and Recognition for VLA",
    "page": 12,
    "token_count": 78
  },
  {
    "chunk_id": "1d4e256a-aeb8-4224-a410-0d3004a6de12",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Object Detection and Recognition for VLA\n\nclass VLAObjectDetector(nn.Module):\n    \"\"\"\n    Object detection system optimized for VLA applications\n    \"\"\"\n    def __init__(self, num_classes=91, use_pretrained=True):\n        super().__init__()\n        \n        if use_pretrained:\n            # Start with pre-trained Faster R-CNN\n            self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n            \n            # Replace classification head with custom head that includes attribute prediction\n            in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n            self.model.roi_heads.box_predictor = CustomBoxPredictor(\n                in_features, num_classes\n            )\n        else:\n            # Custom backbone\n            backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n            self.model = torchvision.models.detection.FasterRCNN(\n                backbone=backbone,\n                num_classes=num_classes\n            )\n        \n        # Attribute prediction head (for colors, materials, etc.)\n        self.attribute_predictor = AttributePredictionHead(in_features)\n        \n        # Affordance prediction head\n        self.affordance_predictor = AffordancePredictionHead(in_features)\n    \n    def forward(self, images, targets=None):\n        \"\"\"\n        Forward pass with additional VLA-specific outputs\n        \"\"\"\n        if self.training and targets is not None:\n            # Training mode with targets\n            return self.model(images, targets)\n        else:\n            # Inference mode\n            detections = self.model(images)\n            \n            # For each detection, predict attributes and affordances\n            enhanced_detections = []\n            for i, detection in enumerate(detections):\n                boxes = detection['boxes']\n                labels = detection['labels']\n                scores = detection['scores']\n                \n                # Extract features for attribute and affordance prediction\n                # This is simplified - in practice, you'd use RoIAlign features\n                box_features = self._extract_roi_features(images[i], boxes)\n                \n                # Predict attributes\n                attributes = self.attribute_predictor(box_features)\n                \n                # Predict affordances\n                affordances = self.affordance_predictor(box_features)\n                \n                enhanced_detection = {\n                    'boxes': boxes,\n                    'labels': labels,\n                    'scores': scores,\n                    'attributes': attributes,\n                    'affordances': affordances\n                }\n                \n                enhanced_detections.append(enhanced_detection)\n            \n            return enhanced_detections\n    \n    def _extract_roi_features(self, image, boxes):\n        \"\"\"\n        Extract features for each detected bounding box\n        \"\"\"\n        # In practice, this would use RoIAlign/Pool\n        # For simplicity, we'll return placeholder features\n        return torch.randn(len(boxes), 256)",
    "chapter": 5,
    "section": "Object Detection and Recognition for VLA",
    "page": 13,
    "token_count": 562
  },
  {
    "chunk_id": "1a14ad59-e2ca-44cc-9cea-60db6658ae24",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Object Detection and Recognition for VLA\n\nclass CustomBoxPredictor(nn.Module):\n    \"\"\"\n    Custom box predictor that can handle VLA-specific requirements\n    \"\"\"\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.cls_score = nn.Linear(in_channels, num_classes)\n        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n        self.confidence_score = nn.Linear(in_channels, 1)  # Additional confidence for grounding\n    \n    def forward(self, x):\n        if x.dim() == 4:\n            x = torch.flatten(x, start_dim=1)\n        \n        scores = self.cls_score(x)\n        bbox_deltas = self.bbox_pred(x)\n        confidence = torch.sigmoid(self.confidence_score(x))\n        \n        return scores, bbox_deltas, confidence",
    "chapter": 5,
    "section": "Object Detection and Recognition for VLA",
    "page": 14,
    "token_count": 171
  },
  {
    "chunk_id": "c74e0315-8948-4ed3-8b7f-8ee9dbc9e685",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Object Detection and Recognition for VLA\n\nclass AttributePredictionHead(nn.Module):\n    \"\"\"\n    Predicts object attributes relevant for language grounding\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        # Predict color, size, material, etc.\n        self.color_predictor = nn.Linear(feature_dim, 10)  # 10 common colors\n        self.size_predictor = nn.Linear(feature_dim, 3)    # small, medium, large\n        self.material_predictor = nn.Linear(feature_dim, 8) # wood, metal, plastic, etc.\n        \n    def forward(self, features):\n        return {\n            'colors': torch.softmax(self.color_predictor(features), dim=-1),\n            'sizes': torch.softmax(self.size_predictor(features), dim=-1),\n            'materials': torch.softmax(self.material_predictor(features), dim=-1)\n        }\n\nclass AffordancePredictionHead(nn.Module):\n    \"\"\"\n    Predicts object affordances for action planning\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        # Predict action capabilities for each object\n        self.action_predictor = nn.Linear(feature_dim, len(affordance_categories))\n    \n    def forward(self, features):\n        return torch.sigmoid(self.action_predictor(features))\n```",
    "chapter": 5,
    "section": "Object Detection and Recognition for VLA",
    "page": 15,
    "token_count": 267
  },
  {
    "chunk_id": "a28b1ff5-9cda-429e-8509-09932c237979",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Scene Understanding and Spatial Reasoning\n\nVLA systems need to understand not just individual objects but also their spatial relationships:",
    "chapter": 5,
    "section": "Scene Understanding and Spatial Reasoning",
    "page": 16,
    "token_count": 24
  },
  {
    "chunk_id": "a320eec6-497b-4927-8f3c-7e6ab38899c1",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Scene Understanding and Spatial Reasoning\n\n```python\nclass SceneUnderstandingModule(nn.Module):\n    \"\"\"\n    Understands scene layout and object relationships for VLA systems\n    \"\"\"\n    def __init__(self, feature_dim=256):\n        super().__init__()\n        \n        # Spatial relation network\n        self.spatial_relation_net = SpatialRelationNetwork(feature_dim)\n        \n        # Scene graph generator\n        self.scene_graph_generator = SceneGraphGenerator(feature_dim)\n        \n        # Navigation space detector\n        self.traversable_space_detector = TraversableSpaceDetector()\n        \n        # Functional region detector\n        self.functional_region_detector = FunctionalRegionDetector()\n    \n    def forward(self, image, detections):\n        \"\"\"\n        image: input image [B, C, H, W]\n        detections: object detection results from VLAObjectDetector\n        \"\"\"\n        batch_size = image.size(0)\n        results = []\n        \n        for i in range(batch_size):\n            # Analyze spatial relationships between objects\n            spatial_relations = self._analyze_spatial_relations(detections[i])\n            \n            # Generate scene graph\n            scene_graph = self.scene_graph_generator(\n                detections[i], spatial_relations\n            )\n            \n            # Detect navigable areas\n            navigable_areas = self.traversable_space_detector(image[i])\n            \n            # Detect functional regions (kitchen counter, dining table, etc.)\n            functional_regions = self.functional_region_detector(image[i])\n            \n            result = {\n                'spatial_relations': spatial_relations,\n                'scene_graph': scene_graph,\n                'navigable_areas': navigable_areas,\n                'functional_regions': functional_regions,\n                'detections': detections[i]\n            }\n            \n            results.append(result)\n        \n        return results\n    \n    def _analyze_spatial_relations(self, detections):\n        \"\"\"\n        Analyze spatial relationships between detected objects\n        \"\"\"\n        boxes = detections['boxes']\n        labels = detections['labels']\n        \n        relations = []\n        \n        for i in range(len(boxes)):\n            for j in range(len(boxes)):\n                if i != j:\n                    rel = self._compute_spatial_relation(boxes[i], boxes[j])\n                    relations.append({\n                        'subject': {'id': i, 'class': labels[i]},\n                        'relation': rel['type'],\n                        'object': {'id': j, 'class': labels[j]},\n                        'confidence': rel['confidence']\n                    })\n        \n        return relations\n    \n    def _compute_spatial_relation(self, box1, box2):\n        \"\"\"\n        Compute spatial relationship between two bounding boxes\n        \"\"\"\n        x1_1, y1_1, x2_1, y2_1 = box1\n        x1_2, y1_2, x2_2, y2_2 = box2\n        \n        # Calculate centers\n        center1 = torch.tensor([(x1_1 + x2_1) / 2, (y1_1 + y2_1) / 2])\n        center2 = torch.tensor([(x1_2 + x2_2) / 2, (y1_2 + y2_2) / 2])\n        \n        # Calculate offset\n        offset = center1 - center2\n        \n        # Determine spatial relation based on offset\n        if abs(offset[0]) > abs(offset[1]):\n            # Horizontal dominates\n            if offset[0] > 0:\n                relation = 'left_of'\n            else:\n                relation = 'right_of'\n        else:\n            # Vertical dominates\n            if offset[1] > 0:\n                relation = 'above'\n            else:\n                relation = 'below'\n        \n        # Calculate distance for proximity relations\n        distance = torch.norm(offset)\n        \n        # If boxes are close enough, add proximity relation\n        if distance < 50:  # pixels, adjust threshold as needed\n            if relation in ['left_of', 'right_of', 'above', 'below']:\n                relation = f'close_to_{relation}'\n            else:\n                relation = 'near'\n        \n        return {'type': relation, 'confidence': 0.9}",
    "chapter": 5,
    "section": "Scene Understanding and Spatial Reasoning",
    "page": 17,
    "token_count": 840
  },
  {
    "chunk_id": "095aad76-8e44-400d-b0d9-15e07fffd324",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Scene Understanding and Spatial Reasoning\n\nclass SpatialRelationNetwork(nn.Module):\n    \"\"\"\n    Neural network for learning spatial relations\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.relation_encoder = nn.Sequential(\n            nn.Linear(feature_dim * 2 + 4, feature_dim),  # +4 for position info\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 2, len(spatial_relations))\n        )\n    \n    def forward(self, features1, features2, pos1, pos2):\n        # Concatenate features and position information\n        pos_rel = pos2 - pos1\n        combined_input = torch.cat([\n            features1, features2, pos_rel\n        ], dim=-1)\n        \n        relations = self.relation_encoder(combined_input)\n        return torch.softmax(relations, dim=-1)",
    "chapter": 5,
    "section": "Scene Understanding and Spatial Reasoning",
    "page": 18,
    "token_count": 201
  },
  {
    "chunk_id": "0e2f6dc1-ba41-4485-9f4d-1ae3ca3be2c4",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Scene Understanding and Spatial Reasoning\n\nclass SceneGraphGenerator(nn.Module):\n    \"\"\"\n    Generates scene graphs for semantic understanding\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.graph_nn = nn.ModuleList([\n            nn.Linear(feature_dim, feature_dim),\n            nn.ReLU(),\n            nn.Linear(feature_dim, feature_dim),\n            nn.ReLU()\n        ])\n    \n    def forward(self, detections, spatial_relations):\n        \"\"\"\n        Generate scene graph from detections and relations\n        \"\"\"\n        # Create nodes for objects\n        nodes = []\n        for i, (box, label) in enumerate(zip(detections['boxes'], detections['labels'])):\n            node = {\n                'id': i,\n                'bbox': box,\n                'class': label,\n                'attributes': detections['attributes'][i] if 'attributes' in detections else {},\n                'affordances': detections['affordances'][i] if 'affordances' in detections else {}\n            }\n            nodes.append(node)\n        \n        # Create edges from spatial relations\n        edges = []\n        for rel in spatial_relations:\n            edges.append({\n                'subject': rel['subject']['id'],\n                'predicate': rel['relation'],\n                'object': rel['object']['id'],\n                'confidence': rel['confidence']\n            })\n        \n        return {\n            'nodes': nodes,\n            'edges': edges,\n            'num_nodes': len(nodes)\n        }",
    "chapter": 5,
    "section": "Scene Understanding and Spatial Reasoning",
    "page": 19,
    "token_count": 295
  },
  {
    "chunk_id": "6699bf9a-5a5c-4470-a0e0-1a5d30c3f0b6",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Scene Understanding and Spatial Reasoning\n\n# Define common spatial relations for VLA systems\nspatial_relations = [\n    'left_of', 'right_of', 'above', 'below', \n    'near', 'far_from', 'on', 'under', \n    'in_front_of', 'behind', 'next_to'\n]\n```",
    "chapter": 5,
    "section": "Scene Understanding and Spatial Reasoning",
    "page": 20,
    "token_count": 70
  },
  {
    "chunk_id": "ae072239-81db-498b-86f7-43c1dd5f8f31",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Visual Grounding for Language Commands",
    "chapter": 5,
    "section": "Visual Grounding for Language Commands",
    "page": 21,
    "token_count": 7
  },
  {
    "chunk_id": "45e5bdf6-87cf-4331-b6d1-6c24e63376c2",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Grounding Textual References in Images\n\nVisual grounding connects language references to visual entities:",
    "chapter": 5,
    "section": "Grounding Textual References in Images",
    "page": 22,
    "token_count": 18
  },
  {
    "chunk_id": "7b4194dc-b617-40b5-8e6e-a0386aa5dfc4",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Grounding Textual References in Images\n\n```python\nclass VisualGroundingModule(nn.Module):\n    \"\"\"\n    Grounds linguistic references in visual scenes\n    \"\"\"\n    def __init__(self, feature_dim=768):\n        super().__init__()\n        \n        # Cross-modal attention for text-image alignment\n        self.cross_attention = CrossModalAttention(feature_dim)\n        \n        # Region classification head\n        self.region_classifier = nn.Linear(feature_dim, 2)  # relevant/irrelevant\n        \n        # Confidence predictor\n        self.confidence_predictor = nn.Linear(feature_dim, 1)\n        \n    def forward(self, image_features, text_features, object_detections):\n        \"\"\"\n        image_features: [B, N, D] - features from all image regions\n        text_features: [B, T, D] - features from text tokens\n        object_detections: dict from object detector\n        \"\"\"\n        batch_size = image_features.size(0)\n        results = []\n        \n        for b in range(batch_size):\n            # Apply cross-modal attention between text and image features\n            attended_image_features, attention_weights = self.cross_attention(\n                image_features[b],  # [N, D]\n                text_features[b]    # [T, D]\n            )\n            \n            # Classify each region as relevant or irrelevant to text\n            region_scores = self.region_classifier(attended_image_features)  # [N, 2]\n            region_probs = torch.softmax(region_scores, dim=-1)  # [N, 2]\n            relevance_scores = region_probs[:, 1]  # Probability of being relevant\n            \n            # Predict confidence scores\n            confidence_scores = torch.sigmoid(\n                self.confidence_predictor(attended_image_features)\n            ).squeeze(-1)  # [N]\n            \n            # Match with object detections\n            grounded_objects = self._match_to_detections(\n                object_detections[b],\n                relevance_scores,\n                confidence_scores,\n                attention_weights\n            )\n            \n            results.append({\n                'grounded_objects': grounded_objects,\n                'relevance_scores': relevance_scores,\n                'attention_weights': attention_weights\n            })\n        \n        return results\n    \n    def _match_to_detections(self, detections, relevance_scores, confidence_scores, attention_weights):\n        \"\"\"\n        Match grounding results to object detection results\n        \"\"\"\n        if len(detections['boxes']) == 0:\n            return []\n        \n        # Map relevance scores to detection indices\n        # This assumes image features correspond to detection regions\n        max_relevance_idx = torch.argmax(relevance_scores)\n        \n        # Create grounded object with grounding confidence\n        grounded_objects = []\n        for i, (box, label) in enumerate(zip(detections['boxes'], detections['labels'])):\n            # Calculate grounding score based on attention and relevance\n            grounding_score = relevance_scores[i] * confidence_scores[i]\n            \n            if grounding_score > 0.5:  # confidence threshold\n                grounded_obj = {\n                    'box': box,\n                    'label': label,\n                    'grounding_score': grounding_score.item(),\n                    'relevance_score': relevance_scores[i].item(),\n                    'confidence_score': confidence_scores[i].item(),\n                    'attributes': detections.get('attributes', [])[i] if i < len(detections.get('attributes', [])) else {},\n                    'affordances': detections.get('affordances', [])[i] if i < len(detections.get('affordances', [])) else {}\n                }\n                grounded_objects.append(grounded_obj)\n        \n        # Sort by grounding score\n        grounded_objects.sort(key=lambda x: x['grounding_score'], reverse=True)\n        \n        return grounded_objects",
    "chapter": 5,
    "section": "Grounding Textual References in Images",
    "page": 23,
    "token_count": 750
  },
  {
    "chunk_id": "709a7dcd-f62e-492d-bc37-ecf0a7cd7f4c",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Grounding Textual References in Images\n\nclass CrossModalAttention(nn.Module):\n    \"\"\"\n    Implements cross-modal attention between vision and text\n    \"\"\"\n    def __init__(self, feature_dim, num_heads=8):\n        super().__init__()\n        self.feature_dim = feature_dim\n        self.num_heads = num_heads\n        self.head_dim = feature_dim // num_heads\n        \n        assert self.head_dim * num_heads == feature_dim, \"Feature dimension must be divisible by num heads\"\n        \n        self.qkv = nn.Linear(feature_dim, feature_dim * 3)\n        self.scale = self.head_dim ** -0.5\n        self.proj = nn.Linear(feature_dim, feature_dim)\n        \n    def forward(self, x, y):\n        \"\"\"\n        x: vision features [N, D]\n        y: text features [T, D]\n        \"\"\"\n        N, D = x.shape\n        T, _ = y.shape\n        \n        # Combine vision and text features for attention\n        combined = torch.cat([x, y], dim=0)  # [N+T, D]\n        \n        # Apply QKV transformation\n        qkv = self.qkv(combined).reshape(\n            N + T, 3, self.num_heads, self.head_dim\n        ).permute(1, 2, 0, 3)  # [3, H, N+T, Dh]\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Compute attention: vision attending to text and vice versa\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        \n        # Apply attention to get output\n        out = (attn @ v).transpose(1, 2).reshape(N + T, D)\n        out = self.proj(out)\n        \n        # Split back to vision and text parts\n        attended_vision = out[:N]  # [N, D]\n        \n        return attended_vision, attn[:N, T:]  # Vision features and attention weights",
    "chapter": 5,
    "section": "Grounding Textual References in Images",
    "page": 24,
    "token_count": 438
  },
  {
    "chunk_id": "fc63411b-bf42-4738-bcf9-0068a4f859b0",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Grounding Textual References in Images\n\nclass ReferringExpressionModule(nn.Module):\n    \"\"\"\n    Handles referring expressions like \"the red cup on the table\"\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.attribute_matcher = AttributeMatcher()\n        self.spatial_matcher = SpatialMatcher()\n        \n    def forward(self, referring_expression, detections, scene_graph):\n        \"\"\"\n        Resolve a referring expression to a specific object\n        \"\"\"\n        # Parse the referring expression to identify attributes and spatial relations\n        parsed_ref = self._parse_referring_expression(referring_expression)\n        \n        # Match attributes\n        attribute_candidates = self.attribute_matcher(\n            parsed_ref['attributes'], detections\n        )\n        \n        # Match spatial relations\n        spatial_candidates = self.spatial_matcher(\n            parsed_ref['spatial_relations'], scene_graph, detections\n        )\n        \n        # Combine results using weighted scoring\n        final_candidates = self._combine_candidates(\n            attribute_candidates, spatial_candidates, parsed_ref\n        )\n        \n        return final_candidates\n    \n    def _parse_referring_expression(self, expr):\n        \"\"\"\n        Parse referring expression into components\n        \"\"\"\n        # Simplified parsing - in practice, use NLP tools\n        words = expr.lower().split()\n        \n        attributes = []\n        spatial_relations = []\n        target_class = \"\"\n        \n        for word in words:\n            if word in ['red', 'blue', 'green', 'large', 'small', 'wooden', 'metal']:\n                attributes.append(word)\n            elif word in ['on', 'in', 'next', 'to', 'near', 'left', 'right']:\n                spatial_relations.append(word)\n            else:\n                target_class = word  # Simplified - would need more sophisticated parsing\n        \n        return {\n            'attributes': attributes,\n            'spatial_relations': spatial_relations,\n            'target_class': target_class\n        }\n    \n    def _combine_candidates(self, attr_candidates, spatial_candidates, parsed_ref):\n        \"\"\"\n        Combine attribute and spatial matching results\n        \"\"\"\n        # Weight scores based on confidence in different modalities\n        combined_scores = {}\n        \n        for obj_id, attr_score in attr_candidates.items():\n            spatial_score = spatial_candidates.get(obj_id, 0.1)  # Low default\n            combined_score = 0.6 * attr_score + 0.4 * spatial_score\n            combined_scores[obj_id] = combined_score\n        \n        # Return candidates sorted by combined score\n        sorted_candidates = sorted(\n            combined_scores.items(), key=lambda x: x[1], reverse=True\n        )\n        \n        return sorted_candidates",
    "chapter": 5,
    "section": "Grounding Textual References in Images",
    "page": 25,
    "token_count": 533
  },
  {
    "chunk_id": "ac116828-67a4-4130-a97d-11c5b0bd7a94",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Grounding Textual References in Images\n\nclass AttributeMatcher(nn.Module):\n    \"\"\"\n    Matches objects based on attribute descriptions\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # This would contain the logic for matching linguistic attributes to visual attributes\n        pass\n    \n    def forward(self, attributes, detections):\n        \"\"\"\n        Match attributes to detection results\n        \"\"\"\n        scores = {}\n        \n        for i, detection in enumerate(detections['grounded_objects']):\n            attr_score = 1.0\n            for attr in attributes:\n                # Compare attribute to object properties\n                if attr in detection.get('attributes', {}).get('colors', []):\n                    attr_score *= 0.9\n                elif attr in detection.get('attributes', {}).get('materials', []):\n                    attr_score *= 0.8\n                elif attr in detection.get('attributes', {}).get('sizes', []):\n                    attr_score *= 0.7\n                else:\n                    attr_score *= 0.1  # Low score for non-matching attribute\n            \n            scores[i] = attr_score\n        \n        return scores",
    "chapter": 5,
    "section": "Grounding Textual References in Images",
    "page": 26,
    "token_count": 226
  },
  {
    "chunk_id": "b7db6973-7aab-465c-867a-334fb9c2a5df",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Grounding Textual References in Images\n\nclass SpatialMatcher(nn.Module):\n    \"\"\"\n    Matches objects based on spatial descriptions\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        pass\n    \n    def forward(self, spatial_relations, scene_graph, detections):\n        \"\"\"\n        Match spatial relations to scene structure\n        \"\"\"\n        scores = {}\n        \n        # Use scene graph to evaluate spatial constraints\n        # This is a simplified example\n        for i, detection in enumerate(detections['grounded_objects']):\n            # Check spatial relations against scene graph\n            spatial_score = 0.5  # Default score\n            scores[i] = spatial_score\n        \n        return scores\n```",
    "chapter": 5,
    "section": "Grounding Textual References in Images",
    "page": 27,
    "token_count": 139
  },
  {
    "chunk_id": "0017a52e-6c79-4dff-ad79-1fed2fd0098f",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Affordance Detection and Action Grounding",
    "chapter": 5,
    "section": "Affordance Detection and Action Grounding",
    "page": 28,
    "token_count": 9
  },
  {
    "chunk_id": "7fbbca40-9f22-4494-b850-9aacc147324c",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Understanding Object Affordances\n\nAffordance detection helps VLA systems understand what actions are possible with objects:",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 29,
    "token_count": 23
  },
  {
    "chunk_id": "6bc3e68b-a20d-41f3-ae6f-6a070dae354c",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Understanding Object Affordances\n\n```python\nclass AffordanceDetectionModule(nn.Module):\n    \"\"\"\n    Detects object affordances for action planning\n    \"\"\"\n    def __init__(self, feature_dim=256):\n        super().__init__()\n        \n        # Affordance classification head\n        self.affordance_classifier = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(feature_dim, len(affordance_categories)),\n            nn.Sigmoid()\n        )\n        \n        # Part-based affordance detector\n        self.part_affordance_detector = PartAffordanceDetector(feature_dim)\n        \n        # Context-aware affordance predictor\n        self.context_affordance_predictor = ContextAffordancePredictor(feature_dim)\n    \n    def forward(self, image, detections, scene_graph):\n        \"\"\"\n        Detect affordances for detected objects\n        \"\"\"\n        batch_size = image.size(0)\n        results = []\n        \n        for b in range(batch_size):\n            affordances_result = {\n                'objects': [],\n                'action_potential': {},\n                'spatial_affordances': []\n            }\n            \n            for i, detection in enumerate(detections[b]['grounded_objects']):\n                # Use detection features for affordance prediction\n                # This would use features from the detection backbone\n                bbox = detection['box']\n                \n                # Extract features for this region\n                roi_features = self._extract_roi_features(image[b], bbox)\n                \n                # Predict basic affordances\n                basic_affordances = self.affordance_classifier(roi_features)\n                \n                # Predict part-based affordances\n                part_affordances = self.part_affordance_detector(\n                    image[b], bbox\n                )\n                \n                # Predict context-aware affordances\n                context_affordances = self.context_affordance_predictor(\n                    detection, scene_graph[b]\n                )\n                \n                # Combine all affordances\n                combined_affordances = self._combine_affordances(\n                    basic_affordances, part_affordances, context_affordances\n                )\n                \n                affordances_result['objects'].append({\n                    'id': i,\n                    'bbox': bbox,\n                    'combined_affordances': combined_affordances,\n                    'action_potential': self._calculate_action_potential(combined_affordances)\n                })\n            \n            results.append(affordances_result)\n        \n        return results\n    \n    def _extract_roi_features(self, image, bbox):\n        \"\"\"\n        Extract features for a specific bounding box\n        \"\"\"\n        # In practice, this would use RoIAlign\n        # For now, return a placeholder\n        return torch.randn(1, 256)\n    \n    def _combine_affordances(self, basic, part, context):\n        \"\"\"\n        Combine different types of affordances\n        \"\"\"\n        # Simple weighted combination\n        combined = 0.4 * basic + 0.3 * part['affordances'] + 0.3 * context\n        \n        return {\n            'scores': combined,\n            'categories': affordance_categories,\n            'confidence': torch.mean(combined).item()\n        }\n    \n    def _calculate_action_potential(self, affordances):\n        \"\"\"\n        Calculate overall action potential for an object\n        \"\"\"\n        scores = affordances['scores']\n        \n        # Weight different affordances based on action potential\n        action_weights = torch.tensor([\n            0.8,  # graspable\n            0.7,  # movable\n            0.6,  # pourable\n            0.5,  # stackable\n            0.9,  # openable\n            0.9,  # closeable\n            0.4,  # sittable\n            0.9   # reachable\n        ]).to(scores.device)\n        \n        action_potential = torch.sum(scores * action_weights)\n        return action_potential.item()",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 30,
    "token_count": 814
  },
  {
    "chunk_id": "18518d40-1076-490c-94ef-b55b2a655075",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Understanding Object Affordances\n\nclass PartAffordanceDetector(nn.Module):\n    \"\"\"\n    Detects affordances based on object parts\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.part_detector = nn.Conv2d(3, 64, 3, padding=1)  # Simplified\n        self.affordance_predictor = nn.Linear(64, len(affordance_categories))\n        \n    def forward(self, image, bbox):\n        \"\"\"\n        Detect affordances from object parts\n        \"\"\"\n        # Extract region from image\n        x1, y1, x2, y2 = map(int, bbox)\n        region = image[:, y1:y2, x1:x2]\n        \n        if region.numel() == 0:\n            return {\n                'affordances': torch.zeros(len(affordance_categories)),\n                'parts': []\n            }\n        \n        # Resize to standard size\n        region = torch.nn.functional.interpolate(\n            region.unsqueeze(0), size=(64, 64), mode='bilinear'\n        ).squeeze(0)\n        \n        # Detect parts and predict affordances\n        part_features = self.part_detector(region)\n        part_affordances = torch.sigmoid(\n            self.affordance_predictor(\n                torch.mean(part_features, dim=[1, 2])  # Global average pooling\n            )\n        )\n        \n        return {\n            'affordances': part_affordances,\n            'parts': []  # In practice, would return detected parts\n        }",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 31,
    "token_count": 323
  },
  {
    "chunk_id": "25bd5f77-72f8-447c-88fe-49b93793a974",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Understanding Object Affordances\n\nclass ContextAffordancePredictor(nn.Module):\n    \"\"\"\n    Predicts affordances based on object context in the scene\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.context_encoder = nn.Linear(feature_dim, feature_dim)\n        self.affordance_predictor = nn.Linear(feature_dim, len(affordance_categories))\n        \n    def forward(self, object_info, scene_graph):\n        \"\"\"\n        Predict context-based affordances\n        \"\"\"\n        # Use scene graph to understand object's context\n        object_context = self._extract_context(object_info, scene_graph)\n        \n        if object_context is not None:\n            context_features = self.context_encoder(object_context)\n            context_affordances = torch.sigmoid(\n                self.affordance_predictor(context_features)\n            )\n            return context_affordances\n        else:\n            return torch.zeros(len(affordance_categories))",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 32,
    "token_count": 193
  },
  {
    "chunk_id": "6715d652-00fa-4b48-a3a0-7dae850c7d65",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Understanding Object Affordances\n\nclass ManipulationTargetDetector(nn.Module):\n    \"\"\"\n    Detects objects suitable for manipulation based on affordances and language\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.graspability_predictor = GraspabilityPredictor()\n        self.safety_checker = SafetyChecker()\n        \n    def forward(self, command, detections, affordances):\n        \"\"\"\n        Identify most suitable objects for manipulation based on command\n        \"\"\"\n        # Parse command to understand manipulation intent\n        manipulation_intent = self._parse_manipulation_intent(command)\n        \n        suitable_targets = []\n        \n        for i, (detection, obj_affordances) in enumerate(zip(detections, affordances)):\n            # Check if object matches manipulation intent\n            intent_match_score = self._check_intent_match(\n                manipulation_intent, detection, obj_affordances\n            )\n            \n            # Check graspability\n            graspability = self.graspability_predictor(\n                detection['box'], detection.get('attributes', {})\n            )\n            \n            # Check safety\n            safety_score = self.safety_checker.check_object_safety(\n                detection, obj_affordances\n            )\n            \n            # Calculate overall suitability\n            overall_score = (\n                0.4 * intent_match_score + \n                0.3 * graspability + \n                0.3 * safety_score\n            )\n            \n            if overall_score > 0.3:  # Threshold\n                suitable_targets.append({\n                    'object_id': i,\n                    'detection': detection,\n                    'affordances': obj_affordances,\n                    'suitability_score': overall_score,\n                    'intent_match': intent_match_score,\n                    'graspability': graspability,\n                    'safety': safety_score\n                })\n        \n        # Sort by suitability\n        suitable_targets.sort(key=lambda x: x['suitability_score'], reverse=True)\n        return suitable_targets\n    \n    def _parse_manipulation_intent(self, command):\n        \"\"\"\n        Parse manipulation intent from command\n        \"\"\"\n        command_lower = command.lower()\n        \n        intents = {\n            'grasp': any(word in command_lower for word in ['grasp', 'pick', 'take', 'lift']),\n            'move': any(word in command_lower for word in ['move', 'transport', 'carry']),\n            'place': any(word in command_lower for word in ['place', 'put', 'set', 'down']),\n            'open': any(word in command_lower for word in ['open', 'uncover']),\n            'close': any(word in command_lower for word in ['close', 'shut', 'cover']),\n            'pour': any(word in command_lower for word in ['pour', 'empty'])\n        }\n        \n        return intents\n    \n    def _check_intent_match(self, intent, detection, affordances):\n        \"\"\"\n        Check how well object matches manipulation intent\n        \"\"\"\n        score = 0.0\n        \n        if intent['grasp'] and affordances['scores'][0] > 0.5:  # graspable\n            score += 0.8\n        if intent['move'] and affordances['scores'][1] > 0.5:  # movable\n            score += 0.7\n        if intent['open'] and affordances['scores'][4] > 0.5:  # openable\n            score += 0.9\n        if intent['close'] and affordances['scores'][5] > 0.5:  # closeable\n            score += 0.9\n        if intent['pour'] and affordances['scores'][2] > 0.5:  # pourable\n            score += 0.8\n            \n        return min(score, 1.0)  # Clamp to [0, 1]",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 33,
    "token_count": 788
  },
  {
    "chunk_id": "d0b13e32-9d2f-42b1-adc2-69107f10ee9e",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Understanding Object Affordances\n\nclass GraspabilityPredictor(nn.Module):\n    \"\"\"\n    Predicts how graspable an object is\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.grasp_predictor = nn.Linear(10, 1)  # Simplified input features\n        \n    def forward(self, bbox, attributes):\n        \"\"\"\n        Predict graspability score for an object\n        \"\"\"\n        # Simple features: size, shape regularity, etc.\n        width = bbox[2] - bbox[0]\n        height = bbox[3] - bbox[1]\n        size = (width + height) / 2\n        \n        # Normalize features\n        features = torch.tensor([size, width/height, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n        \n        graspability = torch.sigmoid(self.grasp_predictor(features)).item()\n        return graspability",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 34,
    "token_count": 212
  },
  {
    "chunk_id": "66babcf0-b3e0-421e-9546-3e4d936587e0",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Understanding Object Affordances\n\nclass SafetyChecker(nn.Module):\n    \"\"\"\n    Checks safety of object interaction\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.dangerous_objects = ['knife', 'scissors', 'blade', 'fire', 'sharp']\n        \n    def check_object_safety(self, detection, affordances):\n        \"\"\"\n        Check if object interaction is safe\n        \"\"\"\n        # Check if object is dangerous\n        obj_class = detection.get('label', '').lower()\n        if any(danger in obj_class for danger in self.dangerous_objects):\n            return 0.1  # Very low safety\n        \n        # Check if object is too heavy based on affordances\n        if affordances['scores'][1] < 0.3:  # movable score\n            return 0.3  # Low safety for heavy objects\n        \n        return 0.9  # High safety for safe objects\n```",
    "chapter": 5,
    "section": "Understanding Object Affordances",
    "page": 35,
    "token_count": 196
  },
  {
    "chunk_id": "ec84c14b-ab82-4292-b0bb-8996cab9ff2b",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Real-Time Processing and Optimization",
    "chapter": 5,
    "section": "Real-Time Processing and Optimization",
    "page": 36,
    "token_count": 6
  },
  {
    "chunk_id": "576fa45f-e763-4f1e-8f44-02e5c814e7b2",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Efficient Vision Processing for Robotics\n\nReal-time requirements in robotics demand efficient vision processing:",
    "chapter": 5,
    "section": "Efficient Vision Processing for Robotics",
    "page": 37,
    "token_count": 17
  },
  {
    "chunk_id": "b44c9397-d56e-4c38-8c98-8f2736732557",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Efficient Vision Processing for Robotics\n\n```python\nclass EfficientVLAVisionProcessor:\n    \"\"\"\n    Efficient vision processor for real-time VLA applications\n    \"\"\"\n    def __init__(self, device='cuda'):\n        self.device = device\n        \n        # Use lightweight models for real-time processing\n        self.feature_extractor = self._load_lightweight_extractor()\n        self.object_detector = self._load_lightweight_detector()\n        self.grounding_module = VisualGroundingModule(320)  # Smaller feature dim\n        \n        # Caching mechanism for repeated processing\n        self.processing_cache = {}\n        self.cache_size_limit = 100\n        \n        # Multi-scale processing for different tasks\n        self.processing_scales = {\n            'detection': (640, 480),    # Full detection at medium resolution\n            'tracking': (320, 240),     # Fast tracking at low resolution  \n            'recognition': (640, 480),  # Recognition at medium resolution\n            'affordance': (160, 120)    # Affordance at very low resolution\n        }\n    \n    def _load_lightweight_extractor(self):\n        \"\"\"\n        Load a lightweight feature extractor\n        \"\"\"\n        # Use MobileNet or EfficientNet for efficiency\n        from torchvision.models import mobilenet_v2\n        model = mobilenet_v2(pretrained=True)\n        \n        # Remove classification head, keep feature extractor\n        feature_extractor = nn.Sequential(*list(model.children())[:-1])\n        return feature_extractor.to(self.device)\n    \n    def _load_lightweight_detector(self):\n        \"\"\"\n        Load a lightweight object detector\n        \"\"\"\n        # Use a smaller backbone or quantized model\n        model = fasterrcnn_resnet50_fpn(pretrained=True)\n        \n        # In practice, you might want to use a quantized version\n        # or a model specifically designed for edge deployment\n        return model.to(self.device)\n    \n    def process_frame(self, image, command=None):\n        \"\"\"\n        Process a single frame efficiently\n        \"\"\"\n        # Convert to tensor and move to device\n        if isinstance(image, np.ndarray):\n            image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)\n        else:\n            image_tensor = image\n        \n        image_tensor = image_tensor.to(self.device) / 255.0  # Normalize\n        \n        # Create cache key if command is provided\n        cache_key = None\n        if command:\n            import hashlib\n            cache_key = hashlib.md5(\n                (str(image_tensor.shape) + command).encode()\n            ).hexdigest()\n            \n            if cache_key in self.processing_cache:\n                return self.processing_cache[cache_key]\n        \n        # Multi-scale processing\n        results = {}\n        \n        # Detection at appropriate scale\n        det_scale = self.processing_scales['detection']\n        det_image = self._resize_image(image_tensor, det_scale)\n        detections = self.object_detector([det_image])[0]\n        \n        results['detections'] = self._adjust_boxes_to_original(\n            detections, det_scale, image_tensor.shape[2:]\n        )\n        \n        # If command is provided, perform grounding\n        if command:\n            # Extract visual features\n            features = self.feature_extractor(\n                self._resize_image(image_tensor, (224, 224))\n            )\n            \n            # Perform visual grounding\n            grounding_result = self.grounding_module(\n                features, \n                self._encode_text(command), \n                {'boxes': results['detections'].boxes.xyx}\n            )\n            \n            results['grounding'] = grounding_result\n        \n        # Add to cache if there's a command\n        if cache_key:\n            self._add_to_cache(cache_key, results)\n        \n        return results\n    \n    def _resize_image(self, image, target_size):\n        \"\"\"\n        Resize image to target size\n        \"\"\"\n        import torch.nn.functional as F\n        return F.interpolate(image, size=target_size, mode='bilinear', align_corners=False)\n    \n    def _adjust_boxes_to_original(self, detections, current_scale, original_scale):\n        \"\"\"\n        Adjust bounding boxes from current scale to original scale\n        \"\"\"\n        # Calculate scale factors\n        h_scale = original_scale[0] / current_scale[0]\n        w_scale = original_scale[1] / current_scale[1]\n        \n        # Adjust boxes\n        if hasattr(detections, 'boxes'):\n            detections.boxes = detections.boxes * torch.tensor([w_scale, h_scale, w_scale, h_scale]).to(detections.boxes.device)\n        \n        return detections\n    \n    def _encode_text(self, text):\n        \"\"\"\n        Encode text for grounding (simplified)\n        \"\"\"\n        # In practice, use a proper tokenizer and encoder\n        # This is just a placeholder\n        return torch.randn(1, 10, 320)  # [batch, seq_len, feature_dim]\n    \n    def _add_to_cache(self, key, value):\n        \"\"\"\n        Add result to cache with size management\n        \"\"\"\n        if len(self.processing_cache) >= self.cache_size_limit:\n            # Remove oldest entries\n            oldest_key = next(iter(self.processing_cache))\n            del self.processing_cache[oldest_key]\n        \n        self.processing_cache[key] = value",
    "chapter": 5,
    "section": "Efficient Vision Processing for Robotics",
    "page": 38,
    "token_count": 1091
  },
  {
    "chunk_id": "b6a542c2-8ebf-4f90-8166-ec0305d87b70",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Efficient Vision Processing for Robotics\n\nclass AdaptiveVisionProcessor:\n    \"\"\"\n    Vision processor that adapts to computational constraints\n    \"\"\"\n    def __init__(self, max_processing_time=0.1):  # 100ms per frame\n        self.max_processing_time = max_processing_time\n        self.current_quality_setting = 'high'\n        self.processing_times = []\n        \n        # Different quality settings with processing speed estimates\n        self.quality_settings = {\n            'high': {\n                'resolution': (1280, 720),\n                'detection_threshold': 0.5,\n                'expected_time': 0.15\n            },\n            'medium': {\n                'resolution': (640, 480),\n                'detection_threshold': 0.4,\n                'expected_time': 0.08\n            },\n            'low': {\n                'resolution': (320, 240),\n                'detection_threshold': 0.3,\n                'expected_time': 0.03\n            }\n        }\n    \n    def process_with_adaptation(self, image, command=None):\n        \"\"\"\n        Process image with adaptive quality based on timing constraints\n        \"\"\"\n        import time\n        \n        start_time = time.time()\n        \n        # Try current quality setting\n        result = self._process_at_quality(image, self.current_quality_setting, command)\n        \n        processing_time = time.time() - start_time\n        self.processing_times.append(processing_time)\n        \n        # Adjust quality based on timing\n        self._adjust_quality_setting(processing_time)\n        \n        return result\n    \n    def _process_at_quality(self, image, quality, command=None):\n        \"\"\"\n        Process image at specified quality level\n        \"\"\"\n        settings = self.quality_settings[quality]\n        \n        # Resize image according to quality setting\n        h, w = settings['resolution']\n        resized_image = self._resize_image(image, (h, w))\n        \n        # Process with appropriate parameters\n        return self._efficient_process(resized_image, command, settings)\n    \n    def _adjust_quality_setting(self, processing_time):\n        \"\"\"\n        Adjust quality setting based on processing time\n        \"\"\"\n        avg_time = np.mean(self.processing_times[-10:]) if self.processing_times else processing_time\n        \n        if avg_time > self.max_processing_time * 0.9:  # 90% threshold\n            # Need to reduce quality\n            if self.current_quality_setting == 'high':\n                self.current_quality_setting = 'medium'\n            elif self.current_quality_setting == 'medium':\n                self.current_quality_setting = 'low'\n        elif avg_time < self.max_processing_time * 0.7:  # 70% threshold\n            # Can increase quality\n            if self.current_quality_setting == 'low':\n                self.current_quality_setting = 'medium'\n            elif self.current_quality_setting == 'medium':\n                self.current_quality_setting = 'high'",
    "chapter": 5,
    "section": "Efficient Vision Processing for Robotics",
    "page": 39,
    "token_count": 593
  },
  {
    "chunk_id": "738e9cf4-dcae-4ea4-b296-ee1923043f07",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Efficient Vision Processing for Robotics\n\n# Example usage of efficient processing\ndef example_efficient_processing():\n    \"\"\"\n    Example of efficient vision processing for VLA\n    \"\"\"\n    processor = EfficientVLAVisionProcessor()\n    \n    # Simulate processing a video stream\n    import time\n    \n    for frame_idx in range(10):  # Process 10 frames\n        # Simulate getting a frame from camera\n        frame = torch.randn(1, 3, 480, 640)  # Dummy frame\n        \n        start_time = time.time()\n        \n        # Process frame\n        results = processor.process_frame(\n            frame, \n            command=\"pick up the red cup\" if frame_idx % 3 == 0 else None\n        )\n        \n        processing_time = time.time() - start_time\n        \n        print(f\"Frame {frame_idx}: Processed in {processing_time:.3f}s\")\n        print(f\"  Detections: {len(results.get('detections', []))}\")\n        if 'grounding' in results:\n            print(f\"  Grounding performed\")\n        \n        # Simulate real-time constraint\n        time.sleep(max(0, 0.1 - processing_time))  # Maintain ~10 FPS\n```",
    "chapter": 5,
    "section": "Efficient Vision Processing for Robotics",
    "page": 40,
    "token_count": 253
  },
  {
    "chunk_id": "67e82c2c-8ed1-49f6-ac39-af9db8b9b5f2",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Integration with Language Understanding",
    "chapter": 5,
    "section": "Integration with Language Understanding",
    "page": 41,
    "token_count": 5
  },
  {
    "chunk_id": "10d31075-2236-4c0b-bc2b-1e64d55d9873",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Vision-Language Fusion for VLA\n\nCombining vision and language processing for effective VLA systems:",
    "chapter": 5,
    "section": "Vision-Language Fusion for VLA",
    "page": 42,
    "token_count": 20
  },
  {
    "chunk_id": "60b3ec4a-6fb8-4a6e-8fa9-4b491662b13e",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision-Language Fusion for VLA\n\n```python\nclass VisionLanguageFusionModule(nn.Module):\n    \"\"\"\n    Fuses vision and language processing for VLA systems\n    \"\"\"\n    def __init__(self, feature_dim=768):\n        super().__init__()\n        \n        # Vision-language attention\n        self.vision_language_attention = CrossModalAttention(feature_dim)\n        \n        # Multimodal fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim, feature_dim)\n        )\n        \n        # Task-specific heads\n        self.detection_refinement_head = DetectionRefinementHead(feature_dim)\n        self.action_prediction_head = ActionPredictionHead(feature_dim)\n        self.safety_validation_head = SafetyValidationHead(feature_dim)\n    \n    def forward(self, vision_features, language_features, detections, scene_graph):\n        \"\"\"\n        Fuse vision and language features for VLA tasks\n        \"\"\"\n        batch_size = vision_features.size(0)\n        results = []\n        \n        for b in range(batch_size):\n            # Cross-modal attention between vision and language\n            fused_features, attention_weights = self.vision_language_attention(\n                vision_features[b],\n                language_features[b]\n            )\n            \n            # Refine object detections based on language\n            refined_detections = self.detection_refinement_head(\n                detections[b], fused_features, language_features[b]\n            )\n            \n            # Predict possible actions based on multimodal input\n            action_predictions = self.action_prediction_head(\n                fused_features, scene_graph[b]\n            )\n            \n            # Validate safety of proposed actions\n            safety_validation = self.safety_validation_head(\n                action_predictions, scene_graph[b]\n            )\n            \n            results.append({\n                'fused_features': fused_features,\n                'refined_detections': refined_detections,\n                'action_predictions': action_predictions,\n                'safety_validation': safety_validation,\n                'attention_weights': attention_weights\n            })\n        \n        return results",
    "chapter": 5,
    "section": "Vision-Language Fusion for VLA",
    "page": 43,
    "token_count": 414
  },
  {
    "chunk_id": "1ebe1098-2d12-4cbd-a23e-30bf7772d236",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision-Language Fusion for VLA\n\nclass DetectionRefinementHead(nn.Module):\n    \"\"\"\n    Refines object detections based on language input\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.refinement_network = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim),  # Vision + Language\n            nn.ReLU(),\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 2, 4)  # Bounding box refinement (dx, dy, dw, dh)\n        )\n        \n    def forward(self, detections, vision_features, language_features):\n        \"\"\"\n        Refine detections based on language guidance\n        \"\"\"\n        refined_detections = []\n        \n        for i, detection in enumerate(detections):\n            # Average features for this detection\n            if i < vision_features.size(0):\n                vis_feat = vision_features[i]\n                \n                # Combine with language features\n                combined_feat = torch.cat([vis_feat, language_features.mean(dim=0)])\n                \n                # Predict bounding box refinement\n                bbox_refinement = self.refinement_network(combined_feat)\n                \n                # Apply refinement to original bounding box\n                orig_box = detection['box']\n                refined_box = self._apply_refinement(orig_box, bbox_refinement)\n                \n                refined_detection = detection.copy()\n                refined_detection['refined_box'] = refined_box\n                \n                refined_detections.append(refined_detection)\n        \n        return refined_detections\n    \n    def _apply_refinement(self, original_box, refinement):\n        \"\"\"\n        Apply bounding box refinement parameters\n        \"\"\"\n        x1, y1, x2, y2 = original_box\n        center_x = (x1 + x2) / 2\n        center_y = (y1 + y2) / 2\n        width = x2 - x1\n        height = y2 - y1\n        \n        dx, dy, dw, dh = refinement\n        \n        new_center_x = center_x + dx\n        new_center_y = center_y + dy\n        new_width = width * (1 + dw)\n        new_height = height * (1 + dh)\n        \n        new_x1 = new_center_x - new_width / 2\n        new_y1 = new_center_y - new_height / 2\n        new_x2 = new_center_x + new_width / 2\n        new_y2 = new_center_y + new_height / 2\n        \n        return torch.tensor([new_x1, new_y1, new_x2, new_y2])",
    "chapter": 5,
    "section": "Vision-Language Fusion for VLA",
    "page": 44,
    "token_count": 530
  },
  {
    "chunk_id": "eae8c934-247d-481b-8670-8dbae441fda6",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision-Language Fusion for VLA\n\nclass ActionPredictionHead(nn.Module):\n    \"\"\"\n    Predicts potential actions based on multimodal input\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.action_predictor = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim // 2, len(action_types))\n        )\n        \n    def forward(self, fused_features, scene_graph):\n        \"\"\"\n        Predict possible actions based on scene understanding\n        \"\"\"\n        # Aggregate scene features\n        scene_features = torch.mean(fused_features, dim=0)\n        \n        # Predict action probabilities\n        action_logits = self.action_predictor(scene_features)\n        action_probs = torch.softmax(action_logits, dim=-1)\n        \n        # Get top actions\n        top_actions = torch.topk(action_probs, k=5)\n        \n        return {\n            'action_probs': action_probs,\n            'top_actions': [\n                (action_types[idx.item()], prob.item()) \n                for idx, prob in zip(top_actions.indices, top_actions.values)\n            ]\n        }",
    "chapter": 5,
    "section": "Vision-Language Fusion for VLA",
    "page": 45,
    "token_count": 238
  },
  {
    "chunk_id": "bbfaa83d-60f8-4082-b114-7a2c079a1ece",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision-Language Fusion for VLA\n\nclass SafetyValidationHead(nn.Module):\n    \"\"\"\n    Validates safety of predicted actions\n    \"\"\"\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.safety_predictor = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim // 2),\n            nn.ReLU(),\n            nn.Linear(feature_dim // 2, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, action_predictions, scene_graph):\n        \"\"\"\n        Validate safety of predicted actions based on scene\n        \"\"\"\n        safety_scores = {}\n        \n        for action, prob in action_predictions['top_actions']:\n            # Use scene graph to validate action safety\n            safety_score = self._validate_action_safety(action, scene_graph)\n            \n            # Combine with action probability\n            overall_score = prob * safety_score\n            \n            safety_scores[action] = {\n                'action_probability': prob,\n                'safety_score': safety_score,\n                'overall_score': overall_score\n            }\n        \n        return safety_scores\n    \n    def _validate_action_safety(self, action, scene_graph):\n        \"\"\"\n        Validate if an action is safe in the current scene\n        \"\"\"\n        # Simple safety rules\n        if action in ['grasp', 'move'] and self._has_obstacles(scene_graph):\n            return 0.6  # Moderately safe\n        elif action in ['fast_move', 'jump'] and self._has_people_nearby(scene_graph):\n            return 0.2  # Not safe\n        else:\n            return 0.9  # Safe\n        \n        return 0.5  # Default safety\n    \n    def _has_obstacles(self, scene_graph):\n        \"\"\"\n        Check if scene has obstacles\n        \"\"\"\n        # Simplified check\n        return len([node for node in scene_graph['nodes'] \n                   if node['class'] in ['wall', 'furniture', 'person']]) > 0\n    \n    def _has_people_nearby(self, scene_graph):\n        \"\"\"\n        Check if scene has people nearby\n        \"\"\"\n        return len([node for node in scene_graph['nodes'] \n                   if node['class'] in ['person', 'human']]) > 0",
    "chapter": 5,
    "section": "Vision-Language Fusion for VLA",
    "page": 46,
    "token_count": 454
  },
  {
    "chunk_id": "1bcade07-975f-4864-bfc4-d6bf2d57a211",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision-Language Fusion for VLA\n\n# Define action types for VLA systems\naction_types = [\n    'navigate', 'grasp', 'move', 'place', 'open', 'close',\n    'pour', 'lift', 'lower', 'push', 'pull', 'rotate',\n    'wait', 'stop', 'continue'\n]\n```",
    "chapter": 5,
    "section": "Vision-Language Fusion for VLA",
    "page": 47,
    "token_count": 72
  },
  {
    "chunk_id": "59a858a0-6f93-4281-b454-d697591decb7",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Diagrams",
    "chapter": 5,
    "section": "Diagrams",
    "page": 48,
    "token_count": 3
  },
  {
    "chunk_id": "da6fc429-1359-48bd-ad8c-94b702c55acb",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Vision Processing Architecture for VLA",
    "chapter": 5,
    "section": "Vision Processing Architecture for VLA",
    "page": 49,
    "token_count": 7
  },
  {
    "chunk_id": "9309576e-1ad1-42b1-aa3f-cd45db4b2801",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Vision Processing Architecture for VLA\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    VLA VISION PROCESSING                        │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐         │\n│  │  INPUT      │───▶│  EFFICIENT  │───▶│  MULTI-     │         │\n│  │  IMAGE      │    │  FEATURE    │    │  MODAL      │         │\n│  │  STREAM     │    │  EXTRACTION │    │  FUSION     │         │\n│  │             │    │ • MobileNet │    │ • Attention │         │\n│  │ • RGB       │    │ • Efficient │    │ • Language  │         │\n│  │ • Depth     │    │   Networks  │    │   Fusion    │         │\n│  └─────────────┘    └─────────────┘    │ • Grounding │         │\n│         │                   │           └─────────────┘         │\n│         ▼                   ▼                   │               │\n│  ┌─────────────┐    ┌─────────────┐             ▼               │\n│  │  OBJECT     │    │  SCENE      │    ┌─────────────────────┐ │\n│  │  DETECTION  │    │  UNDERSTANDING│    │  TASK-SPECIFIC    │ │\n│  │  &          │    │  &          │    │  HEADS            │ │\n│  │  RECOGNITION│    │  SPATIAL     │    │ • Action Planning │ │\n│  │ • YOLOv5    │    │  REASONING   │    │ • Safety Validation││\n│  │ • Efficient │    │ • Affordance │    │ • Manipulation    │ │\n│  │   Detectors │    │   Detection  │    │   Target Selection│ │\n│  └─────────────┘    └─────────────┘    └─────────────────────┘ │\n│         │                   │                       │           │\n│         └───────────────────┼───────────────────────┘           │\n│                             ▼                                   │\n│  ┌─────────────────────────────────────────────────────────┐   │\n│  │                VISUAL GROUNDING                         │   │\n│  │  ┌─────────────────┐ ┌─────────────────┐ ┌───────────┐ │   │\n│  │  │ Referring       │ │ Attribute       │ │ Spatial   │ │   │\n│  │  │ Expression      │ │ Matching        │ │ Reasoning │ │   │\n│  │  │ Resolution      │ │ & Semantic      │ │ &         │ │   │\n│  │  │ & Visual        │ │ Grounding       │ │ Relationship│ │   │\n│  │  │ Correspondence  │ │ Integration     │ │ Analysis  │ │   │\n│  │  └─────────────────┘ └─────────────────┘ └───────────┘ │   │\n│  └─────────────────────────────────────────────────────────┘   │\n└─────────────────────────────────────────────────────────────────┘\n```",
    "chapter": 5,
    "section": "Vision Processing Architecture for VLA",
    "page": 50,
    "token_count": 756
  },
  {
    "chunk_id": "268ac68a-d5ac-4f58-8bfb-94b73fd0bb41",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Real-Time Processing Pipeline\n\n```\nCamera Input ──▶ Preprocessing ──▶ Efficient Feature ──▶ Multi-Task\n(640x480)        (Normalization,     Extraction         Processing\n                  Augmentation)      (MobileNet)        (Detection, \n                                                      Grounding, \n                                                      Affordance)\n     │                                    │                │\n     ▼                                    ▼                ▼\nFrame Buffer ──▶ Resolution  ──▶ Lightweight ──▶ Parallel Processing\n              │  Adaptation   │  Networks    │  (Vision-Language\n              │               │              │   Fusion, Safety)\n     │        ▼               ▼              ▼\n     └─▶ Quality ─────▶ Efficient ─────▶ Real-Time ─────▶ Action\n         Control        Attention         Output          Planning\n         (FPS,         Mechanism        (10-30 FPS)      Decision\n         Latency)                        \n```",
    "chapter": 5,
    "section": "Real-Time Processing Pipeline",
    "page": 51,
    "token_count": 209
  },
  {
    "chunk_id": "8546e4cc-88da-4714-a5dd-f05e3692664a",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Design and implement efficient vision processing pipelines for VLA systems\n2. Integrate object detection and recognition with language grounding\n3. Implement affordance detection for action planning\n4. Optimize vision processing for real-time robotic applications\n5. Create multimodal fusion architectures that combine vision and language\n6. Evaluate vision processing performance in robotic contexts",
    "chapter": 5,
    "section": "Learning Outcomes",
    "page": 52,
    "token_count": 86
  },
  {
    "chunk_id": "918552a5-eec3-4438-b603-ffdde10610a3",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Advanced Topics",
    "chapter": 5,
    "section": "Advanced Topics",
    "page": 53,
    "token_count": 3
  },
  {
    "chunk_id": "a5668b7d-c6ea-4187-8e05-bd169af31d5a",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### 3D Vision for VLA Systems\n\n- Depth estimation and 3D scene reconstruction\n- Multi-view geometry for enhanced scene understanding\n- 3D object detection and manipulation planning",
    "chapter": 5,
    "section": "3D Vision for VLA Systems",
    "page": 54,
    "token_count": 38
  },
  {
    "chunk_id": "238bdac2-0715-4c06-b29c-4ab86263aff1",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Continual Learning in Vision Processing\n\n- Online learning for adapting to new environments\n- Memory-efficient learning in robotic systems\n- Transfer learning between similar tasks",
    "chapter": 5,
    "section": "Continual Learning in Vision Processing",
    "page": 55,
    "token_count": 31
  },
  {
    "chunk_id": "96245476-1e73-4f86-8712-cbcf64625372",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "### Active Vision and Attention\n\n- Active perception planning for information gathering\n- Task-driven visual attention mechanisms\n- Eye-hand coordination in visual processing",
    "chapter": 5,
    "section": "Active Vision and Attention",
    "page": 56,
    "token_count": 28
  },
  {
    "chunk_id": "d5af334d-270e-4c46-a11f-fede41ebca0f",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## Further Reading\n\n- \"Efficient Vision Processing for Robotics: A Survey\" (IEEE Transactions on Robotics, 2024) [1]\n- \"3D Scene Understanding for Vision-Language-Action Systems\" (Computer Vision and Pattern Recognition, 2024) [2]\n- \"Real-Time Multimodal Processing for Robotic Applications\" (International Conference on Robotics and Automation, 2024) [3]\n\n---",
    "chapter": 5,
    "section": "Further Reading",
    "page": 57,
    "token_count": 85
  },
  {
    "chunk_id": "2c0c85ee-94da-4a35-8dac-e86e2181f833",
    "doc_id": "090b0b1d-e815-4c2a-88d2-9bb7aeadd694",
    "chunk_text": "## References\n\n[1] Kim, S., et al. \"Efficient Vision Processing for Resource-Constrained Robotic Systems.\" IEEE Transactions on Robotics, vol. 40, no. 3, pp. 567-582, 2024.\n\n[2] Chen, X., et al. \"3D Scene Understanding for Natural Language-Guided Robotics.\" IEEE/CVF Computer Vision and Pattern Recognition, pp. 1234-1243, 2024.\n\n[3] Rodriguez, A., et al. \"Real-Time Multimodal Processing Architecture for Interactive Robotics.\" IEEE International Conference on Robotics and Automation, pp. 2105-2112, 2024.",
    "chapter": 5,
    "section": "References",
    "page": 58,
    "token_count": 140
  },
  {
    "chunk_id": "9c125ddf-9d22-4f17-be44-764b80f50b8e",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "# Section 2: Vision-Language-Action (VLA) Architecture",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "68278198-fa45-404b-8b31-8e28d0fc3209",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "## Conceptual Framework\n\nVision-Language-Action (VLA) architecture represents a sophisticated approach to multimodal AI that tightly couples visual perception, natural language understanding, and robotic action execution. This architecture creates an end-to-end system where a robot can receive natural language commands, perceive its environment visually, and execute appropriate physical actions in response, forming a complete perception-action cycle driven by natural language.\n\nThe VLA architecture is built on the principle that language understanding must be grounded in visual perception and that actions must be contextualized by both linguistic and visual information. This creates a triadic relationship where the meaning of language is clarified by visual context, the interpretation of visual scenes is guided by linguistic context, and the selection of actions is informed by both visual and linguistic inputs.",
    "chapter": 5,
    "section": "Conceptual Framework",
    "page": 2,
    "token_count": 152
  },
  {
    "chunk_id": "edba9cdd-5c91-4021-9379-5d8754e16b19",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "## Core Components of VLA Architecture",
    "chapter": 5,
    "section": "Core Components of VLA Architecture",
    "page": 3,
    "token_count": 7
  },
  {
    "chunk_id": "ccccaece-cce3-4d7c-ab9f-9239ad7e7721",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### 1. Visual Perception Module\n\nThe visual perception module serves as the sensory input system for the VLA architecture:\n\n**Object Detection and Recognition**: Identifies and classifies objects in the robot's field of view, providing semantic labels and bounding boxes that connect visual elements to linguistic concepts.\n\n**Scene Understanding**: Interprets the spatial layout and semantic meaning of environments, identifying navigable areas, functional spaces, and object relationships.\n\n**Visual Feature Extraction**: Processes raw visual input to extract high-level features that can be integrated with linguistic information, often using pre-trained vision models like those from the CLIP family.\n\n**Affordance Detection**: Identifies what actions are possible with specific objects based on their visual appearance and configuration, connecting perception to action possibilities.",
    "chapter": 5,
    "section": "1. Visual Perception Module",
    "page": 4,
    "token_count": 151
  },
  {
    "chunk_id": "10a71484-bc39-42cb-98d7-6f2fbf58689e",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### 2. Language Understanding Module\n\nThe language understanding module interprets and contextualizes natural language commands:\n\n**Natural Language Processing**: Parses linguistic input to extract semantic meaning, including action verbs, object references, spatial relationships, and contextual constraints.\n\n**Command Decomposition**: Breaks complex linguistic commands into more specific sub-actions that can be mapped to robotic capabilities.\n\n**Context Integration**: Incorporates environmental and situational context to disambiguate language and refine command interpretation.\n\n**Intent Recognition**: Identifies the underlying goals and intentions behind linguistic commands, enabling more flexible and adaptive behavior.",
    "chapter": 5,
    "section": "2. Language Understanding Module",
    "page": 5,
    "token_count": 115
  },
  {
    "chunk_id": "b1d223fe-84cd-481f-b7af-d1c0321a19cb",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### 3. Action Planning and Execution Module\n\nThe action module bridges linguistic interpretation and physical action:\n\n**Action Mapping**: Translates high-level linguistic commands and visual understanding into specific robotic behavior, whether navigation, manipulation, or interaction.\n\n**Path Planning**: Plans trajectories and sequences of actions based on environmental constraints and command requirements.\n\n**Skill Execution**: Calls upon learned motor skills and manipulation primitives to execute specific tasks.\n\n**Feedback Integration**: Incorporates sensory feedback to adjust and refine ongoing action execution.",
    "chapter": 5,
    "section": "3. Action Planning and Execution Module",
    "page": 6,
    "token_count": 97
  },
  {
    "chunk_id": "bf993e65-fb82-4087-a13d-a4d253eb85d0",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "## Architectural Patterns",
    "chapter": 5,
    "section": "Architectural Patterns",
    "page": 7,
    "token_count": 4
  },
  {
    "chunk_id": "561f3ce4-7b1e-44f9-aab8-eaec53608ba8",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Sequential Processing Pattern\n\nIn sequential processing, information flows through the system in stages:\n\n```\nLanguage Input → Visual Processing → Action Planning → Execution\n```\n\nThis pattern processes each modality sequentially, which can be easier to implement but may miss important cross-modal interactions that occur in natural human-robot interaction.",
    "chapter": 5,
    "section": "Sequential Processing Pattern",
    "page": 8,
    "token_count": 62
  },
  {
    "chunk_id": "112ad29c-cc51-473e-b294-0f3b9bb35e4b",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Parallel Processing with Fusion\n\nParallel processing allows each modality to be processed simultaneously before integration:\n\n```\nLanguage ──┐\nVisual   ──┼→ Fusion → Action Planning → Execution\nAction   ──┘\n```\n\nThis approach can be more efficient and allows for better integration of information from different modalities.",
    "chapter": 5,
    "section": "Parallel Processing with Fusion",
    "page": 9,
    "token_count": 71
  },
  {
    "chunk_id": "5f327d22-ccea-4b44-9b25-78f0d0ff46b0",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### End-to-End Learning Architecture\n\nModern VLA systems often use end-to-end learning where all components are jointly optimized:\n\n```\nVisual Input + Language Command → Direct Action Output (with intermediate representations)\n```\n\nThis approach can learn complex multimodal relationships but requires large amounts of training data.",
    "chapter": 5,
    "section": "End-to-End Learning Architecture",
    "page": 10,
    "token_count": 59
  },
  {
    "chunk_id": "a7a36ac3-117a-44d0-b274-99fbf18d3a88",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Modular Architecture with Specialized Components\n\nThe modular approach uses specialized components that can be developed and optimized independently:\n\n```\n[Perception Module] → [Language Module] → [Action Module]\n     ↑                    ↑                   ↑\n   Cameras             NLP Models         Motor Skills\n   LIDAR              LLM Integration     Navigation\n   Tactile            Dialogue Mgmt       Control\n```",
    "chapter": 5,
    "section": "Modular Architecture with Specialized Components",
    "page": 11,
    "token_count": 79
  },
  {
    "chunk_id": "2481658f-8dd9-43db-a481-ae0a4e070eca",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "## Technical Implementation",
    "chapter": 5,
    "section": "Technical Implementation",
    "page": 12,
    "token_count": 3
  },
  {
    "chunk_id": "5a8bfc8a-4483-4017-9816-74b8dbb31b44",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Feature Representation\n\nVLA systems need to represent features from different modalities in compatible ways:\n\n**Unified Embedding Spaces**: Common representation spaces where visual and linguistic features can be directly compared and combined.\n\n**Cross-Modal Attention**: Attention mechanisms that allow information from one modality to guide processing in another modality.\n\n**Hierarchical Representations**: Multiple levels of abstraction that capture both low-level sensory information and high-level semantic concepts.",
    "chapter": 5,
    "section": "Feature Representation",
    "page": 13,
    "token_count": 86
  },
  {
    "chunk_id": "2df9b897-0d98-441f-b3b9-b69e3a0409bf",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Temporal Considerations\n\nVLA systems must handle temporal aspects of both perception and action:\n\n**Sequence Processing**: Handling temporal sequences in both language and visual streams.\n\n**Action Timing**: Coordinating the timing of actions with the progression of linguistic and visual information.\n\n**Memory Integration**: Maintaining temporal context across sequences of actions and language exchanges.",
    "chapter": 5,
    "section": "Temporal Considerations",
    "page": 14,
    "token_count": 69
  },
  {
    "chunk_id": "b64014b6-9734-4a80-86ec-671634fe40df",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "## VLA Architecture Variants",
    "chapter": 5,
    "section": "VLA Architecture Variants",
    "page": 15,
    "token_count": 6
  },
  {
    "chunk_id": "90d2d10d-bf9d-4a53-ab94-e647685e6d03",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Centralized Architecture\n\nA single central controller manages integration across all modalities:\n\n```\n              ┌─────────────────┐\n              │ Central VLA     │\n              │ Controller      │\n              └─────────┬───────┘\n                        │\n        ┌───────────────┼───────────────┐\n        │               │               │\n    Visual          Language        Action\n   Processing      Processing      Planning\n```\n\nAdvantages: Easy coordination, centralized optimization\nDisadvantages: Potential bottleneck, single point of failure",
    "chapter": 5,
    "section": "Centralized Architecture",
    "page": 16,
    "token_count": 116
  },
  {
    "chunk_id": "66f1ae8d-2ac2-4634-b5cb-e1e301eef489",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Distributed Architecture\n\nProcessing is distributed across specialized modules:\n\n```\n    Visual Processing ──┐\n                        ├── Fusion ── Action Planning\n    Language Processing ──┘\n```\n\nAdvantages: Scalability, fault tolerance, specialized optimization\nDisadvantages: Coordination complexity, communication overhead",
    "chapter": 5,
    "section": "Distributed Architecture",
    "page": 17,
    "token_count": 64
  },
  {
    "chunk_id": "3057ce41-c1c9-4c0c-b984-61c37ed2cd26",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Hybrid Architecture\n\nCombines centralized coordination with distributed processing:\n\n```\n              ┌─────────────────┐\n              │   Coordinator   │\n              └─────────┬───────┘\n                        │\n        ┌───────────────┼───────────────┐\n        │               │               │\n    Visual          Language        Action\n   Module          Module          Module\n   (Local)         (Local)         (Local)\n```",
    "chapter": 5,
    "section": "Hybrid Architecture",
    "page": 18,
    "token_count": 96
  },
  {
    "chunk_id": "ff972718-c1fe-4c7b-86d2-8ed6bae4f77b",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "## Integration Challenges",
    "chapter": 5,
    "section": "Integration Challenges",
    "page": 19,
    "token_count": 3
  },
  {
    "chunk_id": "1b8eb449-1ec6-4544-ad38-7fb09372820d",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Cross-Modal Alignment\n\nEnsuring that visual and linguistic information refers to the same entities requires:\n\n**Entity Grounding**: Connecting linguistic references to visual entities and vice versa.\n\n**Spatial Registration**: Aligning visual and linguistic spatial information.\n\n**Temporal Synchronization**: Ensuring that linguistic and visual information corresponds to the same moments in time.",
    "chapter": 5,
    "section": "Cross-Modal Alignment",
    "page": 20,
    "token_count": 67
  },
  {
    "chunk_id": "de6ada1c-5e7d-41b5-a3a2-db87e671036a",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Scalability Considerations\n\nVLA architectures must scale to handle:\n\n**Growing Modality Sets**: Adding new sensor types while maintaining integration quality.\n\n**Complex Task Hierarchies**: Handling increasingly complex commands and tasks.\n\n**Environmental Complexity**: Operating effectively in more complex and varied environments.",
    "chapter": 5,
    "section": "Scalability Considerations",
    "page": 21,
    "token_count": 56
  },
  {
    "chunk_id": "990e6982-9d12-4281-bc18-4cbd143c69fe",
    "doc_id": "cbe8a0ca-b727-4018-8029-add484e9c208",
    "chunk_text": "### Safety and Robustness\n\nCritical considerations for real-world deployment:\n\n**Fail-Safe Mechanisms**: Graceful degradation when modality processing fails.\n\n**Validation Checks**: Verifying that planned actions are safe and appropriate.\n\n**Human Oversight**: Maintaining the ability for human intervention and correction.",
    "chapter": 5,
    "section": "Safety and Robustness",
    "page": 22,
    "token_count": 58
  },
  {
    "chunk_id": "bb984ecd-07dd-41c9-ad37-56686e56f6d2",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "# Chapter 5 Conclusion: Vision-Language-Action Systems",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 11
  },
  {
    "chunk_id": "77f00031-e2a6-4d2e-aeb5-b72729b3293b",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Summary of Key Concepts\n\nVision-Language-Action (VLA) systems represent the ultimate integration in Physical AI, creating a seamless pipeline from natural language commands to physical robot actions. This chapter has explored the fundamental components, architectures, and practical implementations of VLA systems that enable robots to understand human language, perceive their environment visually, and execute appropriate physical actions in response.\n\nThe VLA architecture creates an end-to-end system where language understanding is grounded in visual perception, and action execution is informed by both linguistic and visual context. This triadic relationship ensures that robots can operate effectively in unstructured, real-world environments while maintaining intuitive human-robot interaction through natural language interfaces.",
    "chapter": 5,
    "section": "Summary of Key Concepts",
    "page": 2,
    "token_count": 134
  },
  {
    "chunk_id": "d973c013-53d4-47f7-b448-51d49820b6de",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Key Takeaways",
    "chapter": 5,
    "section": "Key Takeaways",
    "page": 3,
    "token_count": 4
  },
  {
    "chunk_id": "63781b4c-25fe-4887-ad9f-47ac20a4106f",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Architectural Understanding\n\nThe VLA architecture creates an end-to-end system where language understanding is grounded in visual perception, and action execution is informed by both linguistic and visual information. This integration ensures that robots can operate effectively in unstructured, real-world environments while maintaining intuitive human-robot interaction.",
    "chapter": 5,
    "section": "Architectural Understanding",
    "page": 4,
    "token_count": 59
  },
  {
    "chunk_id": "e610f77d-69d5-4eca-9e6a-65d09d3d9f60",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Multimodal Integration\n\nSuccessful VLA systems must effectively integrate information from multiple modalities. This involves not just processing each modality separately but creating unified representations that capture the relationships between visual, linguistic, and action spaces. The quality of this integration directly impacts the robot's ability to understand and execute complex, natural language commands.",
    "chapter": 5,
    "section": "Multimodal Integration",
    "page": 5,
    "token_count": 66
  },
  {
    "chunk_id": "876be494-ea37-41bb-b219-55578a6a1cfc",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Practical Implementation Considerations\n\nReal-world VLA deployment requires careful attention to computational efficiency, real-time processing, and safety considerations. Modern implementations leverage GPU acceleration to achieve the processing speeds necessary for natural interaction while maintaining the accuracy needed for safe robot operation.",
    "chapter": 5,
    "section": "Practical Implementation Considerations",
    "page": 6,
    "token_count": 50
  },
  {
    "chunk_id": "a22ae7b0-ca36-4371-acb9-9b1518fd99c2",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Current State of Technology",
    "chapter": 5,
    "section": "Current State of Technology",
    "page": 7,
    "token_count": 5
  },
  {
    "chunk_id": "cc4a8fea-7da6-45ed-856d-bf256f1f2df7",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Achievements\n\nThe current state of VLA technology has achieved remarkable progress in several areas:\n\n**Natural Language Understanding**: Modern large language models integrated with vision systems can interpret complex, nuanced commands and decompose them into executable actions.\n\n**Perception Quality**: GPU-accelerated computer vision provides robust object detection, scene understanding, and affordance detection that grounds language in visual reality.\n\n**Real-time Performance**: Optimized architectures and processing pipelines can operate at speeds that enable natural human-robot interaction.\n\n**Safety Integration**: VLA systems now incorporate safety validation that ensures proposed actions are physically feasible and environmentally safe.",
    "chapter": 5,
    "section": "Achievements",
    "page": 8,
    "token_count": 122
  },
  {
    "chunk_id": "60f9a1e4-218e-45ab-9b82-129ba74fa735",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Remaining Challenges\n\nDespite significant progress, several challenges remain:\n\n**Robustness**: VLA systems can fail when encountering novel situations or environmental conditions not seen during training.\n\n**Generalization**: Many current systems work well in specific environments but struggle to transfer to new scenarios.\n\n**Interpretability**: The complex multimodal reasoning in VLA systems can be difficult to interpret and explain to users.\n\n**Scalability**: Deploying VLA capabilities at scale while maintaining performance remains challenging.",
    "chapter": 5,
    "section": "Remaining Challenges",
    "page": 9,
    "token_count": 96
  },
  {
    "chunk_id": "d2fd8272-9b42-4c5d-8523-1d99d6b0f859",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Future Directions",
    "chapter": 5,
    "section": "Future Directions",
    "page": 10,
    "token_count": 3
  },
  {
    "chunk_id": "993d17e4-f251-4746-a4d0-bc2ab255a6e3",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Emerging Technologies\n\nSeveral technological trends will shape the future development of VLA systems:\n\n**Foundation Models**: Large, general-purpose models trained on diverse data may enable better generalization across different tasks and environments.\n\n**Continual Learning**: Systems that can learn and adapt from ongoing interaction may enable more robust real-world deployment.\n\n**3D Understanding**: Better integration of 3D scene understanding with language may improve manipulation and navigation capabilities.\n\n**Multimodal Memory**: Systems with persistent memory that connects past experiences with current situations may enable more sophisticated reasoning.",
    "chapter": 5,
    "section": "Emerging Technologies",
    "page": 11,
    "token_count": 108
  },
  {
    "chunk_id": "6c6ce0ec-a717-45e4-b145-2fb6cf96ba83",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "### Application Domains\n\nVLA systems will likely expand into new application areas:\n\n**Healthcare**: Robots that can assist patients and medical staff with natural language interaction.\n\n**Education**: Educational robots that can adapt to different learning styles and provide personalized assistance.\n\n**Industrial Collaboration**: More sophisticated human-robot collaboration in manufacturing and logistics.\n\n**Domestic Assistance**: Household robots that can handle complex, varied tasks in unstructured home environments.",
    "chapter": 5,
    "section": "Application Domains",
    "page": 12,
    "token_count": 85
  },
  {
    "chunk_id": "0bf68da7-9a69-4e07-994e-eaca22b3805e",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Integration with Broader AI Trends\n\nVLA systems intersect with several broader trends in artificial intelligence:\n\n**Multimodal AI**: VLA represents a specific instance of the broader trend toward multimodal AI systems that can process and integrate multiple types of information.\n\n**Embodied AI**: The integration of AI with physical systems that interact with the real world.\n\n**Human-Centered AI**: Systems designed to work alongside humans, understanding natural language and adapting to human needs and preferences.",
    "chapter": 5,
    "section": "Integration with Broader AI Trends",
    "page": 13,
    "token_count": 94
  },
  {
    "chunk_id": "c5580e73-4956-4402-8921-e99d179a08bb",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Performance Evaluation and Benchmarks\n\nEvaluating VLA systems requires considering multiple dimensions:\n\n**Task Success Rate**: The percentage of commands successfully executed as intended.\n\n**Understanding Accuracy**: How accurately the system interprets language commands in the context of visual information.\n\n**Response Time**: The speed of system response, which affects the naturalness of interaction.\n\n**Safety Metrics**: Measures of safe system behavior, including collision avoidance and appropriate action validation.\n\n**User Satisfaction**: Subjective measures of how well the system meets user needs and expectations.",
    "chapter": 5,
    "section": "Performance Evaluation and Benchmarks",
    "page": 14,
    "token_count": 105
  },
  {
    "chunk_id": "cc6040ad-bc06-4ae9-9a40-94107d139caa",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Ethical and Social Considerations\n\nAs VLA systems become more prevalent, several ethical considerations become important:\n\n**Privacy**: Vision systems may capture sensitive information that needs appropriate handling and protection.\n\n**Bias**: Language models can perpetuate biases that may affect how robots interpret and respond to different individuals.\n\n**Accessibility**: Systems should be designed to accommodate users with different abilities and communication styles.\n\n**Transparency**: Users should understand the capabilities and limitations of VLA systems to set appropriate expectations.",
    "chapter": 5,
    "section": "Ethical and Social Considerations",
    "page": 15,
    "token_count": 96
  },
  {
    "chunk_id": "a5700426-f9b6-4c06-8ee7-22591de1b261",
    "doc_id": "7dc0b1d7-e671-4692-9d20-94bf6e54fd1f",
    "chunk_text": "## Looking Forward\n\nVision-Language-Action systems represent a critical step toward truly natural human-robot interaction. By integrating visual perception, natural language understanding, and physical action execution, these systems enable robots to operate more effectively in human environments and respond to human instructions in intuitive ways.\n\nThe success of VLA systems depends not only on advances in individual technologies but on the thoughtful integration of vision, language, and action capabilities. As these systems continue to evolve, they will enable more sophisticated and natural human-robot collaboration, making robots more useful and accessible across a wide range of applications.\n\nThe implementation examples and architectural patterns discussed in this chapter provide a foundation for building effective VLA systems. However, the field continues to evolve rapidly, and practitioners should stay informed about new developments in multimodal AI, robotics, and human-computer interaction.\n\nAs we move forward, the focus will increasingly be on creating VLA systems that are not only technically capable but also robust, safe, interpretable, and aligned with human values and needs. The future of robotics lies in systems that can understand and respond to human intentions expressed in natural language while operating safely and effectively in the physical world.",
    "chapter": 5,
    "section": "Looking Forward",
    "page": 16,
    "token_count": 232
  },
  {
    "chunk_id": "d8a6d028-22cd-419e-9c46-5cb7932c5559",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "# Introduction to Vision-Language-Action Systems",
    "chapter": 5,
    "section": "Introduction",
    "page": 1,
    "token_count": 8
  },
  {
    "chunk_id": "7261f96b-9619-49c5-9208-796f1a8dd4a1",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Overview\n\nVision-Language-Action (VLA) systems represent the cutting edge of artificial intelligence in robotics, integrating three fundamental capabilities: visual perception, natural language understanding, and robot action execution. These systems enable robots to interpret human language commands, perceive their environment through visual sensors, and execute appropriate physical actions in response. This integration creates a new paradigm for human-robot interaction where robots can understand and respond to natural language instructions in real-world environments.\n\nVLA systems address a critical challenge in robotics: how to create robots that can operate effectively in unstructured, human-designed environments using intuitive communication methods. Traditional robotics approaches required pre-programmed behaviors for specific tasks, but VLA systems can understand and execute novel commands in varied environments, making them more versatile and accessible to non-expert users.",
    "chapter": 5,
    "section": "Overview",
    "page": 2,
    "token_count": 158
  },
  {
    "chunk_id": "604086f8-356f-439b-80a1-1fa05e937157",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Key Concepts",
    "chapter": 5,
    "section": "Key Concepts",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "82aa6163-30ac-490f-b79c-9037bfed3b8a",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Multimodal Integration\n\nVLA systems represent a true integration of multiple AI modalities:\n\n**Visual Perception**: Processing camera images, depth data, and other visual inputs to understand the robot's environment and locate objects of interest.\n\n**Language Understanding**: Interpreting natural language commands, questions, and instructions from humans, converting linguistic concepts into actionable robot behaviors.\n\n**Action Execution**: Physical movement and manipulation in the real world, executing tasks that bridge the gap between language commands and physical outcomes.",
    "chapter": 5,
    "section": "Multimodal Integration",
    "page": 4,
    "token_count": 98
  },
  {
    "chunk_id": "bd4bfbc5-d0de-4821-af55-b2a0b6545e8c",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Core Components of VLA Systems\n\n**Perception Module**: Processes visual inputs using computer vision and deep learning techniques to identify objects, understand spatial relationships, and recognize environmental contexts.\n\n**Language Module**: Understands natural language inputs, parsing commands and queries to extract semantic meaning and identify intended robot behaviors.\n\n**Action Module**: Plans and executes physical movements, manipulations, and navigation tasks based on the interpreted language command and perceived environment.\n\n**Integration Layer**: Coordinates between the three modules, ensuring coherent interpretation of commands and appropriate action selection based on environmental constraints.",
    "chapter": 5,
    "section": "Core Components of VLA Systems",
    "page": 5,
    "token_count": 110
  },
  {
    "chunk_id": "c3b4e40e-c78f-4ff0-a025-0e5c7de54332",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### VLA System Architecture\n\nThe typical architecture of a VLA system includes:",
    "chapter": 5,
    "section": "VLA System Architecture",
    "page": 6,
    "token_count": 16
  },
  {
    "chunk_id": "5049f5e5-f8d3-4819-a7fb-75dc43f6654e",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## VLA System Architecture\n\n```\n┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   HUMAN         │    │   VLA SYSTEM    │    │   ROBOT         │\n│   USER          │    │                 │    │   ENVIRONMENT   │\n│                 │    │  ┌─────────────┐ │    │                 │\n│  ┌───────────┐  │    │  │ PERCEPTION  │ │    │  ┌───────────┐  │\n│  │Natural    │  │───▶│  │ Module      │ │───▶│  │Physical   │  │\n│  │Language   │  │    │  │ • Object    │ │    │  │Environment│  │\n│  │Command    │  │    │  │   Detection │ │    │  │• Objects  │  │\n│  └───────────┘  │    │  │ • Depth     │ │    │  │• Surfaces │  │\n│                 │    │  │   Analysis  │ │    │  │• Spaces   │  │\n└─────────────────┘    │  │ • Scene     │ │    │  └───────────┘  │\n                       │  │   Understanding││    │                 │\n                       │  └─────────────┘ │    │  ┌───────────┐  │\n                       │                  │    │  │ROBOT      │  │\n                       │  ┌─────────────┐ │    │  │• Manipulator│ │\n                       │  │ LANGUAGE    │ │    │  │• Mobile   │  │\n                       │  │ UNDERSTANDING│ │    │  │  Base     │  │\n                       │  │ Module      │ │    │  │• Sensors  │  │\n                       │  │ • Command   │ │    │  └───────────┘  │\n                       │  │   Parsing   │ │    │                 │\n                       │  │ • Task      │ │    │  ┌───────────┐  │\n                       │  │   Decomposition││    │  │TASK       │  │\n                       │  │ • Semantic  │ │    │  │EXECUTION  │  │\n                       │  │   Grounding │ │    │  │• Navigation│  │\n                       │  └─────────────┘ │    │  │• Manipulation│ │\n                       │                  │    │  │• Interaction│ │\n                       │  ┌─────────────┐ │    │  └───────────┘  │\n                       │  │ ACTION      │ │    │                 │\n                       │  │ PLANNING/   │ │    │                 │\n                       │  │ EXECUTION   │ │    │                 │\n                       │  │ Module      │ │    │                 │\n                       │  │ • Path      │ │    │                 │\n                       │  │   Planning  │ │    │                 │\n                       │  │ • Motion    │ │    │                 │\n                       │  │   Control   │ │    │                 │\n                       │  │ • Safety    │ │    │                 │\n                       │  │   Validation│ │    │                 │\n                       │  └─────────────┘ │    │                 │\n                       └─────────────────┘    └─────────────────┘\n```",
    "chapter": 5,
    "section": "VLA System Architecture",
    "page": 7,
    "token_count": 769
  },
  {
    "chunk_id": "a7c103bf-7deb-49be-894b-0aba8f79f83b",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Historical Context and Evolution",
    "chapter": 5,
    "section": "Historical Context and Evolution",
    "page": 8,
    "token_count": 5
  },
  {
    "chunk_id": "cb4d5754-0d94-4ca8-b84d-148ff2dabae2",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### From Simple Commands to Natural Language\n\nEarly robotics systems used simple, structured commands like \"move forward 10cm\" or \"grasp object at coordinates x,y,z\". These systems required users to learn robot-specific command languages and operate in highly structured environments.\n\nThe evolution progressed through several stages:\n\n**Stage 1: Structured Commands** (1960s-1990s)\n- Fixed command sets with specific syntax\n- Limited environmental understanding\n- Pre-programmed behaviors only\n\n**Stage 2: Goal-Based Navigation** (2000s-2010s)\n- Navigation to specified locations\n- Basic object interaction\n- Simple task execution\n\n**Stage 3: Natural Language Interfaces** (2010s-2020s)\n- Processing natural language queries\n- Limited contextual understanding\n- Template-based language processing\n\n**Stage 4: Vision-Language-Action Systems** (2020s-present)\n- Multimodal understanding of environment and commands\n- Context-aware task execution\n- Real-time adaptation to environmental changes",
    "chapter": 5,
    "section": "From Simple Commands to Natural Language",
    "page": 9,
    "token_count": 206
  },
  {
    "chunk_id": "24512aef-8913-4276-8a6e-92f3caf64d14",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Key Technological Enablers\n\nSeveral technological advances have made VLA systems possible:\n\n**Large Language Models**: Models like GPT, PaLM, and Claude that can understand and generate natural language with human-like capabilities.\n\n**Vision-Language Models**: Models like CLIP, BLIP, and Flamingo that can connect visual and linguistic representations.\n\n**Robotics Hardware**: More capable robot platforms with better sensing, actuation, and computing resources.\n\n**Deep Learning Frameworks**: Tools like TensorFlow, PyTorch, and specialized libraries for multimodal AI.",
    "chapter": 5,
    "section": "Key Technological Enablers",
    "page": 10,
    "token_count": 112
  },
  {
    "chunk_id": "2aa37a04-ddb9-4027-969c-b3ddbba84789",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## VLA System Applications",
    "chapter": 5,
    "section": "VLA System Applications",
    "page": 11,
    "token_count": 5
  },
  {
    "chunk_id": "465c7723-29bb-473d-ad52-eeb32987a8c1",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Household Robotics\n\n**Companion Robots**: Robots that can understand and execute commands like \"Please bring me my glasses from the bedroom\" or \"Clean up the mess on the kitchen counter.\"\n\n**Assistive Technologies**: Robots that assist elderly or disabled individuals with daily tasks based on natural language instructions.",
    "chapter": 5,
    "section": "Household Robotics",
    "page": 12,
    "token_count": 60
  },
  {
    "chunk_id": "978c849a-2f65-4174-a548-32cc9399c422",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Industrial and Commercial Applications\n\n**Warehouse Automation**: Robots that can understand complex picking instructions like \"Find the red box near the loading dock and place it in the shipping container.\"\n\n**Restaurant Service**: Robots that take and execute food and drink orders from customers using natural language.\n\n**Healthcare Assistance**: Robots that can understand nurse instructions like \"Move the patient from bed A to the wheelchair\" or \"Fetch the blood pressure monitor from room 102.\"",
    "chapter": 5,
    "section": "Industrial and Commercial Applications",
    "page": 13,
    "token_count": 89
  },
  {
    "chunk_id": "be4e122f-67a6-49d4-8b75-6c7d648c9d07",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Research and Development\n\n**Laboratory Automation**: Robots that can follow complex experimental protocols described in natural language.\n\n**Search and Rescue**: Robots that can follow high-level commands like \"Find survivors in the building\" while interpreting visual information about obstacles and hazards.\n\n**Educational Robots**: Robots that can follow teaching instructions and interact naturally with students during learning activities.",
    "chapter": 5,
    "section": "Research and Development",
    "page": 14,
    "token_count": 71
  },
  {
    "chunk_id": "b5ac4976-be22-402d-913f-5fdec3014d51",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Technical Challenges",
    "chapter": 5,
    "section": "Technical Challenges",
    "page": 15,
    "token_count": 3
  },
  {
    "chunk_id": "c722fb30-b415-46a2-ba4d-d84c2d12f268",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Vision-Language Grounding\n\nOne of the primary challenges in VLA systems is **grounding** - connecting linguistic concepts to visual elements in the environment. When a human says \"pick up the red cup,\" the system must:\n\n1. Identify what constitutes \"red\" in the visual scene\n2. Recognize what objects in the scene could be considered \"cups\"\n3. Determine which of the red objects is the intended target\n4. Account for perspective, lighting, and occlusion that might affect visual recognition",
    "chapter": 5,
    "section": "Vision-Language Grounding",
    "page": 16,
    "token_count": 104
  },
  {
    "chunk_id": "1807547a-4162-43e7-8ccf-519a902b4f6a",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Action Space Mapping\n\nConverting linguistic commands to robotic actions requires mapping high-level instructions to low-level motor commands:\n\n**High-level command**: \"Put the book on the table\"\n**Medium-level interpretation**: Grasp book → Navigate to table → Place at appropriate location\n**Low-level execution**: Joint trajectories, gripper control, path planning",
    "chapter": 5,
    "section": "Action Space Mapping",
    "page": 17,
    "token_count": 68
  },
  {
    "chunk_id": "aca925d1-bed7-4e9e-9eff-649b16db855c",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Context and Common Sense Reasoning\n\nVLA systems must incorporate contextual understanding and common sense:\n\n- Understanding that \"the cup\" refers to a previously mentioned or visible cup\n- Knowing that \"put it there\" requires identifying an appropriate \"there\" based on context\n- Understanding physical constraints (e.g., fragile objects require gentle handling)",
    "chapter": 5,
    "section": "Context and Common Sense Reasoning",
    "page": 18,
    "token_count": 68
  },
  {
    "chunk_id": "18d6bf6a-db21-4356-81a2-732116787cfd",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Real-time Processing Requirements\n\nVLA systems must process visual information and language commands in real-time while simultaneously planning and executing actions, creating a significant computational challenge that requires efficient algorithms and potentially specialized hardware.",
    "chapter": 5,
    "section": "Real-time Processing Requirements",
    "page": 19,
    "token_count": 40
  },
  {
    "chunk_id": "9f3d2fb0-e5ce-4a93-aba0-e1f07c9c192a",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## VLA System Design Principles",
    "chapter": 5,
    "section": "VLA System Design Principles",
    "page": 20,
    "token_count": 6
  },
  {
    "chunk_id": "2bc18bf0-981d-4e58-9f1b-110c56716761",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Safety-First Architecture\n\nVLA systems must incorporate multiple safety layers:\n\n**Perceptual Safety**: Ensuring the robot correctly perceives obstacles, fragile objects, and humans in the environment.\n\n**Action Safety**: Validating that planned actions won't cause harm to humans, objects, or the robot itself.\n\n**Communication Safety**: Ensuring the robot understands commands correctly and can clarify ambiguous instructions.",
    "chapter": 5,
    "section": "Safety-First Architecture",
    "page": 21,
    "token_count": 79
  },
  {
    "chunk_id": "a6c6d159-2c35-4bbd-8494-fd429888247d",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Interpretability and Explainability\n\nFor effective human-robot interaction, VLA systems should be able to explain their decisions:\n\n- Why they selected a particular object as the target\n- How they interpreted an ambiguous command\n- What environmental factors influenced their action selection",
    "chapter": 5,
    "section": "Interpretability and Explainability",
    "page": 22,
    "token_count": 52
  },
  {
    "chunk_id": "9429f0a0-93be-4f9a-8d98-658bf91b42d7",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Robustness and Fallback Strategies\n\nVLA systems must handle failures gracefully:\n\n- When language understanding fails, ask for clarification\n- When visual perception is uncertain, request confirmation\n- When action execution fails, attempt alternative approaches",
    "chapter": 5,
    "section": "Robustness and Fallback Strategies",
    "page": 23,
    "token_count": 46
  },
  {
    "chunk_id": "d466d920-e511-455b-bd63-14a7d5b4248a",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Examples of VLA Systems",
    "chapter": 5,
    "section": "Examples of VLA Systems",
    "page": 24,
    "token_count": 6
  },
  {
    "chunk_id": "3bb4aca1-ce7a-4909-8b36-2255f2b414fd",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Waymo and Autonomous Vehicles\n\nModern autonomous vehicles incorporate VLA principles by:\n- Processing visual inputs to understand traffic, pedestrians, and road signs\n- Interpreting traffic rules and regulations (language-based rules)\n- Executing driving actions based on both perception and rules",
    "chapter": 5,
    "section": "Waymo and Autonomous Vehicles",
    "page": 25,
    "token_count": 54
  },
  {
    "chunk_id": "16eb22b6-9c4e-44e3-bfca-262e28f2aa04",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Amazon Robotics\n\nWarehouse robots that:\n- Process visual information to locate packages and obstacles\n- Interpret picking and placing instructions from warehouse management systems\n- Execute navigation and manipulation tasks in complex environments",
    "chapter": 5,
    "section": "Amazon Robotics",
    "page": 26,
    "token_count": 38
  },
  {
    "chunk_id": "86bf1460-0a07-4a6f-8e46-f15a72d7b1ca",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Google Robotics\n\nSystems that can:\n- Understand natural language commands like \"open the drawer and take the red mug\"\n- Process visual information to identify the drawer and mug\n- Execute the sequence of actions required to complete the task",
    "chapter": 5,
    "section": "Google Robotics",
    "page": 27,
    "token_count": 46
  },
  {
    "chunk_id": "f6af6dbe-5174-41f8-b0f8-ce11307a045a",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### OpenVLA and Open-Instruct Projects\n\nOpen-source VLA systems that demonstrate:\n- How to train multimodal models on robot datasets\n- Techniques for grounding language in visual scenes\n- Approaches to action generation from language commands",
    "chapter": 5,
    "section": "OpenVLA and Open-Instruct Projects",
    "page": 28,
    "token_count": 47
  },
  {
    "chunk_id": "dec796da-e14c-4b8f-a5ac-601b9d9bce11",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Learning Outcomes\n\nAfter completing this section, students will be able to:\n\n1. Define Vision-Language-Action systems and their key components\n2. Explain the historical evolution from simple command systems to VLA systems\n3. Identify applications of VLA systems across different domains\n4. Recognize the technical challenges in VLA system development\n5. Understand the design principles that guide VLA system architecture\n6. Describe examples of current VLA system implementations",
    "chapter": 5,
    "section": "Learning Outcomes",
    "page": 29,
    "token_count": 92
  },
  {
    "chunk_id": "95933838-33ab-4fb4-a812-0e8c765c22fd",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Advanced Topics",
    "chapter": 5,
    "section": "Advanced Topics",
    "page": 30,
    "token_count": 3
  },
  {
    "chunk_id": "ccfde83a-e111-4790-94b7-8ddefe65411f",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Emergent Behaviors in VLA Systems\n\nAs VLA systems become more sophisticated, they may exhibit emergent behaviors that weren't explicitly programmed, such as:\n\n- Learning from environmental feedback to improve performance\n- Developing novel strategies for complex tasks\n- Adapting to new environments without explicit reprogramming",
    "chapter": 5,
    "section": "Emergent Behaviors in VLA Systems",
    "page": 31,
    "token_count": 61
  },
  {
    "chunk_id": "4090de1f-973a-46cd-9c81-7079b2b845b3",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Human-Robot Collaboration\n\nAdvanced VLA systems will focus on:\n- Natural collaboration between humans and robots\n- Understanding and predicting human intentions\n- Proactive assistance based on environmental context\n- Social interaction and communication skills",
    "chapter": 5,
    "section": "Human-Robot Collaboration",
    "page": 32,
    "token_count": 44
  },
  {
    "chunk_id": "c89568bc-de72-4e32-a7b1-c6e537d8efb3",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "### Multi-Modal Integration Beyond Vision and Language\n\nFuture VLA systems may incorporate:\n- Tactile sensing for manipulation tasks\n- Auditory processing for sound-based environmental understanding\n- Olfactory sensing for specific applications\n- Integration of multiple sensory modalities for richer environmental understanding",
    "chapter": 5,
    "section": "Multi-Modal Integration Beyond Vision and Language",
    "page": 33,
    "token_count": 56
  },
  {
    "chunk_id": "5d5e817d-ab71-4d34-98b7-a2ce26185bc9",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## Further Reading\n\n- \"Vision-Language Models in Robotics: A Survey\" (IEEE Transactions on Robotics, 2024) [1]\n- \"Multimodal AI for Robotics: Challenges and Opportunities\" (Annual Review of Control, Robotics, and Autonomous Systems, 2024) [2]\n- \"Natural Language Guided Robot Manipulation\" (Robotics and Autonomous Systems, 2024) [3]\n\n---",
    "chapter": 5,
    "section": "Further Reading",
    "page": 34,
    "token_count": 85
  },
  {
    "chunk_id": "52780b38-49ca-464f-b9b1-7135c6d21032",
    "doc_id": "b4b54237-ec09-43b5-9bc9-ee42ac84fe8a",
    "chunk_text": "## References\n\n[1] Chen, X., et al. \"Vision-Language Models in Robotics: Current State and Future Directions.\" IEEE Transactions on Robotics, vol. 40, no. 4, pp. 1123-1140, 2024.\n\n[2] Kumar, A., et al. \"Multimodal AI for Robotics: Challenges and Opportunities.\" Annual Review of Control, Robotics, and Autonomous Systems, vol. 7, pp. 235-260, 2024.\n\n[3] Rodriguez, M., et al. \"Natural Language Guided Robot Manipulation: A Comprehensive Review.\" Robotics and Autonomous Systems, vol. 168, pp. 104-125, 2024.",
    "chapter": 5,
    "section": "References",
    "page": 35,
    "token_count": 147
  },
  {
    "chunk_id": "a61119e5-c62f-45b5-bcde-36a8a1c95678",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "# Glossary of Terms",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 5
  },
  {
    "chunk_id": "305fb35c-b0b3-4d76-bf77-443c548f4509",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## A\n\n**Affordance**: In robotics, the property of an object that indicates its possible uses. For example, a handle affords grasping, and a flat surface affords placing objects.\n\n**Artificial Intelligence (AI)**: The simulation of human intelligence processes by machines, especially computer systems. In robotics, AI enables robots to perform tasks that typically require human intelligence.",
    "chapter": 0,
    "section": "A",
    "page": 2,
    "token_count": 77
  },
  {
    "chunk_id": "57219eb5-bbb7-4013-8722-0d0e75969c68",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## B\n\n**Behavior Trees**: A mathematical model of task planning hierarchy that can be used to manage complex robot behaviors in a modular way.",
    "chapter": 0,
    "section": "B",
    "page": 3,
    "token_count": 28
  },
  {
    "chunk_id": "1cd4e0a7-791e-40f3-80f3-c03e0bcafb22",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## C\n\n**CLIP (Contrastive Language-Image Pre-training)**: A neural network architecture that connects visual and linguistic representations by contrasting matching and non-matching image-text pairs.\n\n**Computer Vision**: A field of artificial intelligence that trains computers to interpret and understand the visual world. Used extensively in robotics for object recognition and scene understanding.\n\n**Cross-Modal Attention**: A mechanism in multimodal AI that allows information from one modality (e.g., text) to guide processing in another modality (e.g., vision).",
    "chapter": 0,
    "section": "C",
    "page": 4,
    "token_count": 107
  },
  {
    "chunk_id": "45a88cce-5f52-45d0-a6f5-836f6a8ce8a0",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## D\n\n**Dense Object Detector**: A computer vision system that detects and classifies all objects in an image, providing bounding boxes and class labels.\n\n**Digital Twin**: A virtual representation of a physical robot or robotic system that exists in real-time, allowing engineers to simulate, analyze, and optimize robot behavior before deploying on the physical robot.\n\n**Domain Randomization**: A technique used in simulation to train AI models by randomizing visual properties such as textures, lighting, and colors to improve model generalization to real-world conditions.\n\n**DQN (Deep Q-Network)**: A deep reinforcement learning algorithm that combines Q-learning with deep neural networks to enable agents to learn policies for complex decision-making tasks.",
    "chapter": 0,
    "section": "D",
    "page": 5,
    "token_count": 141
  },
  {
    "chunk_id": "08d22f96-4119-4bda-a3df-153850903543",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## E\n\n**Embodied AI**: Artificial intelligence systems that are implemented in physical or simulated robots, allowing them to interact with the physical world.\n\n**Embodied Intelligence**: The idea that intelligence emerges from the interaction between an agent and its environment, where the physical form influences cognitive processes.",
    "chapter": 0,
    "section": "E",
    "page": 6,
    "token_count": 57
  },
  {
    "chunk_id": "47e1199e-31aa-440b-bdfd-79fb080b3587",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## F\n\n**Feature Extraction**: The process of identifying and extracting relevant information from raw data for use in machine learning models.\n\n**Foundation Model**: Large-scale machine learning models trained on diverse data that can be adapted to various downstream tasks.",
    "chapter": 0,
    "section": "F",
    "page": 7,
    "token_count": 47
  },
  {
    "chunk_id": "9f556cbc-c282-429a-9455-02424c5087d5",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## G\n\n**Gazebo**: A robotics simulation environment that provides accurate physics simulation, realistic sensor simulation, and rendering capabilities.\n\n**GPU (Graphics Processing Unit)**: A specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. Used extensively in robotics for deep learning acceleration.\n\n**Grounding**: The process of connecting abstract linguistic or conceptual representations to physical entities in the environment.",
    "chapter": 0,
    "section": "G",
    "page": 8,
    "token_count": 92
  },
  {
    "chunk_id": "8e5e255c-b05e-4154-9797-d0044db0a607",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## I\n\n**Isaac ROS**: A collection of hardware-accelerated perception, navigation, and manipulation packages that run on NVIDIA Jetson and RTX GPU platforms.\n\n**Isaac Sim**: A high-fidelity simulation environment built on NVIDIA's Omniverse platform for training AI models for robotics applications.\n\n**IMU (Inertial Measurement Unit)**: A device that measures and reports a body's specific force, angular rate, and sometimes the magnetic field surrounding the body.\n\n**Inertial Navigation**: Navigation based on the integration of acceleration measurements from accelerometers and angular rate measurements from gyroscopes.",
    "chapter": 0,
    "section": "I",
    "page": 9,
    "token_count": 121
  },
  {
    "chunk_id": "20293cf8-8275-4b4d-8c3b-62f3e6893841",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## K\n\n**Kinematics**: The branch of mechanics concerned with the motion of objects without reference to the forces that cause the motion.",
    "chapter": 0,
    "section": "K",
    "page": 10,
    "token_count": 27
  },
  {
    "chunk_id": "87db756f-23af-4b20-b3bc-68ac7a447d44",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## L\n\n**Large Language Model (LLM)**: A type of language model that is trained on vast amounts of text data and typically contains billions of parameters.\n\n**LIDAR (Light Detection and Ranging)**: A remote sensing method that uses light in the form of a pulsed laser to measure distances to objects.\n\n**Loop Closure**: In SLAM (Simultaneous Localization and Mapping), recognizing when a robot returns to a previously visited location to correct accumulated errors.",
    "chapter": 0,
    "section": "L",
    "page": 11,
    "token_count": 96
  },
  {
    "chunk_id": "3dd97f03-0d04-4213-a449-ffd5d8494a65",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## M\n\n**Multimodal AI**: Artificial intelligence systems that can process and integrate information from multiple modalities such as vision, language, audio, and tactile sensing.\n\n**Machine Learning**: A subset of AI that enables systems to automatically learn and improve from experience without being explicitly programmed.",
    "chapter": 0,
    "section": "M",
    "page": 12,
    "token_count": 57
  },
  {
    "chunk_id": "7315c388-c331-408b-a10b-fce006602e78",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## N\n\n**Natural Language Processing (NLP)**: A field of AI that focuses on the interaction between computers and humans through natural language.\n\n**Neural Radiance Fields (NeRFs)**: A machine learning approach for synthesizing novel views of complex 3D scenes based on a sparse set of 2D images.",
    "chapter": 0,
    "section": "N",
    "page": 13,
    "token_count": 67
  },
  {
    "chunk_id": "78866022-d703-4e89-840b-6058cb2700e8",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## O\n\n**Object Detection**: A computer vision technique for identifying and locating objects within an image or video.\n\n**Ontology**: A formal naming and definition of the types, properties, and interrelationships of entities in a particular domain.",
    "chapter": 0,
    "section": "O",
    "page": 14,
    "token_count": 47
  },
  {
    "chunk_id": "cb8df550-54d4-4f93-b016-aea42bccc2f4",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## P\n\n**Perception Pipeline**: A sequence of computer vision and machine learning algorithms that process sensor data to extract meaningful information about the environment.\n\n**Point Cloud**: A collection of data points in a three-dimensional coordinate system, typically produced by 3D scanners or LIDAR sensors.\n\n**Probabilistic Robotics**: A framework for robot perception and action that uses probability theory to represent and reason about uncertainty in the robot's environment.",
    "chapter": 0,
    "section": "P",
    "page": 15,
    "token_count": 87
  },
  {
    "chunk_id": "b3398fe1-6b73-4b4a-b11e-0046b29f51d0",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## Q\n\n**Quality of Service (QoS)**: In ROS 2, configurable policies that define communication behavior including reliability, durability, and history.",
    "chapter": 0,
    "section": "Q",
    "page": 16,
    "token_count": 31
  },
  {
    "chunk_id": "56af7191-3f44-4a2d-967d-489e9a954654",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## R\n\n**ROS (Robot Operating System)**: A flexible framework for writing robot software that provides a collection of tools, libraries, and conventions for creating robot applications.\n\n**ROS 2**: The second generation of the Robot Operating System, designed to be production-ready with improved security, determinism, and real-time capabilities.\n\n**Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward.\n\n**Robot Operating System (ROS)**: A flexible framework for writing robot software that provides a collection of tools, libraries, and conventions for creating robot applications.\n\n**ROS 2 Navigation (Nav2)**: The navigation stack for ROS 2, providing capabilities for path planning, obstacle avoidance, and autonomous navigation.",
    "chapter": 0,
    "section": "R",
    "page": 17,
    "token_count": 156
  },
  {
    "chunk_id": "c38e0d97-c07a-41e8-92eb-73699e2fe36c",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## S\n\n**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.\n\n**Sensor Fusion**: The process of combining data from multiple sensors to achieve more accurate and reliable information than possible with any single sensor.\n\n**Sim-to-Real Transfer**: The process of transferring behaviors, policies, or models learned in simulation to real-world robotic systems.\n\n**Spatial Understanding**: The ability to comprehend and navigate the 3D spatial relationships between objects and locations in an environment.\n\n**SDF (Simulation Description Format)**: The native file format for describing simulation environments in Gazebo.\n\n**URDF (Unified Robot Description Format)**: An XML format for representing a robot model including links, joints, and inertial properties.",
    "chapter": 0,
    "section": "S",
    "page": 18,
    "token_count": 166
  },
  {
    "chunk_id": "6e9da34b-c302-44ba-8f9d-f8a07a5dff31",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## T\n\n**Transformer Architecture**: A deep learning model architecture that uses self-attention mechanisms to process sequence data, widely used in NLP and computer vision.\n\n**Task Planning**: The process of determining a sequence of actions to achieve a specific goal.\n\n**Teleoperation**: The remote control of a robot by a human operator, often with sensory feedback to enhance the operator's awareness.",
    "chapter": 0,
    "section": "T",
    "page": 19,
    "token_count": 76
  },
  {
    "chunk_id": "dd6f0cfb-4532-48ed-aca0-f736d07d40bf",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## V\n\n**VLA (Vision-Language-Action)**: Systems that integrate visual perception, natural language understanding, and robot action execution.\n\n**Visual Servoing**: A control strategy that uses visual feedback to control robot motion.\n\n**Virtual Reality (VR)**: A simulated experience that can be similar to or completely different from the real world, used in robotics for training and teleoperation.\n\n**Velocity Controllers**: Controllers that regulate the velocity of robot joints or actuators.",
    "chapter": 0,
    "section": "V",
    "page": 20,
    "token_count": 94
  },
  {
    "chunk_id": "21b69488-06c9-4b07-a9ad-453690579a9f",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## W\n\n**World Model**: An internal representation of the environment that a robot uses for planning and decision-making.",
    "chapter": 0,
    "section": "W",
    "page": 21,
    "token_count": 23
  },
  {
    "chunk_id": "6118e788-d757-46be-a7c1-121e5e317c0f",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## X\n\n**Xacro (XML Macros)**: An XML macro language that extends URDF, allowing for more readable and maintainable robot descriptions through parameterization and modular inclusion.",
    "chapter": 0,
    "section": "X",
    "page": 22,
    "token_count": 36
  },
  {
    "chunk_id": "9a50a7a3-e5bd-44c8-95e0-3152fd6ab949",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## Y\n\n**Yaw**: The rotation of an object around its vertical axis.",
    "chapter": 0,
    "section": "Y",
    "page": 23,
    "token_count": 16
  },
  {
    "chunk_id": "1d0c3ac3-2da2-4905-acfa-1200ada768b6",
    "doc_id": "7f725d6e-f83c-4382-904e-f3ccc1888f52",
    "chunk_text": "## Z\n\n**Zero-Shot Learning**: The ability of a model to perform tasks it has never been explicitly trained on, using knowledge transferred from related tasks.",
    "chapter": 0,
    "section": "Z",
    "page": 24,
    "token_count": 32
  },
  {
    "chunk_id": "49c6551c-3250-4c28-bf07-34a7c3810cc3",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "# Introduction to AI-Native Robotics\n\nWelcome to **AI-Native Robotics** - your comprehensive guide to building intelligent robotic systems that seamlessly integrate artificial intelligence, perception, and physical action.",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 37
  },
  {
    "chunk_id": "8c108c9e-69a6-4627-b641-27ba902bc6d5",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🎯 What You'll Learn\n\nThis textbook takes you on a journey from fundamental concepts to advanced implementations in robotics and AI. By the end, you'll be able to:\n\n- **Design and build** intelligent robotic systems from scratch\n- **Integrate AI models** with physical robots for real-world tasks\n- **Simulate and test** robots in digital twin environments\n- **Deploy vision-language-action (VLA)** models for embodied AI\n- **Master industry-standard tools** like ROS2, Gazebo, and NVIDIA Isaac",
    "chapter": 0,
    "section": "🎯 What You'll Learn",
    "page": 2,
    "token_count": 110
  },
  {
    "chunk_id": "966f13a2-5a68-4dfd-aa91-59ac2ffe26bd",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🤖 Why AI-Native Robotics?\n\nThe robotics landscape is undergoing a fundamental transformation. Traditional robots followed pre-programmed rules and operated in controlled environments. **AI-Native Robots** are different:\n\n- **Learn from experience** using machine learning and neural networks\n- **Understand natural language** commands and context\n- **Perceive their environment** using computer vision and sensor fusion\n- **Adapt to changes** in real-time without reprogramming\n- **Collaborate with humans** in unstructured environments\n\nThis shift represents the convergence of:\n- 🧠 **Artificial Intelligence** (deep learning, foundation models)\n- 👁️ **Computer Vision** (perception, scene understanding)\n- 🦾 **Robotics** (manipulation, navigation, control)\n- 🌐 **Simulation** (digital twins, synthetic data generation)",
    "chapter": 0,
    "section": "🤖 Why AI-Native Robotics?",
    "page": 3,
    "token_count": 176
  },
  {
    "chunk_id": "90beb529-2702-402f-9efe-9327026c4903",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 📚 Textbook Structure\n\nThis course is organized into five comprehensive chapters:",
    "chapter": 0,
    "section": "📚 Textbook Structure",
    "page": 4,
    "token_count": 17
  },
  {
    "chunk_id": "ce06a6f8-614c-484b-a5b3-be511dc1226f",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Chapter 1: Introduction to Physical AI\nExplore the foundations of embodied AI and understand how artificial intelligence transitions from digital algorithms to physical robots interacting with the real world.\n\n**Topics:**\n- The Physical AI revolution\n- From digital to physical intelligence\n- The humanoid robotics landscape\n- Sensor systems and perception fundamentals",
    "chapter": 0,
    "section": "Chapter 1: Introduction to Physical AI",
    "page": 5,
    "token_count": 63
  },
  {
    "chunk_id": "8d9d4c61-2c23-49a5-a894-ab17dd3dbf01",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Chapter 2: The Robotic Nervous System (ROS2)\nMaster ROS2 (Robot Operating System 2), the industry-standard middleware that serves as the nervous system connecting all components of modern robots.\n\n**Topics:**\n- ROS2 architecture and core concepts\n- Nodes, topics, services, and actions\n- Building modular robotic systems\n- Real-time communication patterns\n- Creating production-ready robot applications",
    "chapter": 0,
    "section": "Chapter 2: The Robotic Nervous System (ROS2)",
    "page": 6,
    "token_count": 83
  },
  {
    "chunk_id": "86eae2d0-5e2c-4768-be87-fcd132c6bb9d",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Chapter 3: Digital Twin (Gazebo & Unity)\nLearn to create and leverage digital twins - virtual replicas of physical robots that enable safe testing, rapid iteration, and synthetic data generation.\n\n**Topics:**\n- Digital twin fundamentals and benefits\n- Gazebo simulation environment\n- URDF robot modeling and description\n- Unity integration for photorealistic simulation\n- Sim-to-real transfer techniques",
    "chapter": 0,
    "section": "Chapter 3: Digital Twin (Gazebo & Unity)",
    "page": 7,
    "token_count": 81
  },
  {
    "chunk_id": "6c17f247-5aff-4e4c-bd2f-7cbaf63b76da",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)\nDive into NVIDIA's Isaac platform - a complete toolkit for building, training, and deploying AI-powered robots with GPU-accelerated perception and navigation.\n\n**Topics:**\n- Isaac Sim for robot simulation\n- Isaac ROS for AI-accelerated perception\n- Navigation and path planning\n- Manipulation and grasping\n- Synthetic data generation for training",
    "chapter": 0,
    "section": "Chapter 4: The AI-Robot Brain (NVIDIA Isaac™)",
    "page": 8,
    "token_count": 88
  },
  {
    "chunk_id": "0a900abf-5425-4c39-a1f8-ff3e4a307adf",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Chapter 5: Vision-Language-Action (VLA)\nExplore the cutting edge of robotics AI - foundation models that understand vision, process natural language, and generate robotic actions.\n\n**Topics:**\n- Vision-language models for robotics\n- Transformer architectures for embodied AI\n- Action prediction and policy learning\n- Multi-modal learning (vision + language + action)\n- Deploying VLA models on physical robots",
    "chapter": 0,
    "section": "Chapter 5: Vision-Language-Action (VLA)",
    "page": 9,
    "token_count": 81
  },
  {
    "chunk_id": "c842e32d-0585-4e24-ad68-63689ca2e723",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🛠️ Prerequisites\n\nTo get the most from this textbook, you should have:",
    "chapter": 0,
    "section": "🛠️ Prerequisites",
    "page": 10,
    "token_count": 20
  },
  {
    "chunk_id": "6d0d6aa8-a9fe-4041-9c46-7cf24185c618",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Essential\n- **Python Programming**: Comfortable with Python 3.8+\n- **Linux Basics**: Familiarity with Ubuntu/Linux command line\n- **Basic Mathematics**: Linear algebra, calculus fundamentals",
    "chapter": 0,
    "section": "Essential",
    "page": 11,
    "token_count": 41
  },
  {
    "chunk_id": "78c2c63d-9e1a-49bb-9aec-b2cda78d8589",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Recommended\n- **Machine Learning Basics**: Understanding of neural networks\n- **Computer Vision**: Image processing fundamentals\n- **Version Control**: Git and GitHub workflow",
    "chapter": 0,
    "section": "Recommended",
    "page": 12,
    "token_count": 32
  },
  {
    "chunk_id": "63ade6e2-1910-4f91-9fab-4c16db4abb65",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Nice to Have\n- **C++ Experience**: For performance-critical components\n- **ROS1 Knowledge**: Helpful but not required\n- **3D Graphics**: Understanding of coordinate systems and transformations",
    "chapter": 0,
    "section": "Nice to Have",
    "page": 13,
    "token_count": 40
  },
  {
    "chunk_id": "23d259d1-e154-4e28-9b9a-155ef2a014d2",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 💻 Development Environment\n\nYou'll need the following tools and platforms:",
    "chapter": 0,
    "section": "💻 Development Environment",
    "page": 14,
    "token_count": 15
  },
  {
    "chunk_id": "c125991d-0d78-46f3-8c40-f3b8d23f76b9",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Required Software\n- **Ubuntu 22.04 LTS** (or compatible Linux distribution)\n- **ROS2 Humble** (Robot Operating System 2)\n- **Python 3.10+** with pip and virtual environments\n- **Docker** (for containerized workflows)",
    "chapter": 0,
    "section": "Required Software",
    "page": 15,
    "token_count": 59
  },
  {
    "chunk_id": "b425ac0c-e290-49df-b70c-4efd96b43bb4",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Simulation Tools\n- **Gazebo Garden** (physics simulation)\n- **NVIDIA Isaac Sim** (GPU-accelerated simulation)\n- **Unity** (optional, for photorealistic rendering)",
    "chapter": 0,
    "section": "Simulation Tools",
    "page": 16,
    "token_count": 43
  },
  {
    "chunk_id": "0848a9c6-7031-4e9e-b6c3-d2efd50f0045",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### AI/ML Frameworks\n- **PyTorch 2.0+** (deep learning framework)\n- **NVIDIA Isaac ROS** (AI perception packages)\n- **OpenCV** (computer vision library)\n- **Transformers** (Hugging Face library for foundation models)",
    "chapter": 0,
    "section": "AI/ML Frameworks",
    "page": 17,
    "token_count": 59
  },
  {
    "chunk_id": "9a5f6a6d-e773-4967-8c48-d110c2785749",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Hardware Recommendations\n- **GPU**: NVIDIA RTX 3060 or better (for Isaac Sim)\n- **RAM**: 16GB minimum, 32GB recommended\n- **Storage**: 100GB free space for simulation environments\n- **CPU**: Multi-core processor (6+ cores recommended)",
    "chapter": 0,
    "section": "Hardware Recommendations",
    "page": 18,
    "token_count": 61
  },
  {
    "chunk_id": "7ffdfe03-bfb2-45dd-8191-4fff3f5f604b",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🚀 Getting Started\n\nReady to begin your journey? Here's how to get started:",
    "chapter": 0,
    "section": "🚀 Getting Started",
    "page": 19,
    "token_count": 20
  },
  {
    "chunk_id": "a8808159-acd5-4b42-a25b-c3bcbe046147",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### 1. Set Up Your Environment\nFollow our detailed [Setup Guide](/docs/resources/setup-guide) to install all required software and configure your development environment.",
    "chapter": 0,
    "section": "1. Set Up Your Environment",
    "page": 20,
    "token_count": 33
  },
  {
    "chunk_id": "e3a8073b-ba07-4715-8611-c74105feb000",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### 2. Start with Chapter 1\nBegin with [Chapter 1: Introduction to Physical AI](/docs/chapter1/physical-ai) to understand the fundamentals of AI-Native Robotics.",
    "chapter": 0,
    "section": "2. Start with Chapter 1",
    "page": 21,
    "token_count": 42
  },
  {
    "chunk_id": "46406f98-3f8a-4bf3-85d6-d861cd37616b",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### 3. Follow Along with Examples\nEvery chapter includes hands-on code examples, tutorials, and exercises. Clone the companion repository:\n\n```bash\ngit clone https://github.com/AsmaIqbal01/ai-native-book.git\ncd ai-native-book\n```",
    "chapter": 0,
    "section": "3. Follow Along with Examples",
    "page": 22,
    "token_count": 55
  },
  {
    "chunk_id": "aa2affa6-731f-4aea-9a4d-7babda044956",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### 4. Join the Community\n- **GitHub Discussions**: Ask questions and share projects\n- **Issue Tracker**: Report bugs or suggest improvements\n- **Lab Notes**: Read our blog for updates and advanced topics",
    "chapter": 0,
    "section": "4. Join the Community",
    "page": 23,
    "token_count": 43
  },
  {
    "chunk_id": "9a626814-c40b-4c4f-86dc-ab8c21d4f5cb",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🎓 Learning Path\n\nWe recommend following this structured learning path:\n\n```mermaid\ngraph TD\n    A[Ch1: Physical AI Foundations] --> B[Ch2: ROS2 Fundamentals]\n    B --> C[Ch3: Digital Twin & Simulation]\n    C --> D[Ch4: NVIDIA Isaac Platform]\n    D --> E[Ch5: Vision-Language-Action]\n    E --> F[Final Project: Build Your AI Robot]\n```\n\n**Time Commitment**:\n- **Full Course**: 12-16 weeks (10-15 hours/week)\n- **Each Chapter**: 2-3 weeks\n- **Hands-on Labs**: 4-6 hours per chapter",
    "chapter": 0,
    "section": "🎓 Learning Path",
    "page": 24,
    "token_count": 144
  },
  {
    "chunk_id": "7c69cbd6-1651-4910-aa3b-538f013db694",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 📖 How to Use This Textbook",
    "chapter": 0,
    "section": "📖 How to Use This Textbook",
    "page": 25,
    "token_count": 10
  },
  {
    "chunk_id": "1c3bd477-c458-472b-84c9-6a0824f691e5",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Reading the Content\n- Each chapter builds on previous knowledge\n- Code examples are fully documented and runnable\n- Visual diagrams illustrate complex concepts\n- Key concepts are highlighted in callout boxes",
    "chapter": 0,
    "section": "Reading the Content",
    "page": 26,
    "token_count": 38
  },
  {
    "chunk_id": "1c26a909-6e84-4d41-adca-ec51f014be50",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Hands-On Practice\n:::tip Practice Makes Perfect\nThe best way to learn robotics is by building. Each chapter includes:\n- **Tutorials**: Step-by-step guided exercises\n- **Lab Assignments**: Challenge problems to test your understanding\n- **Projects**: End-to-end implementations of robotic systems\n:::",
    "chapter": 0,
    "section": "Hands-On Practice",
    "page": 27,
    "token_count": 64
  },
  {
    "chunk_id": "f7a21bdb-21ad-4cdd-9f36-f96331df8042",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "### Additional Resources\n- **Video Tutorials**: Supplementary video content for complex topics\n- **Reference Sheets**: Quick-reference guides and cheatsheets\n- **External Links**: Curated resources from the robotics community\n- **Troubleshooting**: Common issues and solutions",
    "chapter": 0,
    "section": "Additional Resources",
    "page": 28,
    "token_count": 52
  },
  {
    "chunk_id": "64491c76-c497-4c30-88ab-bcdabaac7cd9",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🌟 What Makes This Different?\n\nThis textbook is **AI-Native** from the ground up:\n\n1. **Modern Stack**: Uses the latest tools (ROS2, Isaac, VLA models)\n2. **Hands-On First**: Learn by building real robotic systems\n3. **Industry Relevant**: Skills directly applicable to robotics companies\n4. **Foundation Models**: Covers cutting-edge AI approaches (transformers, VLAs)\n5. **Simulation-First**: Extensive use of digital twins for safe learning\n6. **Open Source**: All code and examples are freely available",
    "chapter": 0,
    "section": "🌟 What Makes This Different?",
    "page": 29,
    "token_count": 117
  },
  {
    "chunk_id": "5e05f863-f712-4733-8130-717b796e8f0a",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🤝 Contributing\n\nThis is a living textbook that evolves with the field. Contributions are welcome:\n- **Fix typos or errors**: Submit a pull request\n- **Add examples**: Share your robot implementations\n- **Suggest improvements**: Open an issue with ideas\n- **Share feedback**: Let us know what works and what doesn't",
    "chapter": 0,
    "section": "🤝 Contributing",
    "page": 30,
    "token_count": 71
  },
  {
    "chunk_id": "8654d4bb-4379-42f7-a492-2413dce882e0",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 📬 Get Help\n\nStuck on something? Here's where to get help:\n\n1. **Check the [Troubleshooting Guide](/docs/resources/troubleshooting)**\n2. **Search existing [GitHub Issues](https://github.com/AsmaIqbal01/ai-native-book/issues)**\n3. **Ask in [Discussions](https://github.com/AsmaIqbal01/ai-native-book/discussions)**",
    "chapter": 0,
    "section": "📬 Get Help",
    "page": 31,
    "token_count": 92
  },
  {
    "chunk_id": "18d71bec-bba4-49e5-bebc-29759dd0875a",
    "doc_id": "996f4b23-eceb-4d4a-b76a-40fe6df1c74f",
    "chunk_text": "## 🚦 Ready to Begin?\n\nYou're about to embark on an exciting journey into the world of AI-Native Robotics. Let's start building intelligent machines that can sense, think, and act in the real world.\n\n<div style={{textAlign: 'center', margin: '40px 0'}}>\n  <a href=\"/docs/chapter1/physical-ai\" className=\"button button--primary button--lg\">\n    Start Chapter 1: Physical AI →\n  </a>\n</div>\n\n---\n\n**Author**: Asma Iqbal\n**Last Updated**: December 2025\n**License**: MIT\n\n:::info Questions or Feedback?\nHave questions about the course structure or content? Open a discussion on our [GitHub repository](https://github.com/AsmaIqbal01/ai-native-book/discussions).\n:::",
    "chapter": 0,
    "section": "🚦 Ready to Begin?",
    "page": 32,
    "token_count": 171
  },
  {
    "chunk_id": "d9f3c531-ecfe-4d0a-b62a-f749c1ed4ea8",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "# Gazebo Installation\n\nComplete guide for installing Gazebo Classic and Gazebo for robotics simulation.",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 22
  },
  {
    "chunk_id": "a936d51b-0eef-4a26-99aa-080bb21c664d",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Overview\n\nGazebo is a 3D robotics simulator that accurately simulates robots, sensors, and environments. This guide covers installation for various platforms.\n\n:::info\nFor ROS 2 Humble, we use **Gazebo Classic (Gazebo 11)** or the newer **Gazebo (formerly Ignition)**.\n:::",
    "chapter": 0,
    "section": "Overview",
    "page": 2,
    "token_count": 72
  },
  {
    "chunk_id": "491de99f-882d-4ab4-be97-d6273193a580",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Ubuntu Installation",
    "chapter": 0,
    "section": "Ubuntu Installation",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "6a769c0c-7b88-4433-9e04-f6498b8194bd",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Method 1: Gazebo Classic (Recommended for ROS 2 Humble)\n\n```bash\n# Install Gazebo Classic 11\nsudo apt update\nsudo apt install -y gazebo11 libgazebo11-dev\n\n# Install ROS 2 Gazebo packages\nsudo apt install -y ros-humble-gazebo-ros-pkgs\nsudo apt install -y ros-humble-gazebo-ros2-control\n```",
    "chapter": 0,
    "section": "Method 1: Gazebo Classic (Recommended for ROS 2 Humble)",
    "page": 4,
    "token_count": 93
  },
  {
    "chunk_id": "1d7dc3c4-b624-4f6a-9b5d-80bb95504315",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Method 2: New Gazebo (Fortress)\n\n```bash\n# Add Gazebo repository\nsudo sh -c 'echo \"deb http://packages.osrfoundation.org/gazebo/ubuntu-stable `lsb_release -cs` main\" > /etc/apt/sources.list.d/gazebo-stable.list'\nwget https://packages.osrfoundation.org/gazebo.key -O - | sudo apt-key add -\n\n# Install Gazebo Fortress\nsudo apt update\nsudo apt install -y gz-fortress\n\n# Install ROS 2 Gazebo bridge\nsudo apt install -y ros-humble-ros-gz\n```",
    "chapter": 0,
    "section": "Method 2: New Gazebo (Fortress)",
    "page": 5,
    "token_count": 133
  },
  {
    "chunk_id": "994e1e1f-b793-4f67-b86a-aa00674deb0d",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## WSL2 Installation",
    "chapter": 0,
    "section": "WSL2 Installation",
    "page": 6,
    "token_count": 5
  },
  {
    "chunk_id": "de23ec86-856d-47d6-a270-18c4d6a460e0",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Prerequisites\n\n1. **Install VcXsrv or X410** for GUI support on Windows\n   - Download VcXsrv: https://sourceforge.net/projects/vcxsrv/\n\n2. **Configure X Server**\n\n```bash\n# Add to ~/.bashrc\nexport DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):0\nexport LIBGL_ALWAYS_INDIRECT=1\n```",
    "chapter": 0,
    "section": "Prerequisites",
    "page": 7,
    "token_count": 91
  },
  {
    "chunk_id": "69de1f7d-0c6c-4104-b141-a27229bcc803",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Install Gazebo on WSL2\n\n```bash\n# Update system\nsudo apt update && sudo apt upgrade -y\n\n# Install Gazebo Classic\nsudo apt install -y gazebo11 libgazebo11-dev\n\n# Install ROS 2 integration\nsudo apt install -y ros-humble-gazebo-ros-pkgs\n```",
    "chapter": 0,
    "section": "Install Gazebo on WSL2",
    "page": 8,
    "token_count": 74
  },
  {
    "chunk_id": "f4d11c27-6ce0-4852-8747-f67d47c1c020",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Launch VcXsrv\n\n1. Open VcXsrv\n2. Select \"Multiple windows\"\n3. Set display number: 0\n4. Start with \"Disable access control\" checked",
    "chapter": 0,
    "section": "Launch VcXsrv",
    "page": 9,
    "token_count": 41
  },
  {
    "chunk_id": "b818c6ea-bb03-412b-8ea9-542511d2fdc2",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Docker Installation",
    "chapter": 0,
    "section": "Docker Installation",
    "page": 10,
    "token_count": 3
  },
  {
    "chunk_id": "78469f05-36d5-47b0-94a1-a86f8b30a65a",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Using Official Gazebo Docker Image\n\n```bash\n# Pull Gazebo Classic image\ndocker pull osrf/ros:humble-desktop-full\n\n# Run with GPU support (NVIDIA)\ndocker run -it --rm \\\n  --gpus all \\\n  --env=\"DISPLAY\" \\\n  --env=\"QT_X11_NO_MITSHM=1\" \\\n  --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n  osrf/ros:humble-desktop-full \\\n  bash\n```",
    "chapter": 0,
    "section": "Using Official Gazebo Docker Image",
    "page": 11,
    "token_count": 109
  },
  {
    "chunk_id": "58889b22-8cab-491c-811e-af4a86bb8b99",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Build Custom Dockerfile\n\n```dockerfile\nFROM osrf/ros:humble-desktop-full\n\n# Install Gazebo and dependencies\nRUN apt-get update && apt-get install -y \\\n    gazebo11 \\\n    libgazebo11-dev \\\n    ros-humble-gazebo-ros-pkgs \\\n    ros-humble-gazebo-ros2-control \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Setup environment\nRUN echo \"source /opt/ros/humble/setup.bash\" >> ~/.bashrc\n```",
    "chapter": 0,
    "section": "Build Custom Dockerfile",
    "page": 12,
    "token_count": 110
  },
  {
    "chunk_id": "320d4e11-727d-4a52-9d2a-612c6f6c775f",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## macOS Installation\n\n:::warning\nGazebo has limited support on macOS. Docker is recommended.\n:::",
    "chapter": 0,
    "section": "macOS Installation",
    "page": 13,
    "token_count": 23
  },
  {
    "chunk_id": "be5184cc-3cde-4dd1-811b-06d2b29d9d9f",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Using Docker\n\n```bash\n# Install XQuartz for display\nbrew install --cask xquartz\n\n# Allow connections from localhost\nxhost +localhost\n\n# Run Gazebo container\ndocker run -it --rm \\\n  -e DISPLAY=host.docker.internal:0 \\\n  osrf/ros:humble-desktop-full \\\n  gazebo\n```",
    "chapter": 0,
    "section": "Using Docker",
    "page": 14,
    "token_count": 74
  },
  {
    "chunk_id": "24fad8c5-a058-4cfb-baf9-129a691d8e29",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Verification",
    "chapter": 0,
    "section": "Verification",
    "page": 15,
    "token_count": 2
  },
  {
    "chunk_id": "77419d75-6b0b-407a-9226-ae5ea7524f6e",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Test Gazebo Classic\n\n```bash\n# Launch Gazebo Classic\ngazebo --version\n\n# Launch with GUI\ngazebo\n\n# Launch with a world file\ngazebo worlds/willowgarage.world\n```",
    "chapter": 0,
    "section": "Test Gazebo Classic",
    "page": 16,
    "token_count": 50
  },
  {
    "chunk_id": "3120cfce-1ff9-42d5-b550-35613045dff5",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Test ROS 2 Integration\n\n```bash\n# Source ROS 2\nsource /opt/ros/humble/setup.bash\n\n# Launch Gazebo with ROS 2 bridge\nros2 launch gazebo_ros gazebo.launch.py\n```",
    "chapter": 0,
    "section": "Test ROS 2 Integration",
    "page": 17,
    "token_count": 49
  },
  {
    "chunk_id": "27019d13-6f03-4f04-b792-1db720902696",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Test with Sample Robot\n\n```bash\n# Install demo packages\nsudo apt install -y ros-humble-gazebo-ros-demos\n\n# Launch turtlebot3 world\nexport TURTLEBOT3_MODEL=waffle\nros2 launch gazebo_ros_demos spawn_turtlebot3.launch.py\n```",
    "chapter": 0,
    "section": "Test with Sample Robot",
    "page": 18,
    "token_count": 64
  },
  {
    "chunk_id": "b1d108d3-0edb-49a8-9b85-dae348186d03",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## GPU Acceleration",
    "chapter": 0,
    "section": "GPU Acceleration",
    "page": 19,
    "token_count": 4
  },
  {
    "chunk_id": "bdc7a13e-5d7b-4cf2-8571-ef2d6bd25447",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### NVIDIA GPU Setup\n\n```bash\n# Install NVIDIA drivers\nsudo apt install -y nvidia-driver-525\n\n# Verify GPU\nnvidia-smi\n\n# Enable GPU in Gazebo\nexport SVGA_VGPU10=0\n```",
    "chapter": 0,
    "section": "NVIDIA GPU Setup",
    "page": 20,
    "token_count": 51
  },
  {
    "chunk_id": "3136abae-246d-4dd9-afbd-d7795b2ced42",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### AMD GPU Setup\n\n```bash\n# Install Mesa drivers\nsudo apt install -y mesa-utils\n\n# Verify OpenGL\nglxinfo | grep \"OpenGL version\"\n```",
    "chapter": 0,
    "section": "AMD GPU Setup",
    "page": 21,
    "token_count": 35
  },
  {
    "chunk_id": "1fd1bc33-fbdd-4212-966a-d3b33c11ce27",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Common Configuration",
    "chapter": 0,
    "section": "Common Configuration",
    "page": 22,
    "token_count": 3
  },
  {
    "chunk_id": "1e59330d-c510-4e1c-83cb-a65dec9d8ac0",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Gazebo Resource Path\n\n```bash\n# Add model paths to ~/.bashrc\necho \"export GAZEBO_MODEL_PATH=/usr/share/gazebo-11/models:$GAZEBO_MODEL_PATH\" >> ~/.bashrc\necho \"export GAZEBO_RESOURCE_PATH=/usr/share/gazebo-11:$GAZEBO_RESOURCE_PATH\" >> ~/.bashrc\nsource ~/.bashrc\n```",
    "chapter": 0,
    "section": "Gazebo Resource Path",
    "page": 23,
    "token_count": 82
  },
  {
    "chunk_id": "f15fa1ab-2ccb-437b-93b0-a85a093aae16",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Physics Engine Tuning\n\nEdit `~/.gazebo/gui.ini`:\n\n```ini\n[physics]\nmax_step_size = 0.001\nreal_time_factor = 1.0\nreal_time_update_rate = 1000\n```",
    "chapter": 0,
    "section": "Physics Engine Tuning",
    "page": 24,
    "token_count": 51
  },
  {
    "chunk_id": "2b4db503-040e-4c3d-a13b-3b0cdfb31f3a",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Troubleshooting",
    "chapter": 0,
    "section": "Troubleshooting",
    "page": 25,
    "token_count": 4
  },
  {
    "chunk_id": "1bae09de-da01-42ee-8ea7-1beb4c46a451",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Issue: Black screen on launch\n\n**Solution**: Update graphics drivers and set environment variable\n```bash\nexport SVGA_VGPU10=0\ngazebo\n```",
    "chapter": 0,
    "section": "Issue: Black screen on launch",
    "page": 26,
    "token_count": 36
  },
  {
    "chunk_id": "cb18f4a1-3041-4f90-9fb5-2f71d6dc7d9c",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Issue: \"Cannot find model\" error\n\n**Solution**: Set Gazebo model path\n```bash\nexport GAZEBO_MODEL_PATH=/usr/share/gazebo-11/models\n```",
    "chapter": 0,
    "section": "Issue: \"Cannot find model\" error",
    "page": 27,
    "token_count": 40
  },
  {
    "chunk_id": "95c69b97-94d0-4193-9f1b-31e0adeb07f7",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Issue: Slow simulation on WSL2\n\n**Solution**: Use hardware acceleration\n```bash\nexport LIBGL_ALWAYS_SOFTWARE=0\nexport MESA_GL_VERSION_OVERRIDE=3.3\n```",
    "chapter": 0,
    "section": "Issue: Slow simulation on WSL2",
    "page": 28,
    "token_count": 41
  },
  {
    "chunk_id": "5d68926c-d98f-4ad7-84ad-0631eb69a652",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Issue: Connection timeout\n\n**Solution**: Reset Gazebo\n```bash\nkillall gzserver gzclient\nrm -rf ~/.gazebo\n```",
    "chapter": 0,
    "section": "Issue: Connection timeout",
    "page": 29,
    "token_count": 33
  },
  {
    "chunk_id": "fe0ca070-711b-4eeb-97e1-f510a4624305",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Additional Plugins",
    "chapter": 0,
    "section": "Additional Plugins",
    "page": 30,
    "token_count": 3
  },
  {
    "chunk_id": "32fa732a-03c7-44c4-b5be-9eed62f69101",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Camera Plugin\n\n```bash\nsudo apt install -y ros-humble-gazebo-plugins\n```",
    "chapter": 0,
    "section": "Camera Plugin",
    "page": 31,
    "token_count": 21
  },
  {
    "chunk_id": "c377ff9e-0199-48c7-a0f3-7eea5a2289e9",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Robot State Publisher\n\n```bash\nsudo apt install -y ros-humble-robot-state-publisher\n```",
    "chapter": 0,
    "section": "Robot State Publisher",
    "page": 32,
    "token_count": 23
  },
  {
    "chunk_id": "d4f5c2e0-0650-49d5-8b21-b21cd0721874",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "### Control Plugins\n\n```bash\nsudo apt install -y ros-humble-gazebo-ros2-control\nsudo apt install -y ros-humble-ros2-control\nsudo apt install -y ros-humble-ros2-controllers\n```",
    "chapter": 0,
    "section": "Control Plugins",
    "page": 33,
    "token_count": 51
  },
  {
    "chunk_id": "feaf66b7-6b54-4e95-9486-25b3a4e33036",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Next Steps\n\n- Explore [Gazebo Tutorials](http://classic.gazebosim.org/tutorials)\n- Learn [URDF modeling](/docs/chapter2/urdf-humanoid-robots)\n- Build your first [robot simulation](/docs/chapter3/gazebo-world-creation)",
    "chapter": 0,
    "section": "Next Steps",
    "page": 34,
    "token_count": 65
  },
  {
    "chunk_id": "4ca6065c-03bd-4670-8346-5f3433befa1a",
    "doc_id": "95d8eb87-a888-4040-a869-69f7131a51dd",
    "chunk_text": "## Further Reading\n\n- [Gazebo API Documentation](http://osrf-distributions.s3.amazonaws.com/gazebo/api/dev/index.html)\n- [ROS 2 Gazebo Integration Guide](https://github.com/ros-simulation/gazebo_ros_pkgs)\n- [Gazebo Model Database](https://github.com/osrf/gazebo_models)",
    "chapter": 0,
    "section": "Further Reading",
    "page": 35,
    "token_count": 74
  },
  {
    "chunk_id": "60ea6a4c-53cf-477f-a515-e01adc86cbb0",
    "doc_id": "857f2c82-ba45-40d8-a9a1-eb159d821eea",
    "chunk_text": "# Hardware Requirements\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "7b617dc1-df20-4d49-8155-7936ca8d65d9",
    "doc_id": "857f2c82-ba45-40d8-a9a1-eb159d821eea",
    "chunk_text": "## Overview\n\nThis section outlines the hardware requirements for following the AI-Native Robotics textbook.",
    "chapter": 0,
    "section": "Overview",
    "page": 2,
    "token_count": 18
  },
  {
    "chunk_id": "0974266f-5c62-4dfb-9441-725ce1190194",
    "doc_id": "857f2c82-ba45-40d8-a9a1-eb159d821eea",
    "chunk_text": "## Minimum Requirements\n\n- **RAM**: 8GB\n- **CPU**: 4-core processor (Intel i5 or AMD equivalent)\n- **Storage**: 50GB free space\n- **Graphics**: Integrated graphics (Intel HD, AMD Radeon)\n- **OS**: Ubuntu 22.04, Windows 10/11 (WSL2), macOS (Docker)",
    "chapter": 0,
    "section": "Minimum Requirements",
    "page": 3,
    "token_count": 76
  },
  {
    "chunk_id": "afc73b3c-f77c-4631-a19b-e52d02837e31",
    "doc_id": "857f2c82-ba45-40d8-a9a1-eb159d821eea",
    "chunk_text": "## Recommended Requirements\n\n- **RAM**: 16GB or more\n- **CPU**: 8-core processor (Intel i7/i9 or AMD Ryzen)\n- **Storage**: 100GB SSD\n- **Graphics**: Dedicated GPU (NVIDIA preferred for Isaac Sim)\n- **OS**: Ubuntu 22.04 LTS (native installation)",
    "chapter": 0,
    "section": "Recommended Requirements",
    "page": 4,
    "token_count": 69
  },
  {
    "chunk_id": "e0379079-1e8c-4c7e-bd12-89d88fc84592",
    "doc_id": "857f2c82-ba45-40d8-a9a1-eb159d821eea",
    "chunk_text": "## GPU Requirements for Isaac Sim\n\n- NVIDIA GPU with RTX support\n- CUDA 11.8 or later\n- 8GB+ VRAM",
    "chapter": 0,
    "section": "GPU Requirements for Isaac Sim",
    "page": 5,
    "token_count": 31
  },
  {
    "chunk_id": "38714c10-948e-4fd5-bdf3-c3b25a06f93d",
    "doc_id": "857f2c82-ba45-40d8-a9a1-eb159d821eea",
    "chunk_text": "## Network Requirements\n\n- Stable internet connection for downloading packages and resources",
    "chapter": 0,
    "section": "Network Requirements",
    "page": 6,
    "token_count": 13
  },
  {
    "chunk_id": "994d6754-093b-48c1-b6f3-ae4465959b60",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "# NVIDIA Isaac Sim Installation\n\nComplete guide for installing NVIDIA Isaac Sim for GPU-accelerated robotics simulation and AI training.",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 25
  },
  {
    "chunk_id": "17538191-4bb3-4390-9513-ae10c48fed8d",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Overview\n\nNVIDIA Isaac Sim is a scalable robotics simulation platform powered by Omniverse, featuring:\n- Photorealistic rendering with RTX ray tracing\n- Accurate physics simulation\n- ROS 2 integration\n- Synthetic data generation for AI training\n- Digital twin capabilities",
    "chapter": 0,
    "section": "Overview",
    "page": 2,
    "token_count": 55
  },
  {
    "chunk_id": "05f599f0-8d9d-4b44-8386-97a8153ca64b",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## System Requirements",
    "chapter": 0,
    "section": "System Requirements",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "c1504710-fd52-41ae-81e7-4725c9363c67",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Minimum Requirements\n\n- **GPU**: NVIDIA RTX 2060 or higher\n- **VRAM**: 8GB minimum\n- **CPU**: Intel Core i7 or AMD Ryzen 7\n- **RAM**: 32GB\n- **Storage**: 50GB SSD\n- **OS**: Ubuntu 22.04 LTS or Windows 10/11\n- **Drivers**: NVIDIA Driver 525.60.11 or newer",
    "chapter": 0,
    "section": "Minimum Requirements",
    "page": 4,
    "token_count": 89
  },
  {
    "chunk_id": "4e65f0da-4385-48ef-9480-0cd7cea6e58a",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Recommended Requirements\n\n- **GPU**: NVIDIA RTX 3080/4080 or A5000/A6000\n- **VRAM**: 16GB or more\n- **CPU**: Intel Core i9 or AMD Ryzen 9\n- **RAM**: 64GB\n- **Storage**: 100GB NVMe SSD\n- **Drivers**: Latest NVIDIA Studio or Gaming drivers",
    "chapter": 0,
    "section": "Recommended Requirements",
    "page": 5,
    "token_count": 79
  },
  {
    "chunk_id": "e21db0ec-12ca-4419-b7b8-b733b683c2e8",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Installation Methods",
    "chapter": 0,
    "section": "Installation Methods",
    "page": 6,
    "token_count": 3
  },
  {
    "chunk_id": "427fce96-bfd0-4114-b77d-2aacad0c97be",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Method 1: Omniverse Launcher (Recommended)\n\n#### Step 1: Install NVIDIA Drivers\n\n**Ubuntu:**\n```bash\n# Add NVIDIA PPA\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update\n\n# Install latest NVIDIA driver\nsudo apt install -y nvidia-driver-535\n\n# Reboot system\nsudo reboot\n\n# Verify installation\nnvidia-smi\n```\n\n**Windows:**\n- Download from [NVIDIA Driver Downloads](https://www.nvidia.com/download/index.aspx)\n- Install and reboot\n\n#### Step 2: Download Omniverse Launcher\n\n1. Visit [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/)\n2. Click \"Download Omniverse\"\n3. Sign in with NVIDIA account (free)\n4. Download installer for your OS\n\n**Ubuntu:**\n```bash\n# Make installer executable\nchmod +x omniverse-launcher-linux.AppImage\n\n# Run installer\n./omniverse-launcher-linux.AppImage\n```\n\n**Windows:**\n- Run `omniverse-launcher-win.exe`\n- Follow installation wizard\n\n#### Step 3: Install Isaac Sim via Launcher\n\n1. Open Omniverse Launcher\n2. Navigate to \"Exchange\" tab\n3. Search for \"Isaac Sim\"\n4. Click \"Install\" (latest version: 2023.1.1)\n5. Wait for download (~30GB)\n\n#### Step 4: Install Dependencies",
    "chapter": 0,
    "section": "Method 1: Omniverse Launcher (Recommended)",
    "page": 7,
    "token_count": 303
  },
  {
    "chunk_id": "75b698ef-ad45-4e4f-9cb2-1a7af5f24959",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Method 1: Omniverse Launcher (Recommended)\n\n**Ubuntu:**\n```bash\n# Install required libraries\nsudo apt install -y \\\n    libatomic1 \\\n    libegl1 \\\n    libglu1-mesa \\\n    libgomp1 \\\n    libxi6 \\\n    libxrandr2 \\\n    libxxf86vm1\n\n# Install Python dependencies\npip3 install numpy==1.23.5\n```",
    "chapter": 0,
    "section": "Method 1: Omniverse Launcher (Recommended)",
    "page": 8,
    "token_count": 90
  },
  {
    "chunk_id": "333c2565-2a12-4dd5-9a55-a85edc9480c3",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Method 2: Docker Container\n\n:::info\nDocker installation is ideal for headless servers or CI/CD pipelines.\n:::\n\n#### Prerequisites\n\n```bash\n# Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt update\nsudo apt install -y nvidia-container-toolkit\nsudo systemctl restart docker\n```\n\n#### Pull Isaac Sim Container\n\n```bash\n# Login to NVIDIA NGC\ndocker login nvcr.io\n# Username: $oauthtoken\n# Password: <Your NGC API Key>\n\n# Pull Isaac Sim container\ndocker pull nvcr.io/nvidia/isaac-sim:2023.1.1",
    "chapter": 0,
    "section": "Method 2: Docker Container",
    "page": 9,
    "token_count": 208
  },
  {
    "chunk_id": "e89d4320-54a3-41ea-85b5-8a775740ce9d",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Method 2: Docker Container\n\n# Run container with display\ndocker run --name isaac-sim --entrypoint bash -it --gpus all \\\n  -e \"ACCEPT_EULA=Y\" \\\n  --rm --network=host \\\n  -e \"PRIVACY_CONSENT=Y\" \\\n  -v ~/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n  -v ~/docker/isaac-sim/cache/ov:/root/.cache/ov:rw \\\n  -v ~/docker/isaac-sim/cache/pip:/root/.cache/pip:rw \\\n  -v ~/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw \\\n  -v ~/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw \\\n  -v ~/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw \\\n  -v ~/docker/isaac-sim/data:/root/.local/share/ov/data:rw \\\n  -v ~/docker/isaac-sim/documents:/root/Documents:rw \\\n  nvcr.io/nvidia/isaac-sim:2023.1.1\n```",
    "chapter": 0,
    "section": "Method 2: Docker Container",
    "page": 10,
    "token_count": 263
  },
  {
    "chunk_id": "b5ca21dc-d27e-46e7-beb5-df1a0c7ea7a5",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Method 3: pip Installation (Python Package)\n\n```bash\n# Create virtual environment\npython3 -m venv isaac-sim-venv\nsource isaac-sim-venv/bin/activate\n\n# Install Isaac Sim (Note: Requires Omniverse Nucleus)\npip install isaacsim==2023.1.1\n```",
    "chapter": 0,
    "section": "Method 3: pip Installation (Python Package)",
    "page": 11,
    "token_count": 73
  },
  {
    "chunk_id": "40480247-10c2-4fba-bd55-a1969b2fd5e8",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## ROS 2 Bridge Setup",
    "chapter": 0,
    "section": "ROS 2 Bridge Setup",
    "page": 12,
    "token_count": 6
  },
  {
    "chunk_id": "6f6eaa76-c24a-4398-af3b-4798539ba80e",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Install ROS 2 Packages\n\n```bash\n# Install Isaac ROS packages\nsudo apt install -y \\\n  ros-humble-isaac-ros-common \\\n  ros-humble-isaac-ros-visual-slam \\\n  ros-humble-isaac-ros-image-pipeline\n\n# Install additional ROS 2 dependencies\nsudo apt install -y \\\n  ros-humble-navigation2 \\\n  ros-humble-nav2-bringup \\\n  ros-humble-ros2-control \\\n  ros-humble-ros2-controllers\n```",
    "chapter": 0,
    "section": "Install ROS 2 Packages",
    "page": 13,
    "token_count": 111
  },
  {
    "chunk_id": "921010f4-ae73-44fc-bf5f-760403279ffb",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Configure ROS 2 Bridge in Isaac Sim\n\n1. Open Isaac Sim\n2. Go to **Window → Extensions**\n3. Search for \"ROS2 Bridge\"\n4. Enable the extension\n\n**Enable via Python:**\n\n```python\nfrom omni.isaac.kit import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": False})\n\n# Enable ROS2 bridge\nfrom omni.isaac.core.utils.extensions import enable_extension\nenable_extension(\"omni.isaac.ros2_bridge\")\n\nsimulation_app.close()\n```",
    "chapter": 0,
    "section": "Configure ROS 2 Bridge in Isaac Sim",
    "page": 14,
    "token_count": 106
  },
  {
    "chunk_id": "2e142f3b-04be-4a2b-b024-084409db26ca",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Test ROS 2 Connection\n\n```bash\n# Terminal 1: Start Isaac Sim with ROS 2 example\ncd ~/.local/share/ov/pkg/isaac_sim-2023.1.1\n./python.sh standalone_examples/api/omni.isaac.ros2_bridge/carter_stereo.py\n\n# Terminal 2: Check ROS 2 topics\nsource /opt/ros/humble/setup.bash\nros2 topic list\n\n# You should see Isaac Sim topics\nros2 topic echo /clock\n```",
    "chapter": 0,
    "section": "Test ROS 2 Connection",
    "page": 15,
    "token_count": 106
  },
  {
    "chunk_id": "82bdf149-cc9f-49b8-9289-da5a15c547bc",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Verification",
    "chapter": 0,
    "section": "Verification",
    "page": 16,
    "token_count": 2
  },
  {
    "chunk_id": "5c000e81-36c9-4314-a145-f565b3851f16",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Test 1: Launch Isaac Sim\n\n**Via Launcher:**\n1. Open Omniverse Launcher\n2. Click \"Isaac Sim\"\n3. Click \"Launch\"\n\n**Via Command Line:**\n```bash\n# Ubuntu\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/isaac-sim.sh\n\n# Or use Python interface\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/python.sh\n```",
    "chapter": 0,
    "section": "Test 1: Launch Isaac Sim",
    "page": 17,
    "token_count": 100
  },
  {
    "chunk_id": "aba64cc5-8392-4779-9ec2-e1895d75c289",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Test 2: Load Sample Scene\n\n```python\nfrom omni.isaac.kit import SimulationApp\n\nsimulation_app = SimulationApp({\"headless\": False})\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.objects import DynamicCuboid\n\n# Create world\nworld = World()\n\n# Add a cube\ncube = world.scene.add(\n    DynamicCuboid(\n        name=\"cube\",\n        position=[0, 0, 1.0],\n        scale=[0.5, 0.5, 0.5],\n        color=[1.0, 0, 0],\n    )\n)\n\n# Run simulation\nworld.reset()\nfor i in range(1000):\n    world.step(render=True)\n\nsimulation_app.close()\n```",
    "chapter": 0,
    "section": "Test 2: Load Sample Scene",
    "page": 18,
    "token_count": 158
  },
  {
    "chunk_id": "d9f0c8b4-34ed-47f2-91c5-b1915958a717",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Test 3: Camera Feed\n\n```bash\n# Run camera example\ncd ~/.local/share/ov/pkg/isaac_sim-2023.1.1\n./python.sh standalone_examples/api/omni.isaac.sensor/camera.py\n```",
    "chapter": 0,
    "section": "Test 3: Camera Feed",
    "page": 19,
    "token_count": 52
  },
  {
    "chunk_id": "2f2a0abd-a5ab-41d3-a5aa-3ca845b6fd4d",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Performance Optimization",
    "chapter": 0,
    "section": "Performance Optimization",
    "page": 20,
    "token_count": 3
  },
  {
    "chunk_id": "2872527d-964c-441d-b16b-eaff84b3451c",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Enable RTX Mode\n\n```python\nimport carb\n\nsettings = carb.settings.get_settings()\nsettings.set(\"/rtx/rendermode\", \"RayTracedLighting\")\nsettings.set(\"/rtx/pathtracing/spp\", 1)\nsettings.set(\"/rtx/pathtracing/totalSpp\", 64)\n```",
    "chapter": 0,
    "section": "Enable RTX Mode",
    "page": 21,
    "token_count": 65
  },
  {
    "chunk_id": "3c7a9f32-ee78-48ba-83a0-d56479e231b3",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Reduce Resource Usage\n\n```bash\n# Launch in headless mode for better performance\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/isaac-sim.sh --headless\n```",
    "chapter": 0,
    "section": "Reduce Resource Usage",
    "page": 22,
    "token_count": 47
  },
  {
    "chunk_id": "1cc4cd6c-6b8e-429f-8d57-c4487711ff7d",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Multi-GPU Setup\n\n```python\n# Use specific GPU\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use first GPU\n```",
    "chapter": 0,
    "section": "Multi-GPU Setup",
    "page": 23,
    "token_count": 35
  },
  {
    "chunk_id": "5b796112-c9f8-401b-9e02-dd7f1f8305ba",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Troubleshooting",
    "chapter": 0,
    "section": "Troubleshooting",
    "page": 24,
    "token_count": 4
  },
  {
    "chunk_id": "19f76f93-5caa-47a4-a5f7-69c1b76a95a9",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Issue: \"No GPU detected\"\n\n**Solution**: Verify NVIDIA driver installation\n```bash\nnvidia-smi\n# Should show your GPU details\n```",
    "chapter": 0,
    "section": "Issue: \"No GPU detected\"",
    "page": 25,
    "token_count": 32
  },
  {
    "chunk_id": "45bf46f3-72e5-41d1-b8af-02694e0e488e",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Issue: Isaac Sim crashes on startup\n\n**Solution**: Update graphics drivers and disable Vulkan\n```bash\n# Set environment variable\nexport ISAAC_SIM_FORCE_OPENGL=1\n```",
    "chapter": 0,
    "section": "Issue: Isaac Sim crashes on startup",
    "page": 26,
    "token_count": 37
  },
  {
    "chunk_id": "e127c8cd-5aaa-45ae-af83-55c16ecdabb5",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Issue: ROS 2 bridge not working\n\n**Solution**: Check ROS 2 environment\n```bash\n# Ensure ROS 2 is sourced\nsource /opt/ros/humble/setup.bash\n\n# Verify ROS_DOMAIN_ID matches\necho $ROS_DOMAIN_ID\n```",
    "chapter": 0,
    "section": "Issue: ROS 2 bridge not working",
    "page": 27,
    "token_count": 54
  },
  {
    "chunk_id": "ca4a894e-1a6f-4ca4-9bd3-af293e4e195e",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Issue: Out of memory errors\n\n**Solution**: Reduce scene complexity or batch size\n```python\n# Reduce physics update rate\nworld.reset()\nworld.set_simulation_dt(physics_dt=1.0/60.0)  # 60 Hz instead of 120 Hz\n```",
    "chapter": 0,
    "section": "Issue: Out of memory errors",
    "page": 28,
    "token_count": 58
  },
  {
    "chunk_id": "2ad231ea-1650-4c3b-89bb-cb99660adeaf",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### Issue: Slow rendering performance\n\n**Solution**: Disable ray tracing\n```bash\n# Launch with rasterization\n~/.local/share/ov/pkg/isaac_sim-2023.1.1/isaac-sim.sh --/renderer/enabled=rtx --/rtx/enabled=false\n```",
    "chapter": 0,
    "section": "Issue: Slow rendering performance",
    "page": 29,
    "token_count": 64
  },
  {
    "chunk_id": "cb6209a5-d7c5-4b60-8c7f-768a6900184f",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Example Projects",
    "chapter": 0,
    "section": "Example Projects",
    "page": 30,
    "token_count": 3
  },
  {
    "chunk_id": "435426e9-28a8-485b-9fa0-af7abed8a3d0",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### 1. Carter Robot Navigation\n\n```bash\ncd ~/.local/share/ov/pkg/isaac_sim-2023.1.1\n./python.sh standalone_examples/api/omni.isaac.wheeled_robots/carter_lidar.py\n```",
    "chapter": 0,
    "section": "1. Carter Robot Navigation",
    "page": 31,
    "token_count": 53
  },
  {
    "chunk_id": "cb8e368b-9a88-4c52-9cde-9512f2ac5f63",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### 2. UR10 Manipulation\n\n```bash\n./python.sh standalone_examples/api/omni.isaac.manipulators/follow_target.py\n```",
    "chapter": 0,
    "section": "2. UR10 Manipulation",
    "page": 32,
    "token_count": 32
  },
  {
    "chunk_id": "58be3a4b-5e73-4ae0-aa7a-65855aad15ad",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "### 3. Synthetic Data Generation\n\n```bash\n./python.sh standalone_examples/replicator/basic_example.py\n```",
    "chapter": 0,
    "section": "3. Synthetic Data Generation",
    "page": 33,
    "token_count": 25
  },
  {
    "chunk_id": "25d31f59-e4be-42b7-8041-ef4839df89f9",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Useful Extensions\n\n```bash\n# Enable physics inspector\nWindow → Simulation → Physics Inspector\n\n# Enable Scene Optimizer\nWindow → Utilities → Scene Optimizer\n\n# Enable Profiler\nWindow → Profiler\n```",
    "chapter": 0,
    "section": "Useful Extensions",
    "page": 34,
    "token_count": 44
  },
  {
    "chunk_id": "43225519-4b19-4ede-b66c-3525022645f6",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Next Steps\n\n- Explore [Isaac Sim Tutorials](https://docs.omniverse.nvidia.com/isaacsim/latest/tutorials.html)\n- Learn [ROS 2 Integration](https://docs.omniverse.nvidia.com/isaacsim/latest/ros2_tutorials.html)\n- Try [Synthetic Data Generation](https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials.html)\n- Build your first [Digital Twin](/docs/chapter3/digital-twin-concepts)",
    "chapter": 0,
    "section": "Next Steps",
    "page": 35,
    "token_count": 109
  },
  {
    "chunk_id": "373a3e58-dd69-46e0-bf70-b9ed1d00c685",
    "doc_id": "7951685c-8950-4cc5-b2cd-79c2a4bcabdf",
    "chunk_text": "## Further Reading\n\n- [Isaac Sim Documentation](https://docs.omniverse.nvidia.com/isaacsim/latest/index.html)\n- [Omniverse Forum](https://forums.developer.nvidia.com/c/omniverse/)\n- [Isaac ROS GitHub](https://github.com/NVIDIA-ISAAC-ROS)\n- [Isaac Sim Python API](https://docs.omniverse.nvidia.com/isaacsim/latest/python_api.html)",
    "chapter": 0,
    "section": "Further Reading",
    "page": 36,
    "token_count": 94
  },
  {
    "chunk_id": "264f3c98-1c0c-4dc0-bdad-18a66cc0543a",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "# References\n\nComprehensive collection of resources for AI-Native Robotics learning.",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 15
  },
  {
    "chunk_id": "5dbf8df2-2ea8-4122-a6cd-5681fce28cca",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Official Documentation\n\n- [ROS 2 Documentation](https://docs.ros.org/) - Complete ROS 2 API and tutorials\n- [Gazebo Classic Tutorials](http://classic.gazebosim.org/tutorials) - Simulation environment guides\n- [NVIDIA Isaac Sim](https://developer.nvidia.com/isaac-sim) - GPU-accelerated robotics simulation\n- [Nav2 Documentation](https://navigation.ros.org/) - ROS 2 navigation framework\n- [MoveIt 2](https://moveit.ros.org/) - Motion planning framework\n- [PyTorch Documentation](https://pytorch.org/docs/) - Deep learning framework",
    "chapter": 0,
    "section": "Official Documentation",
    "page": 2,
    "token_count": 139
  },
  {
    "chunk_id": "e3df355d-5343-438f-9e2d-908f009bf74e",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Research Papers",
    "chapter": 0,
    "section": "Research Papers",
    "page": 3,
    "token_count": 3
  },
  {
    "chunk_id": "b95c3e31-0a3c-4d63-80d6-62790a0fd379",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Physical AI & Embodied Intelligence\n\n- **RT-2: Vision-Language-Action Models** (Google DeepMind, 2023)\n  [arXiv:2307.15818](https://arxiv.org/abs/2307.15818)\n\n- **PaLM-E: An Embodied Multimodal Language Model** (Google Research, 2023)\n  [arXiv:2303.03378](https://arxiv.org/abs/2303.03378)\n\n- **Open X-Embodiment: Robotic Learning Datasets** (2023)\n  [arXiv:2310.08864](https://arxiv.org/abs/2310.08864)",
    "chapter": 0,
    "section": "Physical AI & Embodied Intelligence",
    "page": 4,
    "token_count": 149
  },
  {
    "chunk_id": "b8440799-9c66-4b26-9109-05ca880a2165",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Robotics Simulation\n\n- **Isaac Gym: High Performance GPU-Based Physics Simulation** (NVIDIA, 2021)\n  [arXiv:2108.10470](https://arxiv.org/abs/2108.10470)\n\n- **Sim-to-Real Transfer in Deep Reinforcement Learning** (OpenAI, 2019)\n  [arXiv:1812.03201](https://arxiv.org/abs/1812.03201)",
    "chapter": 0,
    "section": "Robotics Simulation",
    "page": 5,
    "token_count": 98
  },
  {
    "chunk_id": "d979e6d2-1e62-41c6-8c58-f5e643021289",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Navigation & SLAM\n\n- **ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM** (2021)\n  [arXiv:2007.11898](https://arxiv.org/abs/2007.11898)\n\n- **Fast-LIO2: Fast Direct LiDAR-Inertial Odometry** (2022)\n  [arXiv:2107.06829](https://arxiv.org/abs/2107.06829)",
    "chapter": 0,
    "section": "Navigation & SLAM",
    "page": 6,
    "token_count": 113
  },
  {
    "chunk_id": "2331c673-7319-44c3-8863-d0a67a3928bf",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Books and Textbooks",
    "chapter": 0,
    "section": "Books and Textbooks",
    "page": 7,
    "token_count": 5
  },
  {
    "chunk_id": "9c096e8c-358f-452a-b253-eb76899381bc",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Robotics Fundamentals\n\n- **Modern Robotics: Mechanics, Planning, and Control** by Kevin Lynch and Frank Park\n  Free version: [modernrobotics.org](http://modernrobotics.org/)\n\n- **Probabilistic Robotics** by Sebastian Thrun, Wolfram Burgard, Dieter Fox\n  Essential for understanding SLAM and localization\n\n- **Introduction to Autonomous Mobile Robots** by Roland Siegwart\n  Comprehensive mobile robotics textbook",
    "chapter": 0,
    "section": "Robotics Fundamentals",
    "page": 8,
    "token_count": 91
  },
  {
    "chunk_id": "3d357739-1752-4eeb-a59a-b5b9933e16f0",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### ROS 2 & Programming\n\n- **Programming Robots with ROS** by Morgan Quigley et al.\n  Practical ROS programming guide\n\n- **ROS 2 Design Patterns** by Gregory Pittman\n  Best practices for ROS 2 development",
    "chapter": 0,
    "section": "ROS 2 & Programming",
    "page": 9,
    "token_count": 49
  },
  {
    "chunk_id": "62f04021-107a-4d81-92f1-9caa4ef9addf",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### AI & Machine Learning\n\n- **Deep Learning** by Ian Goodfellow, Yoshua Bengio, Aaron Courville\n  Free version: [deeplearningbook.org](https://www.deeplearningbook.org/)\n\n- **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** by Aurélien Géron\n  Practical ML implementation guide",
    "chapter": 0,
    "section": "AI & Machine Learning",
    "page": 10,
    "token_count": 79
  },
  {
    "chunk_id": "41722a40-844b-4640-9132-9559f40d200d",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Online Courses",
    "chapter": 0,
    "section": "Online Courses",
    "page": 11,
    "token_count": 3
  },
  {
    "chunk_id": "731b274c-b107-44d3-8f09-45aca15c30fe",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### ROS 2 & Robotics\n\n- **ROS 2 For Beginners** (Udemy)\n  [Course Link](https://www.udemy.com/topic/ros-2/)\n\n- **Self-Driving Cars Specialization** (Coursera - University of Toronto)\n  [Course Link](https://www.coursera.org/specializations/self-driving-cars)\n\n- **Robotics Specialization** (Coursera - University of Pennsylvania)\n  [Course Link](https://www.coursera.org/specializations/robotics)",
    "chapter": 0,
    "section": "ROS 2 & Robotics",
    "page": 12,
    "token_count": 112
  },
  {
    "chunk_id": "7d5b2ef2-ad69-45a3-b356-5b05fdf11686",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### AI & Deep Learning\n\n- **Deep Learning Specialization** (Coursera - Andrew Ng)\n  [Course Link](https://www.coursera.org/specializations/deep-learning)\n\n- **Reinforcement Learning Specialization** (Coursera - University of Alberta)\n  [Course Link](https://www.coursera.org/specializations/reinforcement-learning)",
    "chapter": 0,
    "section": "AI & Deep Learning",
    "page": 13,
    "token_count": 80
  },
  {
    "chunk_id": "3b98f826-25af-4ce6-abd1-88c8e4c5cdd5",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Computer Vision\n\n- **CS231n: Convolutional Neural Networks for Visual Recognition** (Stanford)\n  Free lectures: [cs231n.stanford.edu](http://cs231n.stanford.edu/)\n\n- **First Principles of Computer Vision** (YouTube - Columbia University)\n  [Playlist](https://www.youtube.com/channel/UCf0WB91t8Ky6AuYcQV0CcLw)",
    "chapter": 0,
    "section": "Computer Vision",
    "page": 14,
    "token_count": 89
  },
  {
    "chunk_id": "026fab66-8f1d-452f-883f-55067292d03f",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Video Tutorials\n\n- **The Construct ROS 2 Tutorials** - Hands-on ROS 2 learning\n  [YouTube Channel](https://www.youtube.com/@robotigniteacademy)\n\n- **Articulated Robotics** - Practical robotics projects\n  [YouTube Channel](https://www.youtube.com/@ArticulatedRobotics)\n\n- **NVIDIA Omniverse Tutorials**\n  [YouTube Playlist](https://www.youtube.com/c/NVIDIAOmniverse)",
    "chapter": 0,
    "section": "Video Tutorials",
    "page": 15,
    "token_count": 94
  },
  {
    "chunk_id": "eccae510-c84e-4aa6-8c38-3603dc94cde4",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Community Resources",
    "chapter": 0,
    "section": "Community Resources",
    "page": 16,
    "token_count": 3
  },
  {
    "chunk_id": "4f526691-73cc-4dad-ba78-d8d7f6d745e2",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Forums & Discussion\n\n- [ROS Discourse](https://discourse.ros.org/) - Official ROS community forum\n- [Gazebo Community](https://community.gazebosim.org/) - Simulation Q&A\n- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/) - Isaac Sim support\n- [r/ROS](https://www.reddit.com/r/ROS/) - ROS subreddit\n- [r/robotics](https://www.reddit.com/r/robotics/) - General robotics discussion",
    "chapter": 0,
    "section": "Forums & Discussion",
    "page": 17,
    "token_count": 108
  },
  {
    "chunk_id": "cc9271e2-fd9c-4078-839a-859ac8fcf5f6",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Code Repositories\n\n- [ROS 2 Examples](https://github.com/ros2/examples) - Official ROS 2 example code\n- [Nav2 Tutorials](https://github.com/ros-planning/navigation2_tutorials) - Navigation examples\n- [Awesome Robotics](https://github.com/kiloreux/awesome-robotics) - Curated robotics resources\n- [Awesome ROS2](https://github.com/fkromer/awesome-ros2) - ROS 2 packages and tools",
    "chapter": 0,
    "section": "Code Repositories",
    "page": 18,
    "token_count": 103
  },
  {
    "chunk_id": "66fb703f-9b4c-4543-b7b3-a4b8768b709c",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Datasets",
    "chapter": 0,
    "section": "Datasets",
    "page": 19,
    "token_count": 3
  },
  {
    "chunk_id": "13815a3e-bbf4-4af9-ad9e-68ef149f1058",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### Robot Learning\n\n- **Open X-Embodiment Dataset**\n  [Website](https://robotics-transformer-x.github.io/)\n\n- **RoboNet Dataset**\n  Large-scale robot manipulation data",
    "chapter": 0,
    "section": "Robot Learning",
    "page": 20,
    "token_count": 42
  },
  {
    "chunk_id": "c5dc95f3-e04e-433f-9082-a0797f3244df",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "### SLAM & Perception\n\n- **KITTI Dataset**\n  [Website](http://www.cvlibs.net/datasets/kitti/) - Autonomous driving benchmark\n\n- **TUM RGB-D Dataset**\n  [Website](https://vision.in.tum.de/data/datasets/rgbd-dataset) - RGB-D SLAM evaluation",
    "chapter": 0,
    "section": "SLAM & Perception",
    "page": 21,
    "token_count": 66
  },
  {
    "chunk_id": "351d9fe6-a228-4ce3-b3dc-d1e8f8e41e91",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Tools & Software\n\n- **RViz2** - ROS 2 3D visualization\n- **rqt** - ROS 2 Qt-based GUI tools\n- **Foxglove Studio** - Modern robotics visualization\n  [Website](https://foxglove.dev/)\n- **PlotJuggler** - Time series data visualization\n  [GitHub](https://github.com/facontidavide/PlotJuggler)",
    "chapter": 0,
    "section": "Tools & Software",
    "page": 22,
    "token_count": 89
  },
  {
    "chunk_id": "aa81913e-1e94-47e1-bbcd-a0aceabd7124",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Blogs & Newsletters\n\n- [ROS Robotics News](https://discourse.ros.org/c/general/8) - Weekly ROS updates\n- [The Robot Report](https://www.therobotreport.com/) - Robotics industry news\n- [NVIDIA Developer Blog](https://developer.nvidia.com/blog/) - AI and simulation updates\n- [Robohub](https://robohub.org/) - Robotics research and applications",
    "chapter": 0,
    "section": "Blogs & Newsletters",
    "page": 23,
    "token_count": 88
  },
  {
    "chunk_id": "dfb2f8c8-b5cf-4716-a8a3-afdd739062b3",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Conferences & Events\n\n- **ICRA** (IEEE International Conference on Robotics and Automation)\n- **IROS** (IEEE/RSJ International Conference on Intelligent Robots and Systems)\n- **RSS** (Robotics: Science and Systems)\n- **ROSCon** - Annual ROS developers conference\n- **NVIDIA GTC** - GPU Technology Conference",
    "chapter": 0,
    "section": "Conferences & Events",
    "page": 24,
    "token_count": 72
  },
  {
    "chunk_id": "7b3608dd-a818-42c5-ae1e-6ebf3b5a0d9e",
    "doc_id": "f6e9b0ea-d5f3-4190-aa95-e36891aa43e5",
    "chunk_text": "## Getting Help\n\nIf you're stuck, try these resources in order:\n\n1. **Check documentation** - Most issues are covered in official docs\n2. **Search forums** - ROS Discourse and Stack Overflow\n3. **GitHub issues** - Project-specific problems\n4. **Ask the community** - Post detailed questions on forums\n5. **Debug systematically** - Use `ros2 doctor`, logs, and visualizations",
    "chapter": 0,
    "section": "Getting Help",
    "page": 25,
    "token_count": 87
  },
  {
    "chunk_id": "bd71dbc0-8d06-49b4-b327-5390311645bc",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "# ROS 2 Installation\n\nComplete guide for installing ROS 2 Humble Hawksbill on various platforms.",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 21
  },
  {
    "chunk_id": "70df903c-d6a4-4a79-830a-c6507bcedccb",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Overview\n\nROS 2 (Robot Operating System 2) is the industry-standard middleware for robotics development. This guide covers installation for Ubuntu, WSL2, Docker, and macOS.\n\n:::info\nWe use **ROS 2 Humble Hawksbill** - the latest LTS (Long Term Support) release supported until May 2027.\n:::",
    "chapter": 0,
    "section": "Overview",
    "page": 2,
    "token_count": 72
  },
  {
    "chunk_id": "ddff65ce-4f3e-4646-b26b-e855fad7bf46",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Ubuntu 22.04 Installation (Recommended)",
    "chapter": 0,
    "section": "Ubuntu 22.04 Installation (Recommended)",
    "page": 3,
    "token_count": 10
  },
  {
    "chunk_id": "fb179d95-ee9b-4d4b-ad41-cac8e0743c1d",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Step 1: Set Locale\n\n```bash\n# Ensure UTF-8 locale\nlocale  # check for UTF-8\n\nsudo apt update && sudo apt install -y locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nlocale  # verify settings\n```",
    "chapter": 0,
    "section": "Step 1: Set Locale",
    "page": 4,
    "token_count": 82
  },
  {
    "chunk_id": "a0e397a6-fb6e-46b6-960f-ba69f0b1d184",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Step 2: Setup Sources\n\n```bash\n# Enable Ubuntu Universe repository\nsudo apt install -y software-properties-common\nsudo add-apt-repository universe\n\n# Add ROS 2 GPG key\nsudo apt update && sudo apt install -y curl\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\n\n# Add repository to sources list\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\n```",
    "chapter": 0,
    "section": "Step 2: Setup Sources",
    "page": 5,
    "token_count": 165
  },
  {
    "chunk_id": "8ae3b218-db6c-4702-a38f-8fa64c8d6a31",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Step 3: Install ROS 2\n\n```bash\n# Update package index\nsudo apt update\nsudo apt upgrade -y\n\n# Install ROS 2 Desktop (Full)\nsudo apt install -y ros-humble-desktop\n\n# Install development tools\nsudo apt install -y \\\n  ros-dev-tools \\\n  python3-colcon-common-extensions \\\n  python3-rosdep \\\n  python3-vcstool\n```",
    "chapter": 0,
    "section": "Step 3: Install ROS 2",
    "page": 6,
    "token_count": 88
  },
  {
    "chunk_id": "c4b8295d-0973-45a1-b25c-0cba7d557994",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Step 4: Initialize rosdep\n\n```bash\n# Initialize rosdep\nsudo rosdep init\nrosdep update\n```",
    "chapter": 0,
    "section": "Step 4: Initialize rosdep",
    "page": 7,
    "token_count": 27
  },
  {
    "chunk_id": "60a7e0e4-7596-45e8-bd21-08ce9ed2ea74",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Step 5: Environment Setup\n\n```bash\n# Source ROS 2 setup (add to ~/.bashrc)\necho \"source /opt/ros/humble/setup.bash\" >> ~/.bashrc\nsource ~/.bashrc\n\n# Install argcomplete (optional - enables command auto-completion)\nsudo apt install -y python3-argcomplete\n```",
    "chapter": 0,
    "section": "Step 5: Environment Setup",
    "page": 8,
    "token_count": 71
  },
  {
    "chunk_id": "63955798-b8ad-4da7-ac30-4f7deeca11c7",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## WSL2 Installation (Windows)",
    "chapter": 0,
    "section": "WSL2 Installation (Windows)",
    "page": 9,
    "token_count": 8
  },
  {
    "chunk_id": "f4a13c6a-7813-4c2a-bcb2-a5228ee9ee11",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Prerequisites\n\n1. **Enable WSL2**\n   ```powershell\n   # Run in PowerShell as Administrator\n   wsl --install\n   wsl --set-default-version 2\n   ```\n\n2. **Install Ubuntu 22.04**\n   ```powershell\n   wsl --install -d Ubuntu-22.04\n   ```\n\n3. **Update WSL Ubuntu**\n   ```bash\n   sudo apt update && sudo apt upgrade -y\n   ```",
    "chapter": 0,
    "section": "Prerequisites",
    "page": 10,
    "token_count": 100
  },
  {
    "chunk_id": "1238fdde-4f9e-4128-8fc6-ec7fc911c1a5",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Install ROS 2 on WSL2\n\nFollow the same steps as Ubuntu installation above.",
    "chapter": 0,
    "section": "Install ROS 2 on WSL2",
    "page": 11,
    "token_count": 19
  },
  {
    "chunk_id": "1eae3693-7a9c-4d8a-9458-af8aa3a1103e",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### GUI Support (X Server)\n\n**Option 1: WSLg (Built-in - Windows 11)**\n- WSL GUI apps work automatically on Windows 11\n- No configuration needed\n\n**Option 2: VcXsrv (Windows 10)**\n\n1. **Download and Install VcXsrv**\n   - Download from: https://sourceforge.net/projects/vcxsrv/\n\n2. **Configure Display in WSL**\n   ```bash\n   # Add to ~/.bashrc\n   export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):0.0\n   export LIBGL_ALWAYS_INDIRECT=1\n   ```\n\n3. **Launch VcXsrv**\n   - Multiple windows mode\n   - Display number: 0\n   - Start no client\n   - Disable access control: checked",
    "chapter": 0,
    "section": "GUI Support (X Server)",
    "page": 12,
    "token_count": 179
  },
  {
    "chunk_id": "f73ccdf8-35e5-4eca-a1f3-cf6c47299cee",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Test GUI\n\n```bash\n# Install RViz2 for testing\nsudo apt install -y ros-humble-rviz2\n\n# Source ROS 2\nsource /opt/ros/humble/setup.bash\n\n# Launch RViz2\nrviz2\n```",
    "chapter": 0,
    "section": "Test GUI",
    "page": 13,
    "token_count": 54
  },
  {
    "chunk_id": "7abfa08a-6767-4bbc-b842-fa261d9c6873",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Docker Installation",
    "chapter": 0,
    "section": "Docker Installation",
    "page": 14,
    "token_count": 3
  },
  {
    "chunk_id": "ca2c0da2-a052-42f2-86e9-f5fe74370226",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Prerequisites\n\n**Ubuntu/WSL2:**\n```bash\n# Install Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n# Add user to docker group\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n**macOS:**\n```bash\n# Install Docker Desktop\nbrew install --cask docker\n```",
    "chapter": 0,
    "section": "Prerequisites",
    "page": 15,
    "token_count": 86
  },
  {
    "chunk_id": "a7b9c3dd-76bc-4c31-9018-d72743f9da63",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Pull ROS 2 Docker Image\n\n```bash\n# Pull official ROS 2 Humble image\ndocker pull osrf/ros:humble-desktop-full\n\n# Verify installation\ndocker images | grep ros\n```",
    "chapter": 0,
    "section": "Pull ROS 2 Docker Image",
    "page": 16,
    "token_count": 42
  },
  {
    "chunk_id": "e65d71f3-7140-4839-836a-24a0308b2e86",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Run ROS 2 Container\n\n**Basic Container:**\n```bash\ndocker run -it --rm osrf/ros:humble-desktop-full bash\n```\n\n**Container with GUI (X11):**\n```bash\n# Allow X server connection\nxhost +local:docker\n\n# Run with display\ndocker run -it --rm \\\n  --env=\"DISPLAY\" \\\n  --env=\"QT_X11_NO_MITSHM=1\" \\\n  --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n  --volume=\"$HOME/ros2_ws:/root/ros2_ws:rw\" \\\n  osrf/ros:humble-desktop-full \\\n  bash\n```\n\n**Container with GPU Support:**\n```bash\n# NVIDIA GPU\ndocker run -it --rm \\\n  --gpus all \\\n  --env=\"DISPLAY\" \\\n  --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n  osrf/ros:humble-desktop-full \\\n  bash\n```",
    "chapter": 0,
    "section": "Run ROS 2 Container",
    "page": 17,
    "token_count": 212
  },
  {
    "chunk_id": "6faedebc-037f-4a63-a28e-b58bbc2bcf39",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Create Custom Dockerfile\n\n```dockerfile\nFROM osrf/ros:humble-desktop-full\n\n# Install additional packages\nRUN apt-get update && apt-get install -y \\\n    ros-humble-navigation2 \\\n    ros-humble-nav2-bringup \\\n    ros-humble-turtlebot3* \\\n    python3-pip \\\n    vim \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python packages\nRUN pip3 install numpy scipy matplotlib\n\n# Setup workspace\nRUN mkdir -p /root/ros2_ws/src\nWORKDIR /root/ros2_ws\n\n# Source ROS 2 on container start\nRUN echo \"source /opt/ros/humble/setup.bash\" >> ~/.bashrc\n\nCMD [\"bash\"]\n```\n\n**Build and Run:**\n```bash\n# Build custom image\ndocker build -t my-ros2-humble .\n\n# Run custom container\ndocker run -it --rm my-ros2-humble\n```",
    "chapter": 0,
    "section": "Create Custom Dockerfile",
    "page": 18,
    "token_count": 195
  },
  {
    "chunk_id": "116b2bb9-1dd1-4172-a0b6-ec60e2c33850",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## macOS Installation\n\n:::warning\nROS 2 has limited official support on macOS. Docker is recommended.\n:::",
    "chapter": 0,
    "section": "macOS Installation",
    "page": 19,
    "token_count": 24
  },
  {
    "chunk_id": "26d06bad-07fb-415f-b384-9d98776259cd",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Using Docker (Recommended)\n\nFollow the Docker installation steps above with macOS-specific setup:\n\n```bash\n# Install XQuartz for X11\nbrew install --cask xquartz\n\n# Start XQuartz and allow connections\nxhost +localhost\n\n# Run ROS 2 with display\ndocker run -it --rm \\\n  -e DISPLAY=host.docker.internal:0 \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  osrf/ros:humble-desktop-full\n```",
    "chapter": 0,
    "section": "Using Docker (Recommended)",
    "page": 20,
    "token_count": 106
  },
  {
    "chunk_id": "aa51d30f-0933-4209-93fd-69eb6e2d370e",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Native Installation (Advanced)\n\n```bash\n# Install Homebrew dependencies\nbrew install asio tinyxml2 eigen pcre poco\n\n# Build from source (requires significant setup)\n# See: https://docs.ros.org/en/humble/Installation/Alternatives/macOS-Development-Setup.html\n```",
    "chapter": 0,
    "section": "Native Installation (Advanced)",
    "page": 21,
    "token_count": 62
  },
  {
    "chunk_id": "acb80e3f-f36f-4849-8c1b-0017723e0c53",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Verification",
    "chapter": 0,
    "section": "Verification",
    "page": 22,
    "token_count": 2
  },
  {
    "chunk_id": "dc63c8fb-44e1-41cd-98cf-da7404be3e1f",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Test ROS 2 Installation\n\n```bash\n# Check ROS 2 version\nros2 --version\n\n# List available packages\nros2 pkg list\n\n# Run demo talker\nros2 run demo_nodes_cpp talker\n\n# In another terminal, run listener\nros2 run demo_nodes_cpp listener\n```",
    "chapter": 0,
    "section": "Test ROS 2 Installation",
    "page": 23,
    "token_count": 64
  },
  {
    "chunk_id": "3630e5a4-a189-4d89-a5bc-a329fa88f1ee",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Test C++ and Python Support\n\n```bash\n# Test C++ demo\nros2 run demo_nodes_cpp talker\n\n# Test Python demo\nros2 run demo_nodes_py talker\n```",
    "chapter": 0,
    "section": "Test C++ and Python Support",
    "page": 24,
    "token_count": 41
  },
  {
    "chunk_id": "ca312aa3-9920-481f-b5d7-8abf4c5214d2",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Test RViz2 (GUI)\n\n```bash\n# Launch RViz2\nrviz2\n\n# Should open graphical window\n```",
    "chapter": 0,
    "section": "Test RViz2 (GUI)",
    "page": 25,
    "token_count": 28
  },
  {
    "chunk_id": "66596b70-f526-4099-8e5f-2c67683c073a",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Workspace Setup",
    "chapter": 0,
    "section": "Workspace Setup",
    "page": 26,
    "token_count": 3
  },
  {
    "chunk_id": "10f4f7b9-d12e-4e01-9785-1c9e1c4f534a",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Create ROS 2 Workspace\n\n```bash\n# Create workspace directory\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws\n\n# Build workspace\ncolcon build\n\n# Source workspace\nsource install/setup.bash\n\n# Add to ~/.bashrc\necho \"source ~/ros2_ws/install/setup.bash\" >> ~/.bashrc\n```",
    "chapter": 0,
    "section": "Create ROS 2 Workspace",
    "page": 27,
    "token_count": 73
  },
  {
    "chunk_id": "b4a12d1c-d63b-4b61-9bec-d175dd1cf64c",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Clone Example Package\n\n```bash\ncd ~/ros2_ws/src\n\n# Clone examples\ngit clone https://github.com/ros2/examples src/examples -b humble\n\n# Build\ncd ~/ros2_ws\ncolcon build --packages-select examples_rclcpp_minimal_publisher\n\n# Test\nros2 run examples_rclcpp_minimal_publisher publisher_member_function\n```",
    "chapter": 0,
    "section": "Clone Example Package",
    "page": 28,
    "token_count": 75
  },
  {
    "chunk_id": "3c51edba-77d3-47c1-a4fc-80ad4983e4eb",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Essential Packages",
    "chapter": 0,
    "section": "Essential Packages",
    "page": 29,
    "token_count": 3
  },
  {
    "chunk_id": "25422ebe-70c8-4b0c-b748-c24473ff5bb6",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Install Common ROS 2 Packages\n\n```bash\n# Navigation\nsudo apt install -y ros-humble-navigation2 ros-humble-nav2-bringup\n\n# Visualization\nsudo apt install -y ros-humble-rqt* ros-humble-rviz2\n\n# Robot State Publisher\nsudo apt install -y ros-humble-robot-state-publisher ros-humble-joint-state-publisher\n\n# TF2\nsudo apt install -y ros-humble-tf2-tools ros-humble-tf-transformations\n\n# Control\nsudo apt install -y ros-humble-ros2-control ros-humble-ros2-controllers\n\n# Gazebo Integration\nsudo apt install -y ros-humble-gazebo-ros-pkgs\n\n# CV Bridge\nsudo apt install -y ros-humble-cv-bridge ros-humble-vision-opencv\n\n# Serial\nsudo apt install -y ros-humble-serial-driver\n\n# Diagnostic Tools\nsudo apt install -y ros-humble-diagnostic-updater\n```",
    "chapter": 0,
    "section": "Install Common ROS 2 Packages",
    "page": 30,
    "token_count": 209
  },
  {
    "chunk_id": "098f70c6-909d-4744-86ce-78e628e97b29",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Environment Configuration",
    "chapter": 0,
    "section": "Environment Configuration",
    "page": 31,
    "token_count": 3
  },
  {
    "chunk_id": "8ec8d171-5f19-48f0-a7ba-555b420c66bc",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Useful Aliases\n\nAdd to `~/.bashrc`:\n\n```bash\n# ROS 2 aliases\nalias cb='cd ~/ros2_ws && colcon build'\nalias cbs='cd ~/ros2_ws && colcon build --symlink-install'\nalias cbp='cd ~/ros2_ws && colcon build --packages-select'\nalias cs='cd ~/ros2_ws && source install/setup.bash'\nalias rosdep_ws='rosdep install --from-paths src --ignore-src -r -y'\n\n# Navigation aliases\nalias ws='cd ~/ros2_ws'\nalias ws_src='cd ~/ros2_ws/src'\n\n# Useful ROS 2 commands\nalias list_topics='ros2 topic list'\nalias list_nodes='ros2 node list'\n```",
    "chapter": 0,
    "section": "Useful Aliases",
    "page": 32,
    "token_count": 157
  },
  {
    "chunk_id": "fff8abc5-999f-4694-b893-02b2cf654ef2",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### ROS Domain ID\n\n```bash\n# Set ROS_DOMAIN_ID to avoid interference (0-101, 215-232)\necho \"export ROS_DOMAIN_ID=42\" >> ~/.bashrc\nsource ~/.bashrc\n```",
    "chapter": 0,
    "section": "ROS Domain ID",
    "page": 33,
    "token_count": 46
  },
  {
    "chunk_id": "6ba5da06-f4c8-43cd-b7ae-aae10db34755",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Troubleshooting",
    "chapter": 0,
    "section": "Troubleshooting",
    "page": 34,
    "token_count": 4
  },
  {
    "chunk_id": "b6db893d-7e0c-4ab4-8bac-7e6395ee84e1",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Issue: `ros2: command not found`\n\n**Solution**: Source the setup file\n```bash\nsource /opt/ros/humble/setup.bash\n```",
    "chapter": 0,
    "section": "Issue: `ros2: command not found`",
    "page": 35,
    "token_count": 33
  },
  {
    "chunk_id": "d14cac5d-f9d7-48e1-bdba-0d1f4326a083",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Issue: `No module named 'rclpy'`\n\n**Solution**: Install Python ROS packages\n```bash\nsudo apt install -y python3-rclpy ros-humble-rclpy\n```",
    "chapter": 0,
    "section": "Issue: `No module named 'rclpy'`",
    "page": 36,
    "token_count": 42
  },
  {
    "chunk_id": "00217c7c-f877-458a-b424-cc9bd4d7fa84",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Issue: Permission denied on `/dev/ttyUSB0`\n\n**Solution**: Add user to dialout group\n```bash\nsudo usermod -a -G dialout $USER\nsudo reboot\n```",
    "chapter": 0,
    "section": "Issue: Permission denied on `/dev/ttyUSB0`",
    "page": 37,
    "token_count": 41
  },
  {
    "chunk_id": "530853ae-8680-40d8-b047-bfa0cb88b11b",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Issue: `SetuptoolsDeprecationWarning`\n\n**Solution**: Update setuptools\n```bash\npip3 install --upgrade setuptools\n```",
    "chapter": 0,
    "section": "Issue: `SetuptoolsDeprecationWarning`",
    "page": 38,
    "token_count": 28
  },
  {
    "chunk_id": "f54ffc1b-5a1a-4e5f-a975-7193bafb1812",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Issue: Slow build times\n\n**Solution**: Use parallel build\n```bash\ncolcon build --parallel-workers 4\n```",
    "chapter": 0,
    "section": "Issue: Slow build times",
    "page": 39,
    "token_count": 27
  },
  {
    "chunk_id": "5eeccf84-d414-4f90-ade7-f41fd339869f",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "### Issue: DDS communication issues\n\n**Solution**: Check network and firewall\n```bash\n# Allow multicast\nsudo ufw allow from 224.0.0.0/4\nsudo ufw allow from 239.0.0.0/8\n\n# Or disable firewall temporarily\nsudo ufw disable\n```",
    "chapter": 0,
    "section": "Issue: DDS communication issues",
    "page": 40,
    "token_count": 66
  },
  {
    "chunk_id": "d6abfd12-a43d-4bbe-89b3-39eeadd50449",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Next Steps\n\n- Complete [ROS 2 Tutorials](https://docs.ros.org/en/humble/Tutorials.html)\n- Learn [Creating and Running Nodes](/docs/chapter2/creating-running-nodes)\n- Build your [First ROS 2 Package](/docs/chapter2/creating-running-nodes)\n- Explore [ROS 2 Architecture](/docs/chapter2/ros2-architecture)",
    "chapter": 0,
    "section": "Next Steps",
    "page": 41,
    "token_count": 84
  },
  {
    "chunk_id": "e9970038-1540-4a97-ad11-9f53951fcfaa",
    "doc_id": "eedea976-a24e-4b23-82d6-9abb94688770",
    "chunk_text": "## Further Reading\n\n- [ROS 2 Documentation](https://docs.ros.org/en/humble/)\n- [ROS 2 Design](https://design.ros2.org/)\n- [ROS Discourse Forum](https://discourse.ros.org/)\n- [ROS 2 GitHub](https://github.com/ros2)",
    "chapter": 0,
    "section": "Further Reading",
    "page": 42,
    "token_count": 65
  },
  {
    "chunk_id": "b93e14b2-2924-4248-8771-31bcb97596e9",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "# Setup Guide\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 13
  },
  {
    "chunk_id": "349a80bb-e992-4af9-a7ca-199c6f65db21",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "## Overview\n\nThis comprehensive setup guide will help you configure your development environment for the AI-Native Robotics textbook.",
    "chapter": 0,
    "section": "Overview",
    "page": 2,
    "token_count": 22
  },
  {
    "chunk_id": "552af214-7cca-4ca9-b1d3-dde33c3d8c8f",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "## Platform Selection\n\nChoose your platform:\n\n1. **Ubuntu 22.04 LTS (Recommended)**: Native Linux installation\n2. **WSL2**: Windows Subsystem for Linux 2 on Windows 10/11\n3. **Docker**: Containerized environment for macOS or alternative setup",
    "chapter": 0,
    "section": "Platform Selection",
    "page": 3,
    "token_count": 60
  },
  {
    "chunk_id": "f6121fce-a239-4fe4-abfd-b120cba145a9",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "## Installation Workflow",
    "chapter": 0,
    "section": "Installation Workflow",
    "page": 4,
    "token_count": 3
  },
  {
    "chunk_id": "a2dbd91a-84ef-45af-9c3b-53758761a83f",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "### Step 1: Install ROS 2 Humble\n\nSee [ROS2 Installation](ros2-installation.mdx)",
    "chapter": 0,
    "section": "Step 1: Install ROS 2 Humble",
    "page": 5,
    "token_count": 25
  },
  {
    "chunk_id": "0b678191-e306-4b58-8f19-e728aded52cd",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "### Step 2: Install Gazebo Classic\n\nSee [Gazebo Installation](gazebo-installation.mdx)",
    "chapter": 0,
    "section": "Step 2: Install Gazebo Classic",
    "page": 6,
    "token_count": 26
  },
  {
    "chunk_id": "98ff1881-59ee-4462-a63a-43fddfd919dc",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "### Step 3: Install Development Tools\n\n```bash\nsudo apt install -y python3-pip git\npip3 install flake8\n```",
    "chapter": 0,
    "section": "Step 3: Install Development Tools",
    "page": 7,
    "token_count": 31
  },
  {
    "chunk_id": "88a653df-8a4c-4d44-beb2-44313de06f88",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "### Step 4: Clone Repository\n\n```bash\ngit clone https://github.com/your-repo/ai-native-robotics.git\ncd ai-native-robotics\n```",
    "chapter": 0,
    "section": "Step 4: Clone Repository",
    "page": 8,
    "token_count": 37
  },
  {
    "chunk_id": "346a8745-3fd1-44be-b8bd-96c6a3f756bf",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "### Step 5: Verify Setup\n\n```bash\n# Verify ROS 2\nros2 --version\n\n# Verify Gazebo\ngazebo --version\n```",
    "chapter": 0,
    "section": "Step 5: Verify Setup",
    "page": 9,
    "token_count": 35
  },
  {
    "chunk_id": "8b583d19-b8a1-445f-8a14-a4888490df2f",
    "doc_id": "84028f51-8acf-41e8-a353-36feebfc02b6",
    "chunk_text": "## Next Steps\n\nProceed to Chapter 1 to begin learning!",
    "chapter": 0,
    "section": "Next Steps",
    "page": 10,
    "token_count": 13
  },
  {
    "chunk_id": "03943b44-b790-4e79-96b9-38025dd410f6",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "# Troubleshooting\n\n<!-- Add theory, examples, diagrams here -->",
    "chapter": 0,
    "section": "Introduction",
    "page": 1,
    "token_count": 14
  },
  {
    "chunk_id": "eb54413f-d21f-4d56-b169-ae69cf1137e4",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "## Overview\n\nCommon issues and solutions for the AI-Native Robotics textbook.",
    "chapter": 0,
    "section": "Overview",
    "page": 2,
    "token_count": 15
  },
  {
    "chunk_id": "f7f8db4e-5d01-498c-bfc4-973e57e78155",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "## ROS 2 Issues",
    "chapter": 0,
    "section": "ROS 2 Issues",
    "page": 3,
    "token_count": 5
  },
  {
    "chunk_id": "07af9f2c-2743-4857-a479-517735f3103d",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "### Issue: `ros2: command not found`\n\n**Solution**: Source the ROS 2 setup file:\n```bash\nsource /opt/ros/humble/setup.bash\n```",
    "chapter": 0,
    "section": "Issue: `ros2: command not found`",
    "page": 4,
    "token_count": 36
  },
  {
    "chunk_id": "fb334ddf-36e1-4410-a557-5e1ace6e2a6c",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "### Issue: `No module named 'rclpy'`\n\n**Solution**: Install ROS 2 Python packages:\n```bash\nsudo apt install python3-rclpy\n```",
    "chapter": 0,
    "section": "Issue: `No module named 'rclpy'`",
    "page": 5,
    "token_count": 36
  },
  {
    "chunk_id": "4682ba8e-50bb-4a0f-8bd8-ed253d0a6b63",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "## Gazebo Issues",
    "chapter": 0,
    "section": "Gazebo Issues",
    "page": 6,
    "token_count": 5
  },
  {
    "chunk_id": "ee13995b-6b37-48d9-b76b-353df126c31d",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "### Issue: Gazebo GUI not launching\n\n**Solution**: Check X server configuration (WSL2) or GPU drivers.",
    "chapter": 0,
    "section": "Issue: Gazebo GUI not launching",
    "page": 7,
    "token_count": 26
  },
  {
    "chunk_id": "755ac617-a202-40c7-9fc8-b807c4a7a483",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "### Issue: Robot falls through floor\n\n**Solution**: Verify collision geometries in URDF and check physics timestep.",
    "chapter": 0,
    "section": "Issue: Robot falls through floor",
    "page": 8,
    "token_count": 23
  },
  {
    "chunk_id": "d50971a9-7b83-4a5f-a049-4db8eec09827",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "## Isaac Sim Issues",
    "chapter": 0,
    "section": "Isaac Sim Issues",
    "page": 9,
    "token_count": 4
  },
  {
    "chunk_id": "3cabf28c-9d0d-4bf6-b5dc-3c0bea865e48",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "### Issue: GPU not detected\n\n**Solution**: Verify NVIDIA drivers and CUDA installation.",
    "chapter": 0,
    "section": "Issue: GPU not detected",
    "page": 10,
    "token_count": 17
  },
  {
    "chunk_id": "ef75e584-e841-4d3e-858d-0f1a0ed70660",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "## General Tips\n\n- Always check ROS 2 and Gazebo versions match the textbook requirements\n- Use `ros2 doctor` to diagnose ROS 2 issues\n- Check log files in `~/.ros/log/` for detailed error messages",
    "chapter": 0,
    "section": "General Tips",
    "page": 11,
    "token_count": 50
  },
  {
    "chunk_id": "36416265-a48c-47df-8e55-4de24c7cf3c5",
    "doc_id": "23eda9a1-89f8-45aa-9605-34697d7e54a1",
    "chunk_text": "## Getting Help\n\n- [ROS Answers](https://answers.ros.org/)\n- [Gazebo Community](https://community.gazebosim.org/)\n- [GitHub Issues](https://github.com/your-repo/ai-native-robotics/issues)",
    "chapter": 0,
    "section": "Getting Help",
    "page": 12,
    "token_count": 54
  }
]