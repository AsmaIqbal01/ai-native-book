---
id: 008
title: Module 4 VLA Clarifications Resolution
stage: spec
date: 2025-12-07
surface: agent
model: claude-sonnet-4-5
feature: vla-module
branch: 003-digital-twin-module
user: asmaiqbal
command: /sp.clarify
labels: ["clarifications", "vla-systems", "module-spec", "design-decisions"]
links:
  spec: specs/004-vla-module/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/004-vla-module/spec.md
  - history/prompts/vla-module/008-module4-clarifications.spec.prompt.md
tests:
  - Manual validation of all 5 clarification decisions
  - Cross-check implementation impacts with functional requirements
  - Verify acceptance criteria alignment with user goals
---

## Prompt

/sp.clarify
id: module4_clarify
title: "Clarifications for Module 4 â€” Vision-Language-Action Systems"

context:
  - Module 4 introduces Vision-Language-Action (VLA) pipelines for humanoid robots.
  - All actions must remain simulation-only and high-level.
  - The module should be challenging but still accessible to beginners.
  - The constitution requires tight safety constraints and consistent UX format.

questions:

  - id: Q001
    text: >
      What level of complexity should the perception system include?
      Options range from simple RGB processing to full segmentation/depth pipelines.
    options:
      A: Basic RGB + bounding box examples only (beginner-friendly)
      B: RGB + Depth + simple segmentation (balanced)  # recommended
      C: Full semantic + instance segmentation (heavy)
      Sh: "Different 5-word answer"
    recommended: B

  - id: Q002
    text: >
      Should the module use a real VLM/LLM API (Gemini, GPT, Claude) for planning,
      or use a mock/local model to reduce student setup friction?
    options:
      A: Real API calls (auth + cost)
      B: Real API + mock fallback (recommended)
      C: Mock-only (offline, constrained)
      Sh: "Different 5-word answer"
    recommended: B

  - id: Q003
    text: >
      What should be the output format of the LLM/VLA planner?
      JSON is safe and strict, while natural language is easier for beginners
      but increases parsing errors.
    options:
      A: JSON-only structured plans (recommended)
      B: Natural language + parser
      C: Multi-format: JSON primary + NL fallback
      Sh: "Different 5-word answer"
    recommended: A

  - id: Q004
    text: >
      Should VLA examples include navigation + manipulation together, or keep
      manipulation for an advanced module?
    options:
      A: Navigation + light manipulation (recommended)
      B: Navigation only
      C: Full manipulation workflow (complex)
      Sh: "Different 5-word answer"
    recommended: A

  - id: Q005
    text: >
      Should we include a small "VLA dataset" (images + labels) as part of the module?
      This improves exercises but increases repo size and prep time.
    options:
      A: Include mini dataset (recommended)
      B: Provide external links only
      C: No dataset; use simulated scenes
      Sh: "Different 5-word answer"
    recommended: A

key_ambiguities:
  - How advanced the perception stack should be for beginners.
  - Whether API-based VLA planning should be required.
  - Which output format (JSON vs NL) ensures safety + usability.
  - How much manipulation should be included at MVP stage.
  - Whether teaching data handling is in scope for Module 4.

next_steps:
  - Confirm recommended options A/B across questions.
  - Apply decisions in `/sp.plan` for Module 4.
  - Generate consistent folder structure, tasks, and implementation phases.

Additional user requirements:
- Students can explain why VLA systems matter.
- Students can implement a simple perception-to-action mapping.
- Students complete the mini-project with at least 70% accuracy on validation tasks.
- Module is readable, teachable, and integrates smoothly with previous modules.

Timeline:
- Week 1: Draft theory + examples.
- Week 2: Build labs + mini-project.
- Week 3: Review, refine, format.

## Response snapshot

Successfully resolved all 5 clarification questions for Module 4 VLA specification based on user's recommended options:

**Q001 - Perception System Complexity**: RGB + Depth + simple segmentation (Balanced)
- Implements complete perception pipeline with three modalities
- Uses lightweight models (MobileNet-based segmentation)
- Accessible on mid-range student hardware
- Impact: Chapter 2 covers all three modalities with optimization guidance

**Q002 - LLM Backend Selection**: Real API + mock fallback (Hybrid)
- Primary examples use real LLM APIs (OpenAI, Anthropic, Google)
- Mock fallback for offline learning and budget accessibility
- Cost management strategies documented
- Impact: Chapter 5 includes both real API integration and mock planner

**Q003 - LLM Output Format**: JSON-only structured plans (Safe and strict)
- Strict JSON schema validation before plan execution
- Required fields: task_type, parameters, safety_constraints, preconditions
- Reduces errors and safety risks vs. natural language parsing
- Impact: Chapter 5 defines complete JSON schema with validation

**Q004 - VLA Scope**: Navigation + light manipulation (Recommended)
- Combines navigation and light manipulation (alignment, positioning)
- NO actual grasping or force control (high-level only)
- Demonstrates full VLA capabilities safely
- Impact: Chapter 7 scenarios include multi-step coordination

**Q005 - VLA Dataset**: Include mini dataset (Recommended)
- Bundle 50-100 labeled images (<50MB)
- Includes bounding boxes, segmentation masks, object categories
- Images from simulation for consistency
- Impact: Created `examples/vla-module/datasets/` with loader utilities

**Additional Elements Integrated**:
- Added 4 new acceptance criteria (AC-013 to AC-016) from user requirements
- Documented 3-week development timeline with clear milestones
- Updated Next Steps section to reflect clarifications resolved status

**Key Implementation Impacts**:
1. Perception (Ch. 2): All three modalities with lightweight models
2. Planning (Ch. 5): Hybrid API approach with JSON schema validation
3. Execution (Ch. 7): Navigation + light manipulation scenarios
4. Resources: Mini dataset with validation tools
5. Safety: JSON-based validation ensures strict constraint checking

All decisions prioritize balance between:
- Safety (JSON validation, simulation-only, high-level control)
- Accessibility (mock fallback, lightweight models, mid-range hardware)
- Realism (real APIs, complete perception pipeline, industry patterns)
- Pedagogy (beginner-friendly with progressive complexity)

## Outcome

- âœ… Impact: All 5 clarifications resolved and documented in specification with detailed implementation impacts
- ðŸ§ª Tests: Validated consistency between decisions and functional requirements FR-001 through FR-018
- ðŸ“ Files: Updated specs/004-vla-module/spec.md with "Clarifications Resolved" section
- ðŸ” Next prompts:
  - Run `/sp.plan` to create detailed implementation plan based on clarified decisions
  - Generate phase-based task breakdown for 3-week timeline
  - Create code scaffolding for perception, planning, and execution components
  - Develop JSON schema definitions and safety validation framework
- ðŸ§  Reflection: Clarification process successfully captured all user requirements while maintaining strict safety constraints. Hybrid approaches (real API + mock, Gazebo + Isaac Sim options) maximize accessibility while teaching real-world patterns. JSON-only output format is critical for safety validation in robot control context.

## Evaluation notes (flywheel)

- Failure modes observed: None - all clarifications resolved successfully
- Graders run and results (PASS/FAIL):
  - User recommendation alignment: PASS (all 5 recommended options accepted)
  - Safety emphasis validation: PASS (JSON schema, simulation-only, high-level control)
  - Constitution compliance: PASS (accessibility, testing requirements, safety-first)
  - Implementation impact documentation: PASS (each decision includes concrete impacts)
  - Timeline integration: PASS (3-week schedule incorporated)
- Prompt variant (if applicable): Standard /sp.clarify format with 5 design questions
- Next experiment (smallest change to try): Proceed with /sp.plan to translate clarified decisions into actionable implementation phases and task breakdown
